{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "# from ollama import chat\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "# import src.inference as inference\n",
    "# from src.utils import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for bert\n",
    "\n",
    "# s_aduc_cd_fd = r'(?<=\\[SENTENCE\\]: )(.*?)(?=\\n\\[FULL TEXT\\]:)'\n",
    "# for ed s = r'(?<=\\[ARGUMENT\\]: )(.*?)(?=\\n\\[SENTENCE\\]:)' if [ARGUMENT] is needed\n",
    "# for et s = r'(?<=\\[CLAIM\\]: )(.*?)(?=\\n\\[SENTENCE\\]:)' if [CLAIM] is needed\n",
    "# s_ed_et_aqa_sd = r'(?<=\\[SENTENCE\\]: )(.*)'\n",
    "# fd s = # r'(?<=\\[SENTENCE\\]: )(.*?)(?=\\n\\[FULL TEXT\\]:)',\n",
    "\n",
    "def get_regex(task:str):\n",
    "    regex  = {\n",
    "        'aduc':  r'(?<=\\[SENTENCE\\]: )(.*?)(?=\\n\\[FULL TEXT\\]:)',\n",
    "        'claim_detection':  r'(?<=\\[SENTENCE\\]: )(.*?)(?=\\n\\[FULL TEXT\\]:)',\n",
    "        'evidence_detection': r'(?<=\\[SENTENCE\\]: )(.*)',\n",
    "        'evidence_type': r'(?<=\\[SENTENCE\\]: )(.*)',\n",
    "        'fallacies': r'(?s)(?<=\\[SENTENCE\\]: )(.*?)(?=\\n\\[FULL TEXT\\]:)',\n",
    "        'quality': r'(?<=\\[SENTENCE\\]: )(.*)',\n",
    "        'relation': {\n",
    "            'src': r'(?<=\\[SOURCE\\]: )(.*?)(?=\\n\\[TARGET\\]:)',\n",
    "            'trg': r'(?<=\\[TARGET\\]: )(.*)'\n",
    "        },\n",
    "        'stance_detection': r'(?<=\\[SENTENCE\\]: )(.*)',\n",
    "    }\n",
    "    return regex.get(task)\n",
    "\n",
    "def parse_sentence(x, s):\n",
    "    conv = x['conversations']\n",
    "    match = re.search(s, conv[1].get('content'))\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        print(f'Error: {match}, {conv[1].get(\"content\")}, {s}')\n",
    "        raise ValueError\n",
    "\n",
    "def create_data_bert(task_name:str, path_src:str, path_trg:str):\n",
    "    converter = {'conversations': literal_eval, 'answer': literal_eval}\n",
    "    # s = r'^\\./[^/]+/([^/]+)/'\n",
    "    for root, dirs, files in os.walk(path_src):\n",
    "        for file in files:\n",
    "            if 'labels' not in file:\n",
    "                f_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(\n",
    "                    f_path,\n",
    "                    converters=converter\n",
    "                )\n",
    "                task_regex = get_regex(task_name)\n",
    "                if task_name != 'relation':\n",
    "                    parse_data = df.apply(\n",
    "                        lambda x: parse_sentence(x, task_regex),\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    df['conversations'] = parse_data\n",
    "                else:\n",
    "                    src_data = df.apply(\n",
    "                        lambda x: parse_sentence(x, task_regex.get('src')),\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    trg_data = df.apply(\n",
    "                        lambda x: parse_sentence(x, task_regex.get('trg')),\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    df['conversations'] = src_data\n",
    "                    df['trg'] = trg_data\n",
    "                df.to_csv(f'{path_trg}/{file}', index=False)\n",
    "\n",
    "def parse_data(path:str):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dir in dirs:\n",
    "            if 'mt_ft' not in dir:\n",
    "                create_data_bert(dir, os.path.join(root,dir), f'spl_bert/{dir}')\n",
    "parse_data('./sampled_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
    "from datasets import Dataset\n",
    "\n",
    "def get_bert_data(task_name:str):\n",
    "    train_data = pd.read_csv(f'./spl_bert/{task_name}/spl2_train.csv')\n",
    "    test_data = pd.read_csv(f'./spl_bert/{task_name}/spl2_test.csv')\n",
    "    if task_name != 'relation':\n",
    "        train_data = train_data[['conversations', 'single_ans']]\n",
    "        test_data = test_data[['conversations', 'single_ans']]\n",
    "        train_data = train_data.rename(\n",
    "            columns={'conversations': 'text', 'single_ans': 'label'}\n",
    "        )\n",
    "        test_data = test_data.rename(\n",
    "            columns={'conversations': 'text', 'single_ans': 'label'}\n",
    "        )\n",
    "    else:\n",
    "        train_data = train_data[['conversations', 'trg', 'single_ans']]\n",
    "        test_data = test_data[['conversations','trg', 'single_ans']]\n",
    "        train_data = train_data.rename(\n",
    "            {'conversations': 'text_a', 'trg': 'text_b', 'single_ans': 'label'}\n",
    "        )\n",
    "        test_data = test_data.rename(\n",
    "            {'conversations': 'text_a', 'trg': 'text_b', 'single_ans': 'label'}\n",
    "        )\n",
    "    return train_data, test_data\n",
    "\n",
    "def tokenize_fn(batch, tokenizer):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "def get_datasets(tokenizer, tokenize_fn, task_name:str):\n",
    "    train_data, test_data = get_bert_data(task_name)\n",
    "    dataset = [\n",
    "        Dataset.from_pandas(train_data),\n",
    "        Dataset.from_pandas(test_data),\n",
    "    ]\n",
    "    for i, data in enumerate(dataset):\n",
    "        data = data.map(lambda x: tokenize_fn(x, tokenizer), batched=True).class_encode_column(column='label')\n",
    "        data = data.remove_columns(['text'])\n",
    "        data.set_format('torch')\n",
    "        dataset[i] = data\n",
    "    train_dataset = dataset[0]\n",
    "    test_dataset = dataset[1]\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    label = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(label, preds, average='weighted')\n",
    "    acc = accuracy_score(label, preds)\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'acc': acc,\n",
    "    }\n",
    "\n",
    "# task_list=[\n",
    "#         'aduc',\n",
    "#         'claim_detection',\n",
    "#         'evidence_detection',\n",
    "#         'evidence_type',\n",
    "#         'fallacies',\n",
    "#         'quality',\n",
    "#         # 'relation',\n",
    "#         'stance_detection'\n",
    "#     ]\n",
    "\n",
    "# for t in task_list:\n",
    "#     print(t)\n",
    "#     train, test = get_bert_data(t)\n",
    "#     if train['text'].isnull().any():\n",
    "#         print(t, 'train')\n",
    "#     if test['text'].isnull().any():\n",
    "#         print(t, 'test')\n",
    "\n",
    "# train, test = get_bert_data('claim_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert training\n",
    "# TODO : redo sample for claim detection and relation before training and test bert\n",
    "task_name = 'quality'\n",
    "output_dir = f'./outputs/{task_name}/deberta_{task_name}'\n",
    "model_name = 'microsoft/deberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    ")\n",
    "\n",
    "train_dataset, test_dataset = get_datasets(tokenizer, tokenize_fn, task_name)\n",
    "\n",
    "# raise ValueError\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"no\",\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3e-5,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True,\n",
    "    # eval_on_start=True,\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Test the model\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(test_results)\n",
    "test_results\n",
    "test_df = pd.Series(data=test_results)\n",
    "test_df.to_csv(f'./test_res/{task_name}/deberta_res_{task_name}.csv', header=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_f1(all_result:dict):\n",
    "    d = {}\n",
    "    for task, scores in all_result.items():\n",
    "        if task != 'fallacies':\n",
    "            d.update({task: scores.get('score_all_data')[0]})\n",
    "        else:\n",
    "            d.update({'fallacies_single': scores[0].get('score_all_data')[0]})\n",
    "            d.update({'fallacies_multi': scores[1].get('score_all_data')[0]})\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training, Test and saving \n",
    "# model, tokenizer = run('mt_ft', do_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_result = inference.inference_on_all_data(\n",
    "#     model_for_task='aduc',\n",
    "#     model_to_use='fine-tuned'\n",
    "# )\n",
    "# all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_gguf(\n",
    "#     './gguf_model/merged_gguf',\n",
    "#     tokenizer,\n",
    "#     quantization_method = \"q8_0\", # choose quant method: q8_0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thing kept just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp cell \n",
    "import numpy as np\n",
    "# (F1, Precision, Recall)\n",
    "tmp = {\n",
    "    'aduc': {\n",
    "        \"['Premise']\": (np.float64(0.9090909090909091), np.float64(0.8333333333333334),np.float64(1.0)),\n",
    "        \"['Claim']\": (np.float64(0.888888888888889), np.float64(1.0), np.float64(0.8)),\n",
    "        'score_all_data': (np.float64(0.9), np.float64(0.9), np.float64(0.9))\n",
    "    },\n",
    "    'claim_detection': {\n",
    "        \"['Non-claim']\": (np.float64(0.8571428571428571),np.float64(0.75), np.float64(1.0)),\n",
    "        \"['Claim']\": (np.float64(0.6666666666666666), np.float64(1.0), np.float64(0.5)),\n",
    "        'score_all_data': (np.float64(0.8000000000000002), np.float64(0.8), np.float64(0.8))\n",
    "    },\n",
    "     'evidence_detection': {\n",
    "        \"['Evidence']\": (np.float64(0.6666666666666665), np.float64(0.75), np.float64(0.6)),\n",
    "        \"['Non-evidence']\": (np.float64(0.7272727272727272), np.float64(0.6666666666666666), np.float64(0.8)),\n",
    "        'score_all_data': (np.float64(0.7), np.float64(0.7), np.float64(0.7))\n",
    "    },\n",
    "    'evidence_type': {\n",
    "        \"['STUDY']\": (0, 0, 0),\n",
    "        \"['ANECDOTAL']\": (np.float64(0.4),np.float64(0.5), np.float64(0.3333333333333333)),\n",
    "        \"['NONE']\": (0, 0, 0), \"['EXPLANATION']\": (0, 0, 0),\n",
    "        'score_all_data': (np.float64(0.125), np.float64(0.16666666666666666), np.float64(0.1))},\n",
    "    'fallacies': (\n",
    "        {\n",
    "            \"['AWP']\": (np.float64(1.0), np.float64(1.0), np.float64(1.0)),\n",
    "            \"['AA']\": (0, 0, 0),\n",
    "            \"['AR']\": (0, 0, 0),\n",
    "            'score_all_data': (np.float64(0.5), np.float64(1.0), np.float64(0.3333333333333333))},\n",
    "        {\n",
    "            \"['AN', 'FA', 'EQ', 'AT']\": (0, 0, 0.0),\n",
    "            \"['FA', 'STM']\": (np.float64(0.6666666666666666), np.float64(1.0), 0.5),\n",
    "            \"['FC', 'HG']\": (0, 0, 0.0),\n",
    "            \"['COS', 'GA', 'HG']\": (0, 0, 0.0),\n",
    "            \"['FC', 'GA', 'AH']\": (0, 0, 0.0),\n",
    "            'score_all_data': (np.float64(0.19047619047619047),np.float64(0.2857142857142857),np.float64(0.14285714285714285))\n",
    "        }\n",
    "    ),\n",
    "    'quality': {\n",
    "        \"['Average']\": (0, 0, 0),\n",
    "        \"['Low']\": (np.float64(0.6), np.float64(0.5), np.float64(0.75)),\n",
    "        'score_all_data': (np.float64(0.37499999999999994), np.float64(0.5), np.float64(0.3))\n",
    "    },\n",
    "    'relation': {\n",
    "        \"['no relation']\": (np.float64(0.4000000000000001), np.float64(0.4), np.float64(0.4)),\n",
    "        \"['support']\": (np.float64(0.3333333333333333), np.float64(0.5), np.float64(0.25)),\n",
    "        \"['attack']\": (np.float64(0.5), np.float64(0.3333333333333333), np.float64(1.0)),\n",
    "        'score_all_data': (np.float64(0.4000000000000001), np.float64(0.4), np.float64(0.4))\n",
    "    },\n",
    "    'stance_detection': {\n",
    "        \"['Against']\": (np.float64(0.5454545454545454), np.float64(0.6), np.float64(0.5)),\n",
    "        \"['For']\": (np.float64(0.6666666666666666), np.float64(1.0), np.float64(0.5)),\n",
    "        'score_all_data': (np.float64(0.588235294117647), np.float64(0.7142857142857143), np.float64(0.5))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thing for sampling examples in report\n",
    "\n",
    "# load sample for cd\n",
    "train_spl_cd = pd.read_csv('./sampled_data/claim_detection/spl2_train.csv')\n",
    "val_spl_cd = pd.read_csv('./sampled_data/claim_detection/spl2_val.csv')\n",
    "test_spl_cd = pd.read_csv('./sampled_data/claim_detection/spl2_test.csv')\n",
    "\n",
    "# slice iam_claim\n",
    "train_iam_claim  = train_spl_cd[train_spl_cd['datasets'] == 'iam_claim']\n",
    "train_iam_claim_cl = train_iam_claim[train_iam_claim['single_ans'] == 'Claim']\n",
    "train_iam_claim_nn_cl = train_iam_claim[\n",
    "    train_iam_claim['single_ans'] == 'Non-claim'\n",
    "]\n",
    "\n",
    "val_iam_claim = val_spl_cd[val_spl_cd['datasets'] == 'iam_claim']\n",
    "val_iam_claim_cl = val_iam_claim[val_iam_claim['single_ans'] == 'Claim']\n",
    "val_iam_claim_nn_cl = val_iam_claim[val_iam_claim['single_ans'] == 'Non-claim']\n",
    "\n",
    "test_iam_claim = test_spl_cd[test_spl_cd['datasets'] == 'iam_claim']\n",
    "test_iam_claim_cl = test_iam_claim[test_iam_claim['single_ans'] == 'Claim']\n",
    "test_iam_claim_nn_cl = test_iam_claim[\n",
    "    test_iam_claim['single_ans'] == 'Non-claim'\n",
    "]\n",
    "# slice ibm_claim\n",
    "train_ibm_claim = train_spl_cd[train_spl_cd['datasets'] == 'ibm_claim']\n",
    "train_ibm_claim_cl = train_ibm_claim[train_ibm_claim['single_ans'] == 'Claim']\n",
    "train_ibm_claim_nn_cl = train_ibm_claim[\n",
    "    train_ibm_claim['single_ans'] == 'Non-claim'\n",
    "]\n",
    "\n",
    "val_ibm_claim = val_spl_cd[val_spl_cd['datasets'] == 'ibm_claim']\n",
    "val_ibm_claim_cl = val_ibm_claim[val_ibm_claim['single_ans'] == 'Claim']\n",
    "val_ibm_claim_nn_cl = val_ibm_claim[val_ibm_claim['single_ans'] == 'Non-claim']\n",
    "\n",
    "test_ibm_claim = test_spl_cd[test_spl_cd['datasets'] == 'ibm_claim'] \n",
    "test_ibm_claim_cl = test_ibm_claim[test_ibm_claim['single_ans'] == 'Claim']\n",
    "test_ibm_claim_nn_cl = test_ibm_claim[\n",
    "    test_ibm_claim['single_ans'] == 'Non-claim'\n",
    "]\n",
    "#slice ibm_args\n",
    "train_ibm_args = train_spl_cd[train_spl_cd['datasets'] == 'ibm_args']\n",
    "train_ibm_args_cl = train_ibm_args[train_ibm_args['single_ans'] == 'Claim']\n",
    "train_ibm_args_nn_cl = train_ibm_args[\n",
    "    train_ibm_args['single_ans'] == 'Non-claim'\n",
    "]\n",
    "\n",
    "val_ibm_args = val_spl_cd[val_spl_cd['datasets'] == 'ibm_args']\n",
    "val_ibm_args_cl = val_ibm_args[val_ibm_args['single_ans'] == 'Claim']\n",
    "val_ibm_args_nn_cl = val_ibm_args[val_ibm_args['single_ans'] == 'Non-claim']\n",
    "\n",
    "test_ibm_args = test_spl_cd[test_spl_cd['datasets'] == 'ibm_args']\n",
    "test_ibm_args_cl = test_ibm_args[test_ibm_args['single_ans'] == 'Claim']\n",
    "test_ibm_args_nn_cl = test_ibm_args[test_ibm_args['single_ans'] == 'Non-claim']\n",
    "\n",
    "# Sanity Check\n",
    "# for iam claim\n",
    "assert(len(train_iam_claim) == (len(train_iam_claim_cl) + len(train_iam_claim_nn_cl)))\n",
    "assert(len(val_iam_claim) == (len(val_iam_claim_cl) + len(val_iam_claim_nn_cl)))\n",
    "assert(len(test_iam_claim) == (len(test_iam_claim_cl) + len(test_iam_claim_nn_cl)))\n",
    "# for ibm claim\n",
    "assert(len(train_ibm_claim) == (len(train_ibm_claim_cl) + len(train_ibm_claim_nn_cl)))\n",
    "assert(len(val_ibm_claim) == (len(val_ibm_claim_cl) + len(val_ibm_claim_nn_cl)))\n",
    "assert(len(test_ibm_claim) == (len(test_ibm_claim_cl) + len(test_ibm_claim_nn_cl)))\n",
    "# for ibm args\n",
    "assert(len(train_ibm_args) == (len(train_ibm_args_cl) + len(train_ibm_args_nn_cl)))\n",
    "assert(len(val_ibm_args) == (len(val_ibm_args_cl) + len(val_ibm_args_nn_cl)))\n",
    "assert(len(test_ibm_args) == (len(test_ibm_args_cl) + len(test_ibm_args_nn_cl)))\n",
    "\n",
    "# Display\n",
    "print(f'Iam Claim')\n",
    "print(f'Total Train: {len(train_iam_claim)} | Total Val: {len(val_iam_claim)} | Total Test: {len(test_iam_claim)}')\n",
    "print(f'Train Claim: {len(train_iam_claim_cl)} | Val Claim: {len(val_iam_claim_cl)} | Test Claim: {len(test_iam_claim_cl)}')\n",
    "print(f'Train Non-claim: {len(train_iam_claim_nn_cl)} | Val Non-claim: {len(val_iam_claim_nn_cl)} | Test Non-claim: {len(test_iam_claim_nn_cl)}')\n",
    "print(f'Ibm Claim')\n",
    "print(f'Total Train: {len(train_ibm_claim)} | Total Val: {len(val_ibm_claim)} | Total Test: {len(test_ibm_claim)}')\n",
    "print(f'Train Claim: {len(train_ibm_claim_cl)} | Val Claim: {len(val_ibm_claim_cl)} | Test Claim: {len(test_ibm_claim_cl)}')\n",
    "print(f'Train Non-claim: {len(train_ibm_claim_nn_cl)} | Val Non-claim: {len(val_ibm_claim_nn_cl)} | Test Non-claim: {len(test_ibm_claim_nn_cl)}')\n",
    "print(f'Ibm args')\n",
    "print(f'Total Train: {len(train_ibm_args)} | Total Val: {len(val_ibm_args)} | Total Test: {len(test_ibm_args)}')\n",
    "print(f'Train Claim: {len(train_ibm_args_cl)} | Val Claim: {len(val_ibm_args_cl)} | Test Claim: {len(test_ibm_args_cl)}')\n",
    "print(f'Train Non-claim: {len(train_ibm_args_nn_cl)} | Val Non-claim: {len(val_ibm_args_nn_cl)} | Test Non-claim: {len(test_ibm_args_nn_cl)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all sample into 3 distinct set : Train, Validation and Test\n",
    "import os\n",
    "path_to_sampled_data = './sampled_data'\n",
    "def load_all_sampled_csv(path_sample_dir):\n",
    "    train = []\n",
    "    val = []\n",
    "    test = []\n",
    "    for root, dir, files in os.walk(path_sample_dir):\n",
    "        for file in files:\n",
    "            if 'labels' not in file:\n",
    "                if 'train' in file:\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    train.append(df)\n",
    "                if 'val' in file:\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    val.append(df)\n",
    "                if 'test' in file:\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    test.append(df)\n",
    "    all_train = pd.concat(train)\n",
    "    all_val = pd.concat(val)\n",
    "    all_test = pd.concat(test)\n",
    "    return all_train, all_val, all_test\n",
    "\n",
    "train, val, test = load_all_sampled_csv(path_to_sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To csv the 3 distinct set\n",
    "train.to_csv('./sampled_data/all_spl_data/spl2_train.csv', index=False)\n",
    "val.to_csv('./sampled_data/all_spl_data/spl2_val.csv', index=False)\n",
    "test.to_csv('./sampled_data/all_spl_data/spl2_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cell for ollama all data inference\n",
    "# time = datetime.datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "# s_file = get_savefile(\n",
    "#     task_name='aduc',\n",
    "#     spl_name='spl2',\n",
    "#     m_name='Meta-Llama-3.1-8B-Instruct',\n",
    "#     n_sample=4000,\n",
    "#     epoch=2,\n",
    "#     train_resp=f'_ollama_{model_task_name}',\n",
    "#     outputs_dir=f'./outputs/test_aduc',\n",
    "#     time=time\n",
    "# )\n",
    "# print(f'##### Load Data')\n",
    "# labels, tr_d, val_d, test_d, change_lbl = get_data_for_task(\n",
    "#     task_name='aduc',\n",
    "#     s_file=s_file,\n",
    "# )\n",
    "# res = []\n",
    "# s = '<[|]ANSWER[|]>'\n",
    "# names_dataset = test_d['datasets']\n",
    "# true_labels = test_d['answer']\n",
    "# print(f'##### Start Inference')\n",
    "# for i in test_d['conversations'][:2]:\n",
    "#     print(i)\n",
    "#     # tmp = {'role': 'user', 'content': i[0].get('content') +'\\n' + i[1].get('content')}\n",
    "#     # print(tmp)\n",
    "#     response = chat(model='unsloth_model', messages=i)\n",
    "#     print(response['message']['content'])\n",
    "#     print(f'###############################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Parameters\\n\",\n",
    "# task = 'claim_detection'\n",
    "# task_data = 'claim_detection'\n",
    "# task_title = 'Claim Detection'\n",
    "# ckpt_name = checkpoint.get(task)\n",
    "# gguf_file = f'./gguf_model/{task}'\n",
    "# max_seq_lenght = 2048\n",
    "# dtype = None\n",
    "# load_in_4bit = True\n",
    "# gpu_mem_use = 0.6\n",
    "# system_prompt = d_sys_prt.get(task_data)\n",
    "# chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "# {SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "# {INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "# {OUTPUT}<|eot_id|>\"\"\"\n",
    "# time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "# s_file = get_savefile(\n",
    "#     task_name=task_data,\n",
    "#     spl_name='spl2',\n",
    "#     m_name='Meta-Llama-3.1-8B-Instruct',\n",
    "#     n_sample=4000,\n",
    "#     epoch=2,\n",
    "#     train_resp=f'_train_resp',\n",
    "#     outputs_dir=f'./outputs/test_{task}',\n",
    "#     time=time\n",
    "# )\n",
    "# match task_data:\n",
    "#     case 'aduc':\n",
    "#         labels, tr_d, val_d, test_d = aduc.get_data(s_file)\n",
    "#         change_lbl = aduc.change_lbl\n",
    "#     case 'claim_detection':\n",
    "#         labels, tr_d, val_d, test_d = cd.get_data(s_file)\n",
    "#         change_lbl = cd.change_lbl\n",
    "#     case 'evidence_detection':\n",
    "#         labels, tr_d, val_d, test_d = ed.get_data(s_file)\n",
    "#         change_lbl = ed.change_lbl\n",
    "#     case 'evidence_type':\n",
    "#         labels, tr_d, val_d, test_d = et.get_data(s_file)\n",
    "#         change_lbl = et.change_lbl\n",
    "#     case 'fallacies':\n",
    "#         labels, tr_d, val_d, test_d = fd.get_data(s_file)\n",
    "#         change_lbl = fd.change_lbl\n",
    "#     case 'relation':\n",
    "#         labels, tr_d, val_d, test_d = arc.get_data(s_file)\n",
    "#         change_lbl = arc.change_lbl\n",
    "#     case 'stance_detection':\n",
    "#         labels, tr_d, val_d, test_d = sd.get_data(s_file)\n",
    "#         change_lbl = sd.change_lbl\n",
    "#     case 'quality':\n",
    "#         labels, tr_d, val_d, test_d = aq.get_data(s_file)\n",
    "#         change_lbl = aq.change_lbl\n",
    "# # labels, tr_d, val_d, test_d = get_data(s_file)\n",
    "# print(system_prompt)\n",
    "# print(ckpt_name)\n",
    "# print(gguf_file)\n",
    "# print(change_lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Load Model\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=ckpt_name,\n",
    "#     max_seq_length=max_seq_lenght,\n",
    "#     dtype=dtype,\n",
    "#     load_in_4bit=load_in_4bit,\n",
    "#     fast_inference=True,\n",
    "#     gpu_memory_utilization=gpu_mem_use\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Get Datasets\n",
    "# train_set, val_set, test_set = get_datasets(\n",
    "#     tokenizer=tokenizer,\n",
    "#     train=tr_d,\n",
    "#     val=val_d,\n",
    "#     test=test_d, \n",
    "#     chat_template=chat_template,\n",
    "#     sys_prt=system_prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on fewer data\n",
    "# from datasets import Dataset\n",
    "# tmp_set = test_set[:10]\n",
    "# tmp_set = pd.DataFrame().from_records(tmp_set)\n",
    "# tmp_set = Dataset.from_pandas(tmp_set)\n",
    "# tmp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Run On test_set\n",
    "# from src.training import test\n",
    "# res_test_chpt = test(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_test=test_set,\n",
    "#     labels=labels,\n",
    "# )\n",
    "# metric, metric_m = get_metrics(change_lbl, res_test_chpt)\n",
    "# metric, _ = get_metrics(change_lbl, res_test_chpt, is_multi_lbl=False)\n",
    "# plot_metric(\n",
    "#     metric=metric,\n",
    "    # title=f'{task_title}: Scores ckpt{nbckpt}',\n",
    "    # file_plot=f'./img/{task}/scores_ckpt{nbckpt}_metric_single.png',\n",
    "    # file_metric=f'./test_res/{task}/scores_ckpt{nbckpt}_metric_single.csv'\n",
    "# )\n",
    "# plot_metric(\n",
    "#     metric=metric_m\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\n",
    "#     './saved_model/claim_detection',\n",
    "#     tokenizer,\n",
    "#     save_method=\"merged_16bit\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\n",
    "#     './saved_model/claim_detection/saved_lora',\n",
    "#     tokenizer,\n",
    "#     save_method=\"lora\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_gguf(\n",
    "#     gguf_file,\n",
    "#     tokenizer,\n",
    "#     quantization_method = \"q8_0\", # choose quant method: q8_0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ollama import chat\n",
    "\n",
    "# res = []\n",
    "# s = '<[|]ANSWER[|]>'\n",
    "# quant = 'q8_0'\n",
    "# names_dataset = test_d['datasets']\n",
    "# true_labels = test_d['answer']\n",
    "# # print(len(test_d['conversations']))\n",
    "# for i in test_d['conversations']:\n",
    "#     response = chat(model='unsloth_model', messages=[i[1]])\n",
    "#     print(response['message']['content'])\n",
    "#     tmp = re.split(s, response['message']['content'])\n",
    "#     res.append(tmp[1])\n",
    "# d_res = {'names': names_dataset, 'pred': res, 'lbl': true_labels}\n",
    "# df_res = pd.DataFrame(data=d_res)\n",
    "# df_res.to_csv(f'./test_res/{task}/test_result_gguf_{quant}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_single, metric_multi = get_metrics(change_lbl, df_res)\n",
    "# metric_single, _ = get_metrics(change_lbl, df_res, is_multi_lbl=False)\n",
    "# plot_metric(\n",
    "#     metric=metric_single,\n",
    "    # title=f'{task_title}: Score gguf_{quant} single label',\n",
    "    # file_plot=f'./img/{task}/scores_gguf_{quant}_metric_single.png',\n",
    "    # file_metric=f'./test_res/{task}/scores_gguf_{quant}_metric_single.csv'\n",
    "# )\n",
    "# For Fallacies Task Only\n",
    "# plot_metric(\n",
    "#     metric=metric_multi,\n",
    "#     title=f'{task_title}: Score gguf_{quant} multi label',\n",
    "#     file_plot=f'./img/{task}/scores_gguf_{quant}_metric_multi.png',\n",
    "#     file_metric=f'./test_res/{task}/scores_gguf_{quant}_metric_multi.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen via VLLM\n",
    "# def gen(txt, model, sampling_params):\n",
    "#     output = model.fast_generate(\n",
    "#         txt,\n",
    "#         sampling_params = sampling_params,\n",
    "#     )[0].outputs[0].text\n",
    "    \n",
    "#     return output\n",
    "\n",
    "# def format_output(answer: str, fallacies: set) -> list:\n",
    "#     s = '<[|]ANSWER[|]>'\n",
    "#     tmp = re.split(s, answer)\n",
    "#     pred= [i for i in tmp if i in fallacies]\n",
    "#     return pred\n",
    "\n",
    "# def zero_shot_gen(data: list[str], model, fallacies: set, sampling_params) -> list:\n",
    "#     res = []\n",
    "#     for i in data:\n",
    "#         out = gen(i, model, sampling_params)\n",
    "#         pred = format_output(out, fallacies)\n",
    "#         res.append(pred)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class custom_validation_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies, n_step=10):\n",
    "#         super().__init__()\n",
    "#         self.val_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#         self.n_step=n_step\n",
    "#     def on_step_end(self, args, state, control, **kwargs):\n",
    "#         if state.global_step % self.n_step == 0 and state.global_step > 0 :\n",
    "#             model.save_lora('sft_save_lora')\n",
    "#             FastLanguageModel.for_inference(model)\n",
    "#             pred = zero_shot_gen(\n",
    "#                 data=self.val_dataset['text'],\n",
    "#                 model=model,\n",
    "#                 fallacies=self.fallacies,\n",
    "#                 sampling_params=self.sampling_params\n",
    "#             )\n",
    "#             tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#             d = pd.DataFrame().from_records(tmp_pred)\n",
    "#             d['truth_label'] = self.val_dataset['answer']\n",
    "#             d['step'] = np.full((len(d['truth_label']),), state.global_step)\n",
    "#             try:\n",
    "#                 d.to_csv(\n",
    "#                     './validation_res.csv',\n",
    "#                     index=False,\n",
    "#                     mode='a',\n",
    "#                     header=['pred', 'truth_label', 'step']\n",
    "#                 )\n",
    "#             except FileNotFoundError:\n",
    "#                 d.to_csv('./validation_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_step_end(args, state, control, **kwargs)\n",
    "\n",
    "# class custom_test_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies):\n",
    "#         super().__init__()\n",
    "#         self.test_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#     def on_train_end(self, args, state, control, **kwargs):\n",
    "#         model.save_lora('sft_save_lora')\n",
    "#         FastLanguageModel.for_inference(model)\n",
    "#         pred = zero_shot_gen(\n",
    "#             data=self.test_dataset['text'],\n",
    "#             model=model,\n",
    "#             fallacies=self.fallacies,\n",
    "#             sampling_params=self.sampling_params\n",
    "#         )\n",
    "#         tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#         d = pd.DataFrame().from_records(tmp_pred)\n",
    "#         d['truth_label'] = self.test_dataset['answer']\n",
    "#         try:\n",
    "#             d.to_csv('./test_res.csv', index=False, mode='a', header=['pred','truth_label'])\n",
    "#         except FileNotFoundError:\n",
    "#             d.to_csv('./test_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_train_end(args, state, control, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
