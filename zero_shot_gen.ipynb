{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hsavi\\anaconda3\\envs\\stage_env\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3070 Laptop GPU. Num GPUs = 1. Max memory: 8.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.17 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name= \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from sampling.sampling_fallacies_detection import load_all_dataset, sample_data, format_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght: 300\n"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    'cocolofa': './Data_jsonl/cocolofa.jsonl',\n",
    "    'mafalda': './Data_jsonl/mafalda.jsonl'\n",
    "}\n",
    "data = load_all_dataset(paths)\n",
    "sampled_data, all_labels = sample_data(data)\n",
    "cpt = 0\n",
    "for k,v in sampled_data.items():\n",
    "    for i in v:\n",
    "        cpt += 1\n",
    "print(f'lenght: {cpt}')\n",
    "prt = format_prompt(sampled_data, all_labels)\n",
    "df = pd.DataFrame().from_records(prt)\n",
    "# for i in prt:\n",
    "#     for p in i.get('prompt'):\n",
    "#         print(p)\n",
    "#     print('-----')\n",
    "#     print(i.get('answer'))\n",
    "#     print('=====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a47172bf284243a070c6c1a0b5098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['datasets', 'prompt', 'answer', 'text'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "data = Dataset.from_pandas(df)\n",
    "# tok = get_chat_template(\n",
    "#     tokenizer=tokenizer,\n",
    "#     chat_template='llama',\n",
    "# )\n",
    "def formatting_prompt(ex):\n",
    "    text = tokenizer.apply_chat_template(ex.get('prompt'),tokenize = False, add_generation_prompt = False)\n",
    "    return { 'text': text, }\n",
    "\n",
    "data1 = data.map(formatting_prompt, batched=True)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are an expert in argumentation. Your task is to determine the type of fallacy in the given Sentence. The fallacy would be in the Fallacy Set. Utilize the Title and the Full Text as context to support your decision and provide an explanation of the reasoning behind your determination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Fallacy: {'straw man', 'appeal to ridicule','slippery slope', 'appeal to majority', 'guilt by association', 'none', 'equivocation', 'false causality', 'appeal to tradition', 'hasty generalization', 'false analogy', 'ad populum', 'causal oversimplification', 'circular reasoning', 'appeal to authority', 'ad hominem', 'appeal to worse problems', 'appeal to fear', 'false dilemma', 'appeal to nature'}\n",
      "Title: Malaysian lawyers behind â€˜walk for judicial independenceâ€™ facing police probe\n",
      "Sentence:  Once investigations happen like this for little to no reason, it will continue. Its only a matter on time before more happen. I fear for the freedom in the future.\n",
      "Full text: It is natural to think that something is wrong with this investigation. However we should let the investigation play out to see if there is actually any wrongdoing. Let us try to trust the authorities and the process that is involved. Once investigations happen like this for little to no reason, it will continue. Its only a matter on time before more happen. I fear for the freedom in the future.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The fallacy in the given sentence is slippery slope'. \n",
      "\n",
      "Explanation: A slippery slope fallacy occurs when an argument assumes that a relatively small initial action will inevitably lead to a series of extreme consequences without providing any evidence for this chain of events. In this sentence, the speaker claims that if investigations happen for little to no reason, it will continue and eventually lead to a loss of freedom. However, there is no logical connection provided between the initial action (investigations) and the extreme consequence (loss of freedom). This is an example of a slippery slope fallacy because it assumes a chain of events without providing any evidence\n",
      "[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are an expert in argumentation. Your task is to determine the type of fallacy in the given Sentence. The fallacy would be in the Fallacy Set. Utilize the Title and the Full Text as context to support your decision and provide an explanation of the reasoning behind your determination.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nFallacy: {'straw man', 'appeal to ridicule','slippery slope', 'appeal to majority', 'guilt by association', 'none', 'equivocation', 'false causality', 'appeal to tradition', 'hasty generalization', 'false analogy', 'ad populum', 'causal oversimplification', 'circular reasoning', 'appeal to authority', 'ad hominem', 'appeal to worse problems', 'appeal to fear', 'false dilemma', 'appeal to nature'}\\nTitle: Malaysian lawyers behind â€˜walk for judicial independenceâ€™ facing police probe\\nSentence:  Once investigations happen like this for little to no reason, it will continue. Its only a matter on time before more happen. I fear for the freedom in the future.\\nFull text: It is natural to think that something is wrong with this investigation. However we should let the investigation play out to see if there is actually any wrongdoing. Let us try to trust the authorities and the process that is involved. Once investigations happen like this for little to no reason, it will continue. Its only a matter on time before more happen. I fear for the freedom in the future.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe fallacy in the given sentence is'slippery slope'. \\n\\nExplanation: A slippery slope fallacy occurs when an argument assumes that a relatively small initial action will inevitably lead to a series of extreme consequences without providing any evidence for this chain of events. In this sentence, the speaker claims that if investigations happen for little to no reason, it will continue and eventually lead to a loss of freedom. However, there is no logical connection provided between the initial action (investigations) and the extreme consequence (loss of freedom). This is an example of a slippery slope fallacy because it assumes a chain of events without providing any evidence\"]\n",
      "slippery slope\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "# [0].outputs[0].text\n",
    "FastLanguageModel.for_inference(model)\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "def gen(msg, model):\n",
    "    txt = tokenizer.apply_chat_template(\n",
    "        msg,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "    output = model.generate(\n",
    "        txt,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=128,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output\n",
    "\n",
    "msg_test = prt[0].get('prompt')\n",
    "ground = prt[0].get('answer')\n",
    "output = gen(msg_test, model)\n",
    "\n",
    "print(tokenizer.batch_decode(output))\n",
    "print(ground)\n",
    "# txt = tokenizer.apply_chat_template(\n",
    "#     msg_test,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to('cuda')\n",
    "# out = model.generate(\n",
    "#     txt,\n",
    "#     streamer=text_streamer,\n",
    "#     max_new_tokens=128,\n",
    "#     pad_token_id=tokenizer.eos_token_id\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are an expert in argumentation. Your task is to determine the relation from #Argument 1 to Argument 2 as either \"attack\" or \"support\". That is, does #Argument 1 attack or support Argument 2 ?\n",
    "#\n",
    "#Think about the reasoning process first and then give an answer.\n",
    "#Respond in the following format with your thought process in the reasoning #section and only attack or support in the answer section:\n",
    "#<reasoning>\n",
    "#...\n",
    "#</reasoning>\n",
    "#<answer>\n",
    "#...\n",
    "#</answer>\n",
    "#\"\"\"\n",
    "#\n",
    "#def format_user_prompt(arg1, arg2):\n",
    "#    prompt = \"\"\n",
    "#    prompt += \"Argument 1 : \"+arg1+\"\\n\"\n",
    "#    prompt += \"Argument 2 : \"+arg2+\"\\n\"\n",
    "#    return prompt\n",
    "#\n",
    "#def get_dataset_prompts(split = \"train\") -> Dataset:\n",
    "#    if split == \"train\":\n",
    "#        data = load_dataset(\"csv\", data_files='kialoPairs.csv', split='train#[:80%]') # type: ignore\n",
    "#    elif split == \"test\":\n",
    "#        data = load_dataset(\"csv\", data_files='kialoPairs.csv', split='train#[91%:92%]') # type: ignore\n",
    "#    elif split == \"val\":\n",
    "#        # Load full dataset\n",
    "#        data = load_dataset(\"csv\", data_files='kialoPairs.csv', split='train')\n",
    "#        # Select 100 rows starting at around 90'th % index\n",
    "#        start_idx = int(0.90 * len(data))\n",
    "#        offset = -14\n",
    "#        data = data.select(range(start_idx+offset, min(start_idx + 100, len#(data))+offset))\n",
    "#    data = data.map(lambda x: { # type: ignore\n",
    "#        'prompt': [\n",
    "#            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "#            {'role': 'user', 'content': format_user_prompt(x[\"argSrc\"], x#[\"argTrg\"])}\n",
    "#        ],\n",
    "#        'answer': x['relation']\n",
    "#    }) # type: ignore\n",
    "#    return data # type: ignore\n",
    "#\n",
    "#from vllm import SamplingParams\n",
    "#MAX_COMPLETION_LENGTH = 512\n",
    "#\n",
    "#SAMPLING_PARAMS = SamplingParams(\n",
    "#    temperature = 0.8,\n",
    "#    top_p = 0.95,\n",
    "#    max_tokens = MAX_COMPLETION_LENGTH,\n",
    "#)\n",
    "#\n",
    "#def query_llm_classification(arg1:str, arg2:str, sampling_params = #SAMPLING_PARAMS):\n",
    "#    \"\"\"\n",
    "#    Queries the language model with the task specific prompt (a pair of #arguments) and returns the extracted classification answer\n",
    "#    \"\"\"\n",
    "#    text = tokenizer.apply_chat_template([\n",
    "#        {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "#        {\"role\" : \"user\", \"content\" : format_user_prompt(arg1, arg2)},\n",
    "#    ], tokenize = False, add_generation_prompt = True)\n",
    "#\n",
    "#    output = model.fast_generate(\n",
    "#        text,\n",
    "#        sampling_params = sampling_params,\n",
    "#        lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    "#    )[0].outputs[0].text\n",
    "#\n",
    "#    # The output is valid (i.e. parsable) if both sets of tags are present in #the response\n",
    "#    valid_extract = (output.count('<answer>') + output.count('</answer>') == 2) #and (output.count('<reasoning>') + output.count('</reasoning>') == 2)\n",
    "#\n",
    "#    return valid_extract, extract_xml_answer(output), extract_xml_reasoning(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
