{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from ollama import chat\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import src.inference as inference\n",
    "from src.utils import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample for cd\n",
    "train_spl_cd = pd.read_csv('./sampled_data/claim_detection/spl2_train.csv')\n",
    "val_spl_cd = pd.read_csv('./sampled_data/claim_detection/spl2_val.csv')\n",
    "test_spl_cd = pd.read_csv('./sampled_data/claim_detection/spl2_test.csv')\n",
    "\n",
    "# slice iam_claim\n",
    "train_iam_claim  = train_spl_cd[train_spl_cd['datasets'] == 'iam_claim']\n",
    "train_iam_claim_cl = train_iam_claim[train_iam_claim['single_ans'] == 'Claim']\n",
    "train_iam_claim_nn_cl = train_iam_claim[\n",
    "    train_iam_claim['single_ans'] == 'Non-claim'\n",
    "]\n",
    "\n",
    "val_iam_claim = val_spl_cd[val_spl_cd['datasets'] == 'iam_claim']\n",
    "val_iam_claim_cl = val_iam_claim[val_iam_claim['single_ans'] == 'Claim']\n",
    "val_iam_claim_nn_cl = val_iam_claim[val_iam_claim['single_ans'] == 'Non-claim']\n",
    "\n",
    "test_iam_claim = test_spl_cd[test_spl_cd['datasets'] == 'iam_claim']\n",
    "test_iam_claim_cl = test_iam_claim[test_iam_claim['single_ans'] == 'Claim']\n",
    "test_iam_claim_nn_cl = test_iam_claim[\n",
    "    test_iam_claim['single_ans'] == 'Non-claim'\n",
    "]\n",
    "# slice ibm_claim\n",
    "train_ibm_claim = train_spl_cd[train_spl_cd['datasets'] == 'ibm_claim']\n",
    "train_ibm_claim_cl = train_ibm_claim[train_ibm_claim['single_ans'] == 'Claim']\n",
    "train_ibm_claim_nn_cl = train_ibm_claim[\n",
    "    train_ibm_claim['single_ans'] == 'Non-claim'\n",
    "]\n",
    "\n",
    "val_ibm_claim = val_spl_cd[val_spl_cd['datasets'] == 'ibm_claim']\n",
    "val_ibm_claim_cl = val_ibm_claim[val_ibm_claim['single_ans'] == 'Claim']\n",
    "val_ibm_claim_nn_cl = val_ibm_claim[val_ibm_claim['single_ans'] == 'Non-claim']\n",
    "\n",
    "test_ibm_claim = test_spl_cd[test_spl_cd['datasets'] == 'ibm_claim'] \n",
    "test_ibm_claim_cl = test_ibm_claim[test_ibm_claim['single_ans'] == 'Claim']\n",
    "test_ibm_claim_nn_cl = test_ibm_claim[\n",
    "    test_ibm_claim['single_ans'] == 'Non-claim'\n",
    "]\n",
    "#slice ibm_args\n",
    "train_ibm_args = train_spl_cd[train_spl_cd['datasets'] == 'ibm_args']\n",
    "train_ibm_args_cl = train_ibm_args[train_ibm_args['single_ans'] == 'Claim']\n",
    "train_ibm_args_nn_cl = train_ibm_args[\n",
    "    train_ibm_args['single_ans'] == 'Non-claim'\n",
    "]\n",
    "\n",
    "val_ibm_args = val_spl_cd[val_spl_cd['datasets'] == 'ibm_args']\n",
    "val_ibm_args_cl = val_ibm_args[val_ibm_args['single_ans'] == 'Claim']\n",
    "val_ibm_args_nn_cl = val_ibm_args[val_ibm_args['single_ans'] == 'Non-claim']\n",
    "\n",
    "test_ibm_args = test_spl_cd[test_spl_cd['datasets'] == 'ibm_args']\n",
    "test_ibm_args_cl = test_ibm_args[test_ibm_args['single_ans'] == 'Claim']\n",
    "test_ibm_args_nn_cl = test_ibm_args[test_ibm_args['single_ans'] == 'Non-claim']\n",
    "\n",
    "# Sanity Check\n",
    "# for iam claim\n",
    "assert(len(train_iam_claim) == (len(train_iam_claim_cl) + len(train_iam_claim_nn_cl)))\n",
    "assert(len(val_iam_claim) == (len(val_iam_claim_cl) + len(val_iam_claim_nn_cl)))\n",
    "assert(len(test_iam_claim) == (len(test_iam_claim_cl) + len(test_iam_claim_nn_cl)))\n",
    "# for ibm claim\n",
    "assert(len(train_ibm_claim) == (len(train_ibm_claim_cl) + len(train_ibm_claim_nn_cl)))\n",
    "assert(len(val_ibm_claim) == (len(val_ibm_claim_cl) + len(val_ibm_claim_nn_cl)))\n",
    "assert(len(test_ibm_claim) == (len(test_ibm_claim_cl) + len(test_ibm_claim_nn_cl)))\n",
    "# for ibm args\n",
    "assert(len(train_ibm_args) == (len(train_ibm_args_cl) + len(train_ibm_args_nn_cl)))\n",
    "assert(len(val_ibm_args) == (len(val_ibm_args_cl) + len(val_ibm_args_nn_cl)))\n",
    "assert(len(test_ibm_args) == (len(test_ibm_args_cl) + len(test_ibm_args_nn_cl)))\n",
    "\n",
    "# Display\n",
    "print(f'Iam Claim')\n",
    "print(f'Total Train: {len(train_iam_claim)} | Total Val: {len(val_iam_claim)} | Total Test: {len(test_iam_claim)}')\n",
    "print(f'Train Claim: {len(train_iam_claim_cl)} | Val Claim: {len(val_iam_claim_cl)} | Test Claim: {len(test_iam_claim_cl)}')\n",
    "print(f'Train Non-claim: {len(train_iam_claim_nn_cl)} | Val Non-claim: {len(val_iam_claim_nn_cl)} | Test Non-claim: {len(test_iam_claim_nn_cl)}')\n",
    "print(f'Ibm Claim')\n",
    "print(f'Total Train: {len(train_ibm_claim)} | Total Val: {len(val_ibm_claim)} | Total Test: {len(test_ibm_claim)}')\n",
    "print(f'Train Claim: {len(train_ibm_claim_cl)} | Val Claim: {len(val_ibm_claim_cl)} | Test Claim: {len(test_ibm_claim_cl)}')\n",
    "print(f'Train Non-claim: {len(train_ibm_claim_nn_cl)} | Val Non-claim: {len(val_ibm_claim_nn_cl)} | Test Non-claim: {len(test_ibm_claim_nn_cl)}')\n",
    "print(f'Ibm args')\n",
    "print(f'Total Train: {len(train_ibm_args)} | Total Val: {len(val_ibm_args)} | Total Test: {len(test_ibm_args)}')\n",
    "print(f'Train Claim: {len(train_ibm_args_cl)} | Val Claim: {len(val_ibm_args_cl)} | Test Claim: {len(test_ibm_args_cl)}')\n",
    "print(f'Train Non-claim: {len(train_ibm_args_nn_cl)} | Val Non-claim: {len(val_ibm_args_nn_cl)} | Test Non-claim: {len(test_ibm_args_nn_cl)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training, Test and saving \n",
    "# model, tokenizer = run('mt_ft', do_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_savefile, get_templates\n",
    "from src.prompting import get_datasets\n",
    "from src.metrics import get_metrics\n",
    "from src.plot import plot_metric\n",
    "from src.training import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.aduc as aduc\n",
    "import src.claim_detect as cd\n",
    "import src.evidence_detect as ed\n",
    "import src.evidence_type as et\n",
    "import src.fallacies as fd\n",
    "import src.quality as aq\n",
    "import src.relation as arc\n",
    "import src.stance_detect as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all sample into 3 distinct set : Train, Validation and Test\n",
    "import os\n",
    "path_to_sampled_data = './sampled_data'\n",
    "def load_all_sampled_csv(path_sample_dir):\n",
    "    train = []\n",
    "    val = []\n",
    "    test = []\n",
    "    for root, dir, files in os.walk(path_sample_dir):\n",
    "        for file in files:\n",
    "            if 'labels' not in file:\n",
    "                if 'train' in file:\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    train.append(df)\n",
    "                if 'val' in file:\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    val.append(df)\n",
    "                if 'test' in file:\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    test.append(df)\n",
    "    all_train = pd.concat(train)\n",
    "    all_val = pd.concat(val)\n",
    "    all_test = pd.concat(test)\n",
    "    return all_train, all_val, all_test\n",
    "\n",
    "train, val, test = load_all_sampled_csv(path_to_sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To csv the 3 distinct set\n",
    "train.to_csv('./sampled_data/all_spl_data/spl2_train.csv', index=False)\n",
    "val.to_csv('./sampled_data/all_spl_data/spl2_val.csv', index=False)\n",
    "test.to_csv('./sampled_data/all_spl_data/spl2_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint, model and system prompt\n",
    "\n",
    "checkpoint = {\n",
    "    'aduc': f'./outputs/aduc/2025-05-07_17_29_07_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'claim_detection': f'./outputs/claim_detection/2025-05-09_14_57_11_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'evidence_detection': f'./outputs/evidence_detection/2025-05-09_15_51_24_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'evidence_type': f'./outputs/evidence_type/2025-05-12_10_47_06_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'fallacies': f'./outputs/fallacies/2025-05-09_16_41_44_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-340',\n",
    "    'relation': f'./outputs/relation/2025-05-09_18_41_38_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'stance_detection': f'./outputs/stance_detection/2025-05-15_09_50_35_Meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'quality': f'./outputs/quality/2025-05-12_09_44_16_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "}\n",
    "st_model = {\n",
    "    'aduc': f'./gguf_model/aduc',\n",
    "    'claim_detection': f'./gguf_model/claim_detection',\n",
    "    'evidence_detection': f'./gguf_model/evidence_detection',\n",
    "    'evidence_type': f'./gguf_model/evidence_type',\n",
    "    'fallacies': f'./gguf_model/fallacies',\n",
    "    'relation': f'./saved_model/relation',\n",
    "    'stance_detection': f'./gguf_model/stance_detection',\n",
    "    'quality': f'./gguf_model/quality',\n",
    "    'mt_ft': f'./gguf_model/mt_ft',\n",
    "}\n",
    "merge_model = {\n",
    "    'conf_d05_w0125': f'./merged_model/conf_d05_w0125',\n",
    "    'conf_d07_w0125': f'./merged_model/conf_d07_w0125',\n",
    "    'conf_d08_w0125': f'./merged_model/conf_d08_w0125',\n",
    "    'conf_hd_w0125': f'./merged_model/conf_hd_w0125',\n",
    "    'conf_ld_w0125': f'./merged_model/conf_ld_w0125',\n",
    "    'conf_dare_5': f'./merged_model/conf_dare_5',\n",
    "    'conf_dare_6': f'./merged_model/conf_dare_6',\n",
    "    'conf_aduc_cd_ed': f'./merged_model/conf_aduc_cd_ed',\n",
    "    'conf_della_1': f'./merged_model/conf_della_1',\n",
    "    'conf_della_2': f'./merged_model/conf_della_2',\n",
    "    'conf_della_3': f'./merged_model/conf_della_3',\n",
    "    'conf_della_4': f'./merged_model/conf_della_4',\n",
    "    'conf_della_5': f'./merged_model/conf_della_5',\n",
    "    'conf_della_6': f'./merged_model/conf_della_6',\n",
    "    'conf_brcrumb_1': f'./merged_model/conf_brcrumb_1',\n",
    "    'conf_brcrumb_2': f'./merged_model/conf_brcrumb_2',\n",
    "}\n",
    "d_sys_prt = {\n",
    "    'aduc': f'You are an expert in argumentation. Your task is to determine whether the given [SENTENCE] is a Claim or a Premise. Utilize the [TOPIC] and the [FULL TEXT] as context to support your decision\\nYour answer must be in the following format with only Claim or Premise in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'claim_detection': f'You are an expert in argumentation. Your task is to determiner whether the given [SENTENCE] is a Claim or Non-claim. Utilize the [TOPIC] and the [FULL TEXT] as context to support your decision\\nYour answer must be in the following format with only Claim or Non-claim in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'evidence_detection': f'You are an expert in argumentation. Your task is to determine whether the given [SENTENCE] is an Evidence or Non-evidence. Utilize the [TOPIC] and the [ARGUMENT] as context to support your decision\\nYour answer must be in the following format with only Evidence or Non-evidence in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'evidence_type': f'You are an expert in argumentation. Your task is to determine the type of evidence of the given [SENTENCE]. The type of evidence would be in the [TYPE] set. Utilize the [TOPIC] and the [CLAIM] as context to support your decision\\nYour answer must be in the following format with only the type of evidence in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'fallacies': f'You are an expert in argumentation. Your task is to determine the type of fallacy in the given [SENTENCE]. The fallacy would be in the [FALLACY] Set. Utilize the [TITLE] and the [FULL TEXT] as context to support your decision.\\nYour answer must be in the following format with only the fallacy in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'relation': f'You are an expert in argumentation. Your task is to determine the type of relation between [SOURCE] and [TARGET]. The type of relation would be in the [RELATION] set. Utilize the [TOPIC] as context to support your decision\\nYour answer must be in the following format with only the type of the relation in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'stance_detection': f'You are an expert in argumentation. Your task is to determine whether the given [SENTENCE] is For or Against. Utilize the [TOPIC] as context to support your decision\\nYour answer must be in the following format with only For or Against in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'quality': f'You are an expert in argumentation. Your task is to determine the quality of the [SENTENCE]. The quality would be in the [QUALITY] set. Utilize the [TOPIC], the [STANCE] and the [DEFINITION] as context to support your decision\\nYour answer must be in the following format with only the quality in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_lenght = 2048\n",
    "dtype = None\n",
    "load_in_4bit = False\n",
    "gpu_mem_use = 0.6\n",
    "# To modify to load the correct model\n",
    "model_task_name = 'mt_ft'\n",
    "# Load from checkpoint\n",
    "# task_model_name = checkpoint.get(model_task_name)\n",
    "# Load from safetensors\n",
    "task_model_name = st_model.get(model_task_name)\n",
    "# Load Merged Model\n",
    "# task_model_name = merge_model.get(model_task_name)\n",
    "# Zero-shot / Few-shot:\n",
    "# task_model_name = f'unsloth/Meta-Llama-3.1-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Tokenizer\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=task_model_name,\n",
    "    max_seq_length=max_seq_lenght,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=gpu_mem_use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_result = inference.inference_on_all_data(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     model_task_name=model_task_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = datetime.datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "# s_file = get_savefile(\n",
    "#         task_name='aduc',\n",
    "#         spl_name='spl2',\n",
    "#         m_name='Meta-Llama-3.1-8B-Instruct',\n",
    "#         n_sample=4000,\n",
    "#         epoch=2,\n",
    "#         train_resp=f'_tmp',\n",
    "#         outputs_dir=f'./outputs/test_tmp',\n",
    "#         time=time\n",
    "#     )\n",
    "# labels, tr_d, val_d, test_d = aduc.get_data(s_file)\n",
    "# chat_template = get_templates()\n",
    "# train_set, val_set, test_set = get_datasets(\n",
    "#         tokenizer=tokenizer,\n",
    "#         train=tr_d,\n",
    "#         val=val_d,\n",
    "#         test=test_d,\n",
    "#         chat_template=chat_template,\n",
    "#         sys_prt=\"\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_gguf(\n",
    "#     './gguf_model/merged_gguf',\n",
    "#     tokenizer,\n",
    "#     quantization_method = \"q8_0\", # choose quant method: q8_0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def get_data_for_task(\n",
    "    task_name:str,\n",
    "    s_file:dict,\n",
    "):\n",
    "    match task_name:\n",
    "        case 'aduc':\n",
    "            labels, tr_d, val_d, test_d = aduc.get_data(s_file)\n",
    "            change_lbl = aduc.change_lbl\n",
    "        case 'claim_detection':\n",
    "            labels, tr_d, val_d, test_d = cd.get_data(s_file)\n",
    "            change_lbl = cd.change_lbl\n",
    "        case 'evidence_detection':\n",
    "            labels, tr_d, val_d, test_d = ed.get_data(s_file)\n",
    "            change_lbl = ed.change_lbl\n",
    "        case 'evidence_type':\n",
    "            labels, tr_d, val_d, test_d = et.get_data(s_file)\n",
    "            change_lbl = et.change_lbl\n",
    "        case 'fallacies':\n",
    "            labels, tr_d, val_d, test_d = fd.get_data(s_file)\n",
    "            change_lbl = fd.change_lbl\n",
    "        case 'relation':\n",
    "            labels, tr_d, val_d, test_d = arc.get_data(s_file)\n",
    "            change_lbl = arc.change_lbl\n",
    "        case 'stance_detection':\n",
    "            labels, tr_d, val_d, test_d = sd.get_data(s_file)\n",
    "            change_lbl = sd.change_lbl\n",
    "        case 'quality':\n",
    "            labels, tr_d, val_d, test_d = aq.get_data(s_file)\n",
    "            change_lbl = aq.change_lbl\n",
    "    return labels, tr_d, val_d, test_d, change_lbl\n",
    "\n",
    "def get_dataset_for_task(\n",
    "    task_name:str,\n",
    "    tokenizer,\n",
    "    s_file:dict,\n",
    "    system_prompt:str,\n",
    "    chat_template:str,\n",
    "):\n",
    "    labels, tr_d, val_d, test_d, change_lbl = get_data_for_task(\n",
    "        task_name=task_name,\n",
    "        s_file=s_file\n",
    "    )\n",
    "    train_set, val_set, test_set = get_datasets(\n",
    "        tokenizer=tokenizer,\n",
    "        train=tr_d,\n",
    "        val=val_d,\n",
    "        test=test_d,\n",
    "        chat_template=chat_template,\n",
    "        sys_prt=system_prompt\n",
    "    )\n",
    "    # tmp_set = test_set[:10]\n",
    "    # tmp_set = pd.DataFrame().from_records(tmp_set)\n",
    "    # tmp_set = Dataset.from_pandas(tmp_set)\n",
    "    return labels, train_set, val_set, test_set, change_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Function\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "def inference_unsloth(task_name:str, model, tokenizer):\n",
    "    print(f'#### Inference on {task_name}')\n",
    "    chat_template = get_templates()\n",
    "    system_prompt = d_sys_prt.get(task_name)\n",
    "    time = datetime.datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "    s_file = get_savefile(\n",
    "        task_name=task_name,\n",
    "        spl_name='spl2',\n",
    "        m_name='Meta-Llama-3.1-8B-Instruct',\n",
    "        n_sample=4000,\n",
    "        epoch=2,\n",
    "        train_resp=f'_all_data_{model_task_name}',\n",
    "        outputs_dir=f'./outputs/{task_name}',\n",
    "        time=time\n",
    "    )\n",
    "    print(f'##### Load Data')\n",
    "    labels, train_set, val_set, test_set, change_lbl = get_dataset_for_task(\n",
    "        task_name=task_name,\n",
    "        tokenizer=tokenizer,\n",
    "        s_file=s_file,\n",
    "        system_prompt=system_prompt,\n",
    "        chat_template=chat_template\n",
    "    )\n",
    "    print(f'##### Start inference')\n",
    "    test_result = test(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_test=test_set,\n",
    "        labels=labels,\n",
    "        result_file=s_file.get('test_result_file')\n",
    "    )\n",
    "    print(f'##### Metrics')\n",
    "    if task_name == 'fallacies':\n",
    "        metric_single, metric_multi = get_metrics(change_lbl, test_result)\n",
    "        plot_metric(\n",
    "            metric=metric_single,\n",
    "            title=f'Scores single for {task_name}',\n",
    "            file_metric=s_file.get('metric_single')\n",
    "        )\n",
    "        plot_metric(\n",
    "            metric=metric_multi,\n",
    "            title=f'Scores Multi for {task_name}',\n",
    "            file_metric=s_file.get('metric_multi')\n",
    "        )\n",
    "        return (metric_single, metric_multi)\n",
    "    else:\n",
    "        metric_single, _ = get_metrics(\n",
    "            change_lbl,\n",
    "            test_result,\n",
    "            is_multi_lbl=False\n",
    "        )\n",
    "        plot_metric(\n",
    "            metric=metric_single,\n",
    "            title=f'Scores for {task_name}',\n",
    "            file_metric=s_file.get('metric_single')\n",
    "        )\n",
    "        return metric_single\n",
    "\n",
    "def inference_ollama(ollama_model:str, task_name:str, no_sys_prt:bool=True):\n",
    "    cpt=0\n",
    "    print(f'#### Inference on {task_name}')\n",
    "    time = datetime.datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "    s_file = get_savefile(\n",
    "        task_name=task_name,\n",
    "        spl_name='spl2',\n",
    "        m_name='Meta-Llama-3.1-8B-Instruct',\n",
    "        n_sample=4000,\n",
    "        epoch=2,\n",
    "        train_resp=f'_ollama_all_data_{model_task_name}',\n",
    "        outputs_dir=f'./outputs/test_{task_name}',\n",
    "        time=time\n",
    "    )\n",
    "    print(f'##### Load Data')\n",
    "    labels, tr_d, val_d, test_d, change_lbl = get_data_for_task(\n",
    "        task_name=task_name,\n",
    "        s_file=s_file,\n",
    "    )\n",
    "    res = []\n",
    "    # s = '<[|]ANSWER[|]>'\n",
    "    s = r'<\\|ANSWER\\|>(.*?)<\\|(ANSWER|eot_id)\\|>'\n",
    "    names_dataset = test_d['datasets']\n",
    "    true_labels = test_d['answer']\n",
    "    print(f'##### Start Inference')\n",
    "    for i in test_d['conversations']:\n",
    "        response = chat(model=ollama_model, messages=i)\n",
    "        print(response['message']['content'])\n",
    "        print(f'##############################')\n",
    "        tmp = re.split(s, response['message']['content'])\n",
    "        lbs = {lbl.lower() for lbl in labels}\n",
    "        pred = [\n",
    "            re.sub(r'[^\\w\\s_-]', '', i) \n",
    "            for i in tmp \n",
    "            if re.sub(r'[^\\w\\s_-]', '', i).lower() in lbs\n",
    "        ]\n",
    "        print(f'# Prediction : {pred}')\n",
    "        res.append(pred)\n",
    "    tmp_pred = [i if i != [] else ['Failed'] for i in res]\n",
    "    pred_flat = list(itertools.chain.from_iterable(tmp_pred))\n",
    "    d_res = {'names': names_dataset, 'pred': pred_flat, 'lbl': true_labels}\n",
    "    df_res = pd.DataFrame(data=d_res)\n",
    "    df_res.to_csv(\n",
    "        s_file.get('test_result_file'),\n",
    "        index=False\n",
    "    )\n",
    "    print(f'##### Metrics')\n",
    "    if task_name == 'fallacies':\n",
    "        metric_single, metric_multi = get_metrics(change_lbl, df_res)\n",
    "        plot_metric(\n",
    "            metric=metric_single,\n",
    "            title=f'Scores Ollama Single for {task_name}',\n",
    "            file_metric=s_file.get('metric_single')\n",
    "        )\n",
    "        plot_metric(\n",
    "            metric=metric_multi,\n",
    "            title=f'Scores Ollama Multi for {task_name}',\n",
    "            file_metric=s_file.get('metric_multi')\n",
    "        )\n",
    "        return (metric_single, metric_multi)\n",
    "    else:\n",
    "        metric_single, _ = get_metrics(change_lbl, df_res, is_multi_lbl=False)\n",
    "        plot_metric(\n",
    "            metric=metric_single,\n",
    "            title=f'Scores Ollama for {task_name}',\n",
    "            file_metric=s_file.get('metric_single')\n",
    "        )\n",
    "        return metric_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cell for ollama all data inference\n",
    "# time = datetime.datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "# s_file = get_savefile(\n",
    "#     task_name='aduc',\n",
    "#     spl_name='spl2',\n",
    "#     m_name='Meta-Llama-3.1-8B-Instruct',\n",
    "#     n_sample=4000,\n",
    "#     epoch=2,\n",
    "#     train_resp=f'_ollama_{model_task_name}',\n",
    "#     outputs_dir=f'./outputs/test_aduc',\n",
    "#     time=time\n",
    "# )\n",
    "# print(f'##### Load Data')\n",
    "# labels, tr_d, val_d, test_d, change_lbl = get_data_for_task(\n",
    "#     task_name='aduc',\n",
    "#     s_file=s_file,\n",
    "# )\n",
    "# res = []\n",
    "# s = '<[|]ANSWER[|]>'\n",
    "# names_dataset = test_d['datasets']\n",
    "# true_labels = test_d['answer']\n",
    "# print(f'##### Start Inference')\n",
    "# for i in test_d['conversations'][:2]:\n",
    "#     print(i)\n",
    "#     # tmp = {'role': 'user', 'content': i[0].get('content') +'\\n' + i[1].get('content')}\n",
    "#     # print(tmp)\n",
    "#     response = chat(model='unsloth_model', messages=i)\n",
    "#     print(response['message']['content'])\n",
    "#     print(f'###############################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on all data\n",
    "def inference_on_all_data(\n",
    "    model=None,\n",
    "    tokenizer=None,\n",
    "    inference_method:str='',\n",
    "    no_sys_prt:bool=True\n",
    "):\n",
    "    all_result = {}\n",
    "    task_list = [\n",
    "        'aduc',\n",
    "        'claim_detection',\n",
    "        'evidence_detection',\n",
    "        'evidence_type',\n",
    "        'fallacies',\n",
    "        'quality',\n",
    "        'relation',\n",
    "        'stance_detection'\n",
    "    ]\n",
    "    match inference_method:\n",
    "        case 'ollama':\n",
    "            ollama_model = 'unsloth_model'\n",
    "            for task in task_list:\n",
    "                result = inference_ollama(ollama_model, task, no_sys_prt)\n",
    "                all_result.update({task: result})\n",
    "        case _:\n",
    "            for task in task_list:\n",
    "                result = inference_unsloth(task, model, tokenizer)\n",
    "                all_result.update({task: result})\n",
    "    return all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = inference_ollama('unsloth_model', 'aduc', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on all data\n",
    "all_result = inference_on_all_data(model, tokenizer)\n",
    "# all_result = inference_on_all_data(inference_method='ollama')\n",
    "all_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thing kept just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Parameters\\n\",\n",
    "# task = 'claim_detection'\n",
    "# task_data = 'claim_detection'\n",
    "# task_title = 'Claim Detection'\n",
    "# ckpt_name = checkpoint.get(task)\n",
    "# gguf_file = f'./gguf_model/{task}'\n",
    "# max_seq_lenght = 2048\n",
    "# dtype = None\n",
    "# load_in_4bit = True\n",
    "# gpu_mem_use = 0.6\n",
    "# system_prompt = d_sys_prt.get(task_data)\n",
    "# chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "# {SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "# {INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "# {OUTPUT}<|eot_id|>\"\"\"\n",
    "# time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "# s_file = get_savefile(\n",
    "#     task_name=task_data,\n",
    "#     spl_name='spl2',\n",
    "#     m_name='Meta-Llama-3.1-8B-Instruct',\n",
    "#     n_sample=4000,\n",
    "#     epoch=2,\n",
    "#     train_resp=f'_train_resp',\n",
    "#     outputs_dir=f'./outputs/test_{task}',\n",
    "#     time=time\n",
    "# )\n",
    "# match task_data:\n",
    "#     case 'aduc':\n",
    "#         labels, tr_d, val_d, test_d = aduc.get_data(s_file)\n",
    "#         change_lbl = aduc.change_lbl\n",
    "#     case 'claim_detection':\n",
    "#         labels, tr_d, val_d, test_d = cd.get_data(s_file)\n",
    "#         change_lbl = cd.change_lbl\n",
    "#     case 'evidence_detection':\n",
    "#         labels, tr_d, val_d, test_d = ed.get_data(s_file)\n",
    "#         change_lbl = ed.change_lbl\n",
    "#     case 'evidence_type':\n",
    "#         labels, tr_d, val_d, test_d = et.get_data(s_file)\n",
    "#         change_lbl = et.change_lbl\n",
    "#     case 'fallacies':\n",
    "#         labels, tr_d, val_d, test_d = fd.get_data(s_file)\n",
    "#         change_lbl = fd.change_lbl\n",
    "#     case 'relation':\n",
    "#         labels, tr_d, val_d, test_d = arc.get_data(s_file)\n",
    "#         change_lbl = arc.change_lbl\n",
    "#     case 'stance_detection':\n",
    "#         labels, tr_d, val_d, test_d = sd.get_data(s_file)\n",
    "#         change_lbl = sd.change_lbl\n",
    "#     case 'quality':\n",
    "#         labels, tr_d, val_d, test_d = aq.get_data(s_file)\n",
    "#         change_lbl = aq.change_lbl\n",
    "# # labels, tr_d, val_d, test_d = get_data(s_file)\n",
    "# print(system_prompt)\n",
    "# print(ckpt_name)\n",
    "# print(gguf_file)\n",
    "# print(change_lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Load Model\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=ckpt_name,\n",
    "#     max_seq_length=max_seq_lenght,\n",
    "#     dtype=dtype,\n",
    "#     load_in_4bit=load_in_4bit,\n",
    "#     fast_inference=True,\n",
    "#     gpu_memory_utilization=gpu_mem_use\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Get Datasets\n",
    "# train_set, val_set, test_set = get_datasets(\n",
    "#     tokenizer=tokenizer,\n",
    "#     train=tr_d,\n",
    "#     val=val_d,\n",
    "#     test=test_d, \n",
    "#     chat_template=chat_template,\n",
    "#     sys_prt=system_prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on fewer data\n",
    "# from datasets import Dataset\n",
    "# tmp_set = test_set[:10]\n",
    "# tmp_set = pd.DataFrame().from_records(tmp_set)\n",
    "# tmp_set = Dataset.from_pandas(tmp_set)\n",
    "# tmp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Run On test_set\n",
    "# from src.training import test\n",
    "# res_test_chpt = test(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_test=test_set,\n",
    "#     labels=labels,\n",
    "# )\n",
    "# metric, metric_m = get_metrics(change_lbl, res_test_chpt)\n",
    "# metric, _ = get_metrics(change_lbl, res_test_chpt, is_multi_lbl=False)\n",
    "# plot_metric(\n",
    "#     metric=metric,\n",
    "    # title=f'{task_title}: Scores ckpt{nbckpt}',\n",
    "    # file_plot=f'./img/{task}/scores_ckpt{nbckpt}_metric_single.png',\n",
    "    # file_metric=f'./test_res/{task}/scores_ckpt{nbckpt}_metric_single.csv'\n",
    "# )\n",
    "# plot_metric(\n",
    "#     metric=metric_m\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\n",
    "#     './saved_model/claim_detection',\n",
    "#     tokenizer,\n",
    "#     save_method=\"merged_16bit\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\n",
    "#     './saved_model/claim_detection/saved_lora',\n",
    "#     tokenizer,\n",
    "#     save_method=\"lora\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_gguf(\n",
    "#     gguf_file,\n",
    "#     tokenizer,\n",
    "#     quantization_method = \"q8_0\", # choose quant method: q8_0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ollama import chat\n",
    "\n",
    "# res = []\n",
    "# s = '<[|]ANSWER[|]>'\n",
    "# quant = 'q8_0'\n",
    "# names_dataset = test_d['datasets']\n",
    "# true_labels = test_d['answer']\n",
    "# # print(len(test_d['conversations']))\n",
    "# for i in test_d['conversations']:\n",
    "#     response = chat(model='unsloth_model', messages=[i[1]])\n",
    "#     print(response['message']['content'])\n",
    "#     tmp = re.split(s, response['message']['content'])\n",
    "#     res.append(tmp[1])\n",
    "# d_res = {'names': names_dataset, 'pred': res, 'lbl': true_labels}\n",
    "# df_res = pd.DataFrame(data=d_res)\n",
    "# df_res.to_csv(f'./test_res/{task}/test_result_gguf_{quant}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_single, metric_multi = get_metrics(change_lbl, df_res)\n",
    "# metric_single, _ = get_metrics(change_lbl, df_res, is_multi_lbl=False)\n",
    "# plot_metric(\n",
    "#     metric=metric_single,\n",
    "    # title=f'{task_title}: Score gguf_{quant} single label',\n",
    "    # file_plot=f'./img/{task}/scores_gguf_{quant}_metric_single.png',\n",
    "    # file_metric=f'./test_res/{task}/scores_gguf_{quant}_metric_single.csv'\n",
    "# )\n",
    "# For Fallacies Task Only\n",
    "# plot_metric(\n",
    "#     metric=metric_multi,\n",
    "#     title=f'{task_title}: Score gguf_{quant} multi label',\n",
    "#     file_plot=f'./img/{task}/scores_gguf_{quant}_metric_multi.png',\n",
    "#     file_metric=f'./test_res/{task}/scores_gguf_{quant}_metric_multi.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen via VLLM\n",
    "# def gen(txt, model, sampling_params):\n",
    "#     output = model.fast_generate(\n",
    "#         txt,\n",
    "#         sampling_params = sampling_params,\n",
    "#     )[0].outputs[0].text\n",
    "    \n",
    "#     return output\n",
    "\n",
    "# def format_output(answer: str, fallacies: set) -> list:\n",
    "#     s = '<[|]ANSWER[|]>'\n",
    "#     tmp = re.split(s, answer)\n",
    "#     pred= [i for i in tmp if i in fallacies]\n",
    "#     return pred\n",
    "\n",
    "# def zero_shot_gen(data: list[str], model, fallacies: set, sampling_params) -> list:\n",
    "#     res = []\n",
    "#     for i in data:\n",
    "#         out = gen(i, model, sampling_params)\n",
    "#         pred = format_output(out, fallacies)\n",
    "#         res.append(pred)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class custom_validation_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies, n_step=10):\n",
    "#         super().__init__()\n",
    "#         self.val_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#         self.n_step=n_step\n",
    "#     def on_step_end(self, args, state, control, **kwargs):\n",
    "#         if state.global_step % self.n_step == 0 and state.global_step > 0 :\n",
    "#             model.save_lora('sft_save_lora')\n",
    "#             FastLanguageModel.for_inference(model)\n",
    "#             pred = zero_shot_gen(\n",
    "#                 data=self.val_dataset['text'],\n",
    "#                 model=model,\n",
    "#                 fallacies=self.fallacies,\n",
    "#                 sampling_params=self.sampling_params\n",
    "#             )\n",
    "#             tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#             d = pd.DataFrame().from_records(tmp_pred)\n",
    "#             d['truth_label'] = self.val_dataset['answer']\n",
    "#             d['step'] = np.full((len(d['truth_label']),), state.global_step)\n",
    "#             try:\n",
    "#                 d.to_csv(\n",
    "#                     './validation_res.csv',\n",
    "#                     index=False,\n",
    "#                     mode='a',\n",
    "#                     header=['pred', 'truth_label', 'step']\n",
    "#                 )\n",
    "#             except FileNotFoundError:\n",
    "#                 d.to_csv('./validation_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_step_end(args, state, control, **kwargs)\n",
    "\n",
    "# class custom_test_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies):\n",
    "#         super().__init__()\n",
    "#         self.test_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#     def on_train_end(self, args, state, control, **kwargs):\n",
    "#         model.save_lora('sft_save_lora')\n",
    "#         FastLanguageModel.for_inference(model)\n",
    "#         pred = zero_shot_gen(\n",
    "#             data=self.test_dataset['text'],\n",
    "#             model=model,\n",
    "#             fallacies=self.fallacies,\n",
    "#             sampling_params=self.sampling_params\n",
    "#         )\n",
    "#         tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#         d = pd.DataFrame().from_records(tmp_pred)\n",
    "#         d['truth_label'] = self.test_dataset['answer']\n",
    "#         try:\n",
    "#             d.to_csv('./test_res.csv', index=False, mode='a', header=['pred','truth_label'])\n",
    "#         except FileNotFoundError:\n",
    "#             d.to_csv('./test_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_train_end(args, state, control, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
