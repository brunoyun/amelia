{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-06 10:14:13 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import unsloth_train\n",
    "from unsloth.chat_templates import train_on_responses_only, get_chat_template \n",
    "from unsloth import apply_chat_template\n",
    "\n",
    "from datasets import Dataset\n",
    "from vllm import SamplingParams\n",
    "from transformers import TrainingArguments, TrainerCallback\n",
    "from transformers import TextStreamer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from src.utils import run\n",
    "from src.utils import get_savefile\n",
    "from src.fallacies import get_data\n",
    "from src.prompting import get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training\n",
    "model, tokenizer = run('fallacies', do_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Parameters\n",
    "ckpt_names = f'./outputs/fallacies/2025-05-05_08:53:56_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-340'\n",
    "max_seq_lenght = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "gpu_mem_use = 0.6\n",
    "system_prompt = \"You are an expert in argumentation. Your task is to determine the type of fallacy in the given [SENTENCE]. The fallacy would be in the [FALLACY] Set. Utilize the [TITLE] and the [FULL TEXT] as context to support your decision.\\nYour answer must be in the following format with only the fallacy in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.\"\n",
    "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{OUTPUT}<|eot_id|>\"\"\"\n",
    "time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "s_file = get_savefile(\n",
    "    task_name='fallacies',\n",
    "    spl_name='spl2',\n",
    "    m_name='meta-Llama-3.1-8B-Instruct',\n",
    "    n_sample=4000,\n",
    "    epoch=2,\n",
    "    train_resp='_train_resp',\n",
    "    outputs_dir=f'./outputs/test_fallacies',\n",
    "    time=time\n",
    ")\n",
    "fallacies, tr_d, val_d, test_d = get_data(s_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.1: Fast Llama patching. Transformers: 4.50.0. vLLM: 0.7.2.\n",
      "   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.451 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit with actual GPU utilization = 59.58%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 44.45 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 288.\n",
      "Unsloth: vLLM's KV Cache can use up to 20.15 GB. Also swap space = 6 GB.\n",
      "INFO 05-06 10:14:46 config.py:542] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 05-06 10:14:46 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":288}, use_cached_outputs=False, \n",
      "INFO 05-06 10:14:48 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 05-06 10:14:48 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W506 10:14:48.036136862 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-06 10:14:48 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 05-06 10:14:50 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201869d9b94c4675ae6226bc8420eb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f1e0f156714297925e7dda5377e467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-06 10:14:56 model_runner.py:1115] Loading model weights took 5.5976 GB\n",
      "INFO 05-06 10:14:56 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 05-06 10:14:59 worker.py:267] Memory profiling takes 2.76 seconds\n",
      "INFO 05-06 10:14:59 worker.py:267] the current vLLM instance can use total_gpu_memory (44.45GiB) x gpu_memory_utilization (0.60) = 26.48GiB\n",
      "INFO 05-06 10:14:59 worker.py:267] model weights take 5.60GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.35GiB; the rest of the memory reserved for KV Cache is 19.48GiB.\n",
      "INFO 05-06 10:15:00 executor_base.py:110] # CUDA blocks: 9974, # CPU blocks: 3072\n",
      "INFO 05-06 10:15:00 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 77.92x\n",
      "INFO 05-06 10:15:04 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:47<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-06 10:15:52 model_runner.py:1562] Graph capturing finished in 48 secs, took 0.87 GiB\n",
      "INFO 05-06 10:15:52 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 56.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.4.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Load Checkpoint : Load Model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=ckpt_names,\n",
    "    max_seq_length=max_seq_lenght,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=gpu_mem_use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e49eccba06d4e5eb826c362405569bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05e708a0c3440a6a27cbfc983684b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30359eb2fdc4a21a5ee2521c002b4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>none<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad populum<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false analogy<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>straw man<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>false causality<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Load Checkpoint : Run On test_set\n",
    "from src.training import test\n",
    "train_set, val_set, test_set = get_datasets(\n",
    "    tokenizer=tokenizer,\n",
    "    train=tr_d,\n",
    "    val=val_d,\n",
    "    test=test_d, \n",
    "    chat_template=chat_template,\n",
    "    sys_prt=system_prompt\n",
    ")\n",
    "res_test_chpt = test(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_test=test_set,\n",
    "    labels=fallacies,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 88.02 out of 125.79 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:03<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q5_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at ./gguf_model/fallacies into bf16 GGUF format.\n",
      "The output location will be /home2/hsavigny/amelia/gguf_model/fallacies/unsloth.BF16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: fallacies\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'system' %}{{ '<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "' + messages[0]['content'] + '<|eot_id|>' }}{% set loop_messages = messages[1:] %}{% else %}{{ '<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an expert in argumentation. Your task is to determine the type of fallacy in the given [SENTENCE]. The fallacy would be in the [FALLACY] Set. Utilize the [TITLE] and the [FULL TEXT] as context to support your decision.\n",
      "Your answer must be in the following format with only the fallacy in the answer section:\n",
      "<|ANSWER|><answer><|ANSWER|>.<|eot_id|>' }}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' + message['content'] + '<|eot_id|>' }}{% elif message['role'] == 'assistant' %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' + message['content'] + '<|eot_id|>' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/home2/hsavigny/amelia/gguf_model/fallacies/unsloth.BF16.gguf: n_tensors = 292, total_size = 16.1G\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.1G/16.1G [01:19<00:00, 202Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /home2/hsavigny/amelia/gguf_model/fallacies/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: /home2/hsavigny/amelia/gguf_model/fallacies/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This might take 20 minutes...\n",
      "main: build = 0 (unknown)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/home2/hsavigny/amelia/gguf_model/fallacies/unsloth.BF16.gguf' to '/home2/hsavigny/amelia/gguf_model/fallacies/unsloth.Q5_K_M.gguf' as Q5_K_M using 24 threads\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 292 tensors from /home2/hsavigny/amelia/gguf_model/fallacies/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Fallacies\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type bf16:  226 tensors\n",
      "[   1/ 292]                        output.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
      "[   2/ 292]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 292]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   4/ 292]                    token_embd.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q5_K .. size =  1002.00 MiB ->   344.44 MiB\n",
      "[   5/ 292]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[   6/ 292]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 292]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[   8/ 292]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[   9/ 292]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  10/ 292]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  11/ 292]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  12/ 292]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  13/ 292]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  14/ 292]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  15/ 292]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  16/ 292]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  17/ 292]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  18/ 292]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  19/ 292]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  20/ 292]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  21/ 292]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  22/ 292]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  23/ 292]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  24/ 292]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  25/ 292]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  26/ 292]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  27/ 292]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  28/ 292]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  29/ 292]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  30/ 292]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  31/ 292]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  32/ 292]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  33/ 292]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  34/ 292]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  35/ 292]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  36/ 292]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  37/ 292]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  38/ 292]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  39/ 292]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  40/ 292]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  41/ 292]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  42/ 292]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  43/ 292]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  44/ 292]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  45/ 292]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  46/ 292]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  47/ 292]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  48/ 292]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  49/ 292]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  50/ 292]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  51/ 292]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  52/ 292]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  53/ 292]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  54/ 292]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  55/ 292]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  56/ 292]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  57/ 292]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  58/ 292]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  59/ 292]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  60/ 292]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  61/ 292]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  62/ 292]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  63/ 292]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  64/ 292]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  65/ 292]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  66/ 292]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  67/ 292]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  68/ 292]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  69/ 292]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  70/ 292]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  71/ 292]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  72/ 292]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  73/ 292]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  74/ 292]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  75/ 292]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  76/ 292]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  77/ 292]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  78/ 292]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  79/ 292]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  80/ 292]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  81/ 292]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  82/ 292]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  83/ 292]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  84/ 292]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  85/ 292]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  86/ 292]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  87/ 292]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  88/ 292]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  89/ 292]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  90/ 292]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  91/ 292]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  92/ 292]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  93/ 292]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  94/ 292]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  95/ 292]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  96/ 292]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  97/ 292]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  98/ 292]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  99/ 292]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 100/ 292]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 101/ 292]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 102/ 292]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 103/ 292]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 104/ 292]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 105/ 292]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 106/ 292]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 107/ 292]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 108/ 292]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 109/ 292]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 110/ 292]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 111/ 292]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 112/ 292]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 113/ 292]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 114/ 292]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 115/ 292]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 116/ 292]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 117/ 292]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 118/ 292]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 119/ 292]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 120/ 292]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 121/ 292]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 122/ 292]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 123/ 292]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 292]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 125/ 292]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 126/ 292]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 127/ 292]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 128/ 292]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 129/ 292]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 130/ 292]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 131/ 292]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 132/ 292]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 133/ 292]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 134/ 292]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 135/ 292]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 136/ 292]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 137/ 292]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 138/ 292]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 139/ 292]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 140/ 292]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 141/ 292]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 292]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 143/ 292]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 144/ 292]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 145/ 292]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 146/ 292]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 147/ 292]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 148/ 292]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 149/ 292]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 150/ 292]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 151/ 292]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 152/ 292]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 153/ 292]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 154/ 292]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 155/ 292]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 156/ 292]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 157/ 292]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 158/ 292]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 159/ 292]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 292]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 161/ 292]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 162/ 292]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 163/ 292]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 164/ 292]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 165/ 292]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 166/ 292]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 167/ 292]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 168/ 292]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 169/ 292]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 170/ 292]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 171/ 292]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 172/ 292]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 173/ 292]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 174/ 292]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 175/ 292]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 176/ 292]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 177/ 292]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 178/ 292]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 179/ 292]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 180/ 292]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 181/ 292]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 182/ 292]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 183/ 292]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 184/ 292]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 185/ 292]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 186/ 292]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 187/ 292]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 188/ 292]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 189/ 292]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 190/ 292]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 191/ 292]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 192/ 292]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 193/ 292]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 194/ 292]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 195/ 292]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 292]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 197/ 292]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 198/ 292]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 199/ 292]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 200/ 292]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 201/ 292]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 202/ 292]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 203/ 292]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 204/ 292]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 205/ 292]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 206/ 292]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 207/ 292]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 208/ 292]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 209/ 292]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 210/ 292]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 211/ 292]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 212/ 292]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 213/ 292]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 214/ 292]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 215/ 292]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 216/ 292]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 217/ 292]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 218/ 292]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 219/ 292]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 220/ 292]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 221/ 292]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 222/ 292]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 223/ 292]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 224/ 292]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 225/ 292]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 226/ 292]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 227/ 292]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 228/ 292]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 292]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 230/ 292]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 231/ 292]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 232/ 292]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 233/ 292]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 234/ 292]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 235/ 292]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 236/ 292]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 237/ 292]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 292]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 239/ 292]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 240/ 292]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 241/ 292]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 242/ 292]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 243/ 292]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 244/ 292]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 245/ 292]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 246/ 292]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 292]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 248/ 292]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 249/ 292]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 250/ 292]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 251/ 292]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 252/ 292]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 253/ 292]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 254/ 292]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 255/ 292]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 256/ 292]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 257/ 292]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 258/ 292]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 259/ 292]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 260/ 292]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 261/ 292]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 262/ 292]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 263/ 292]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 264/ 292]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 292]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 266/ 292]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 267/ 292]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 268/ 292]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 269/ 292]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 270/ 292]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 271/ 292]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 272/ 292]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 273/ 292]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 292]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 275/ 292]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 276/ 292]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 277/ 292]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 278/ 292]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 279/ 292]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 280/ 292]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 281/ 292]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 282/ 292]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 292]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 284/ 292]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 285/ 292]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 286/ 292]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 287/ 292]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 288/ 292]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 289/ 292]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 290/ 292]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 291/ 292]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 292/ 292]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "llama_model_quantize_impl: model size  = 15317.02 MB\n",
      "llama_model_quantize_impl: quant size  =  5459.93 MB\n",
      "\n",
      "main: quantize time = 70742.50 ms\n",
      "main:    total time = 70742.50 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Conversion completed! Output location: /home2/hsavigny/amelia/gguf_model/fallacies/unsloth.Q5_K_M.gguf\n",
      "Unsloth: Saved Ollama Modelfile to ./gguf_model/fallacies/Modelfile\n"
     ]
    }
   ],
   "source": [
    "# Save Method\n",
    "# model.save_pretrained_gguf(\n",
    "#     './gguf_model/fallacies',\n",
    "#     tokenizer,\n",
    "#     quantization_method = \"q8_0\", # choose quant method: q8_0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to majority<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>appeal to worse problems<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>none<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>appeal to authority<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>slippery slope<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>ad populum<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>ad hominem<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>false analogy<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>appeal to ridicule<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>straw man<|ANSWER|>.\n",
      "<|ANSWER|>equivocation<|ANSWER|>.\n",
      "<|ANSWER|>appeal to tradition<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>appeal to nature<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>false dilemma<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>false causality<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>guilt by association<|ANSWER|>.\n",
      "<|ANSWER|>hasty generalization<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>causal oversimplification<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n",
      "<|ANSWER|>appeal to fear<|ANSWER|>.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "res = []\n",
    "s = '<[|]ANSWER[|]>'\n",
    "task = 'fallacies'\n",
    "quant = 'q5_k_m'\n",
    "names_dataset = test_d['datasets']\n",
    "true_labels = test_d['answer']\n",
    "# print(len(test_d['conversations']))\n",
    "for i in test_d['conversations']:\n",
    "    response = chat(model='unsloth_model', messages=[i[1]])\n",
    "    print(response['message']['content'])\n",
    "    tmp = re.split(s, response['message']['content'])\n",
    "    res.append(tmp[1])\n",
    "d_res = {'names': names_dataset, 'pred': res, 'lbl': true_labels}\n",
    "df_res = pd.DataFrame(data=d_res)\n",
    "df_res.to_csv(f'./test_res/{task}/test_gguf_{quant}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABkEAAAT2CAYAAACVwgTYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAtsxJREFUeJzs3Xuc1nP++P/nzFQzRY1IB3RSIYqoW21CIdJSH2vXKR8drENoaVs/FdsZsVYHp1qHhA8ra1f7+W4OS1sscopoFZKSDx3kUEpqzVy/P9yaNZrSNcrUy/1+u123m3lfr9f7/bqu6QrzmPf7nZPJZDIBAAAAAACQmNyKXgAAAAAAAMD2IIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAsF3NnDkzcnJyYubMmSXbevfuHY0aNdpux+zUqVN06tRpu+2fivHVV1/F5ZdfHvXr14/c3Nw4+eSTK3pJmzV58uTIycmJl19+uaKXUuEaNWoUvXv33qqxW/vZzWaf37Tx76OHHnoo67mbs/F7vXjx4m22TwAAth0RBACAMm38wV5Zj0GDBlX08nYaH330UVx66aVxwAEHRNWqVaN27drRtm3bGDhwYKxZs6ail7dTmTRpUlx//fXxi1/8Iu6+++749a9/vc2Psbk/89dee+02P9aP1bx582L48OGiAQAAP4hKFb0AAAB2bCNHjozGjRuX2taiRYsKWs3W+fvf/17RS4iIiE8++STatGkTq1evjnPOOScOOOCA+Pjjj+P111+PCRMmxIUXXhi77rprRS9zp/GPf/wj9t577xg7dux2Pc5xxx0XPXv2LLXt0EMP3a7H/DGZN29ejBgxIjp16rTJGWE7ymcXAIB0iCAAAGxR165do02bNhW9jKxUqVKlopcQERF33nlnLFmyJJ599tk4/PDDSz23evXqH3Sda9eujV122eUHO972sGLFithtt922+3H222+/+O///u/tfhw2taN8dgEASIfLYQEAUC7vvfdeXHTRRbH//vtH1apVY4899ohTTz213Je4+f3vfx+HH3547LHHHlG1atVo3br1Zq/b/z//8z/Rtm3bqFatWtSsWTOOOuqoUr9BXtZ9BdavXx/Dhg2Lpk2bRn5+ftSvXz8uv/zyWL9+falxTzzxRBxxxBGx2267xa677hr7779/XHHFFaXGLFmyJN58883vfE0LFy6MvLy8+MlPfrLJczVq1IiCgoJS21544YX46U9/GjVr1oxddtklDj744Bg/fnypMf/4xz/iyCOPjF122SV22223+K//+q+YP39+qTHDhw+PnJycmDdvXvTo0SNq1qwZRxxxRKn3r3Xr1lG1atXYfffd44wzzoj333//O19PxNf3VGjTpk0UFBREkyZN4g9/+EPJ8b5p3bp1cckll0StWrWievXq0b179/jggw8iJycnhg8fXjJuc/eH+eY+Fy9eHDk5OTFjxox44403Si5R9c37zHyX2267LZo0aRJVq1aNtm3bxj//+c8t3n9i3bp18eWXX271/rfGp59+Gm3bto199tkn3nrrra2as/EeFg8++GCMGDEi9t5776hevXr84he/iFWrVsX69eujf//+Ubt27dh1112jT58+m/yZLkunTp2iRYsW8frrr0fHjh2jWrVq0bRp05LP3FNPPRXt2rWLqlWrxv777x9PPvlkqflb830ry+TJk+PUU0+NiIijjz56k+9lee/n88knn8Rll10WLVu2jF133TVq1KgRXbt2jddee63M8UVFRXHFFVdE3bp1Y5dddonu3buX+Rl44YUX4oQTTojCwsKoVq1adOzYMZ599tms1wcAQMVxJggAAFu0atWqWLlyZalttWrVipdeeimee+65OOOMM2KfffaJxYsXx4QJE6JTp04xb968qFatWlbHGT9+fHTv3j3OOuus2LBhQzzwwANx6qmnxt/+9rc48cQTS8aNGDEihg8fHocffniMHDkyqlSpEi+88EL84x//iOOPP77MfRcXF0f37t3jmWeeifPPPz+aN28ec+fOjbFjx8bbb78dU6dOjYiIN954I0466aQ4+OCDY+TIkZGfnx/vvPPOJj/07NmzZzz11FORyWS2+JoaNmwYRUVFce+990avXr22OPaJJ56Ik046KerVqxeXXnpp1K1bN+bPnx9/+9vf4tJLL42IiCeffDK6du0a++67bwwfPjzWrVsXN910U3To0CFeeeWVTX4ofeqpp0azZs3immuuKVnr1VdfHUOGDInTTjstzj333Pjoo4/ipptuiqOOOipeffXVLZ5p8eqrr8YJJ5wQ9erVixEjRkRRUVGMHDky9txzz03G9u7dOx588ME4++yz4yc/+Uk89dRTpb6P2dhzzz3j3nvvjauvvjrWrFkTo0ePjoiI5s2bb9X8O++8My644II4/PDDo3///vHuu+9G9+7dY/fdd4/69etvMn7y5Mlx6623RiaTiebNm8dvf/vb6NGjR7nWvtHKlSvjuOOOi08++SSeeuqpaNKkSVbzR48eHVWrVo1BgwbFO++8EzfddFNUrlw5cnNz49NPP43hw4fH888/H5MnT47GjRvH0KFDv3Ofn376aZx00klxxhlnxKmnnhoTJkyIM844I+67777o379/9O3bN3r06FFyH5b3338/qlevXt63ICIijjrqqLjkkkvixhtvjCuuuKLke7i138vNeffdd2Pq1Klx6qmnRuPGjWP58uXxhz/8ITp27Bjz5s2Lvfbaq9T4q6++OnJycmLgwIGxYsWKGDduXHTu3DnmzJkTVatWjYivg2PXrl2jdevWMWzYsMjNzY277rorjjnmmPjnP/8Zbdu2/V5rBgDgB5IBAIAy3HXXXZmIKPORyWQyX3zxxSZzZs2alYmIzD333FOybcaMGZmIyMyYMaNkW69evTINGzYsNffb+9uwYUOmRYsWmWOOOaZk24IFCzK5ubmZn/3sZ5mioqJS44uLi0v+uWPHjpmOHTuWfH3vvfdmcnNzM//85z9LzZk4cWImIjLPPvtsJpPJZMaOHZuJiMxHH320hXfm6/1vzX9KL1u2LLPnnntmIiJzwAEHZPr27Zu5//77M5999lmpcV999VWmcePGmYYNG2Y+/fTTzb6uVq1aZWrXrp35+OOPS7a99tprmdzc3EzPnj1Ltg0bNiwTEZkzzzyz1L4WL16cycvLy1x99dWlts+dOzdTqVKlTbZ/W7du3TLVqlXLfPDBByXbFixYkKlUqVKp92P27NmZiMj079+/1PzevXtnIiIzbNiwkm1l/Vn45mv4po4dO2YOOuigLa7x2zZs2JCpXbt2plWrVpn169eXbL/tttsyEVHqz0kmk8kcfvjhmXHjxmX++te/ZiZMmJBp0aJFJiIyt956a1bH3fj5eemllzJLly7NHHTQQZl99903s3jx4qz2s/Hz06JFi8yGDRtKtp955pmZnJycTNeuXUuNb9++fZnv57dt/DN8//33l2x78803MxGRyc3NzTz//PMl2x9//PFMRGTuuuuukm3ZfN8aNmyY6dWrV8nXf/rTnzb5O+Gb6/r296Qs397nl19+ucnfCYsWLcrk5+dnRo4cWbJt4/u59957Z1avXl2y/cEHH8xERGb8+PGZTObrz12zZs0yXbp0KfUZ/OKLLzKNGzfOHHfccSXbNn6vFy1a9J3rBgDgh+dyWAAAbNEtt9wSTzzxRKlHRJT8tnRExL///e/4+OOPo2nTprHbbrvFK6+8kvVxvrm/Tz/9NFatWhVHHnlkqX1NnTo1iouLY+jQoZGbW/o/Zbd0CZ4//elP0bx58zjggANi5cqVJY9jjjkmIiJmzJgREVFyFsRf//rXKC4u3uz+Zs6c+Z1ngURE1KlTJ1577bXo27dvfPrppzFx4sTo0aNH1K5dO0aNGlWyj1dffTUWLVoU/fv33+RMjI2va+nSpTFnzpzo3bt37L777iXPH3zwwXHcccfFI488ssnx+/btW+rrv/zlL1FcXBynnXZaqfehbt260axZs5L3oSxFRUXx5JNPxsknn1zqt+qbNm0aXbt2LTX2sccei4iIiy66qNT2X/3qV5vd//by8ssvx4oVK6Jv376l7jfRu3fvKCws3GT8s88+G5deeml07949+vbtG7Nnz44WLVrEFVdcEevWrcv6+P/3f/8XHTt2jH//+9/x9NNPR8OGDcv1Onr27BmVK1cu+bpdu3aRyWTinHPOKTWuXbt28f7778dXX331nfvcdddd44wzzij5ev/994/ddtstmjdvHu3atSu1z4ivz7bYUeXn55f8nVBUVBQff/xxyeXsyvr7qGfPnqXOavnFL34R9erVK/kczZkzJxYsWBA9evSIjz/+uOSzsnbt2jj22GPj6aef3uLfEQAA7DhcDgsAgC1q27ZtmTdGX7duXYwePTruuuuu+OCDD0pFgVWrVmV9nL/97W9x1VVXxZw5c0rd0+CbcWPhwoWRm5sbBx54YFb7XrBgQcyfP7/MyzZFfH3D7YiI008/Pe64444499xzY9CgQXHsscfGKaecEr/4xS82iS5bq169ejFhwoS49dZbY8GCBfH444/HddddF0OHDo169erFueeeGwsXLoyIiBYtWmx2P++9915EfP2D6m9r3rx5PP7445vc/Lxx48alxi1YsCAymUw0a9aszGN884fs37ZixYpYt25dNG3adJPnvr3tvffei9zc3E2OX9bc7W3j+/bt11y5cuXYd999v3N+lSpVol+/fiVB5Jv3VtkaZ599dlSqVCnmz58fdevWzWruNzVo0KDU1xsDzrcv51VYWBjFxcWxatWq2GOPPba4z3322WeTeFhYWFjmPiO+jpM7quLi4hg/fnzceuutsWjRoigqKip5rqz34dt/HnJycqJp06Yl9zRasGBBRMQWL2O3atWqqFmz5jZYPQAA25MIAgBAufzqV7+Ku+66K/r37x/t27ePwsLCyMnJiTPOOCPr35D+5z//Gd27d4+jjjoqbr311qhXr15Urlw57rrrrrj//vu/91qLi4ujZcuWMWbMmDKf3/hD36pVq8bTTz8dM2bMiGnTpsVjjz0WU6ZMiWOOOSb+/ve/R15eXrnXkJOTE/vtt1/st99+ceKJJ0azZs3ivvvui3PPPbfc+/wu3zy7JuLr9yEnJyceffTRMl/Lrrvuut3WsjmbO4Pnmz/Ermgb/3x88sknWc895ZRT4p577onx48eX3MukPDb3Z29z27fmTKXvs88d7ft2zTXXxJAhQ+Kcc86JUaNGxe677x65ubnRv3//cp2xsXHO9ddfH61atSpzTEV8XgAAyJ4IAgBAuTz00EPRq1evuOGGG0q2ffnll/HZZ59lva8///nPUVBQEI8//njk5+eXbL/rrrtKjWvSpEkUFxfHvHnzNvuDybI0adIkXnvttTj22GO3eNmsiIjc3Nw49thj49hjj40xY8bENddcE1deeWXMmDEjOnfunNXr2px99903atasGUuXLi1ZX0TEv/71r80eY+NllN56661NnnvzzTejVq1apc4CKUuTJk0ik8lE48aNY7/99stqzbVr146CgoJ45513Nnnu29saNmwYxcXFsWjRolK/cV/W3Jo1a5b5Z2bjGRzf18b3bcGCBSWXP4v4+hJuixYtikMOOeQ797HxMlCbO5NoS371q19F06ZNY+jQoVFYWBiDBg3Keh87ou/zffuuz2B5PPTQQ3H00UfHnXfeWWr7Z599FrVq1dpk/MYzPTbKZDLxzjvvxMEHHxwR//lM1qhRY5t97gEAqBjuCQIAQLnk5eVt8tvmN910U7l+EzwvLy9ycnJKzV28eHFMnTq11LiTTz45cnNzY+TIkZv8dveWfvP9tNNOiw8++CBuv/32TZ5bt25drF27NiLK/k3/jbHlm5foWrJkSbz55pvf+bpeeOGFkn1/04svvhgff/xxyaWtDjvssGjcuHGMGzdukx8sb3xd9erVi1atWsXdd99dasy//vWv+Pvf/x4//elPv3M9p5xySuTl5cWIESM2eb8ymUx8/PHHm52bl5cXnTt3jqlTp8aHH35Ysv2dd96JRx99tNTYLl26RETErbfeWmr7TTfdtMl+mzRpEqtWrYrXX3+9ZNvSpUvj4Ycf/s7XszXatGkTe+65Z0ycODE2bNhQsn3y5MmbvNcfffTRJvM///zzGDduXNSqVStat25drjUMGTIkLrvsshg8eHBMmDChXPvY0Xyf79vGWFeeYLo5Zf199Kc//Sk++OCDMsffc8898fnnn5d8/dBDD8XSpUtL7m/TunXraNKkSfz+97+PNWvWbDK/rD8rAADsmJwJAgBAuZx00klx7733RmFhYRx44IExa9asePLJJ7/zPgRlOfHEE2PMmDFxwgknRI8ePWLFihVxyy23RNOmTUv9kLVp06Zx5ZVXxqhRo+LII4+MU045JfLz8+Oll16Kvfbaa7OXGzr77LPjwQcfjL59+8aMGTOiQ4cOUVRUFG+++WY8+OCD8fjjj0ebNm1i5MiR8fTTT8eJJ54YDRs2jBUrVsStt94a++yzT6l7QfTs2TOeeuqp77zk0L333hv33Xdf/OxnP4vWrVtHlSpVYv78+TFp0qQoKCiIK664IiK+PvtkwoQJ0a1bt2jVqlX06dMn6tWrF2+++Wa88cYb8fjjj0fE15fm6dq1a7Rv3z5++ctfxrp16+Kmm26KwsLCGD58+He+z02aNImrrroqBg8eHIsXL46TTz45qlevHosWLYqHH344zj///Ljssss2O3/48OHx97//PTp06BAXXnhhFBUVxc033xwtWrSIOXPmlIxr3bp1/PznP49x48bFxx9/HD/5yU/iqaeeirfffjsiSp8JcMYZZ8TAgQPjZz/7WVxyySXxxRdfxIQJE2K//fYr84bW2apcuXJcddVVccEFF8QxxxwTp59+eixatCjuuuuuTe4Jcsstt8TUqVOjW7du0aBBg1i6dGlMmjQplixZEvfee2+pG6tn6/rrr49Vq1bFxRdfHNWrV4///u///r4vrUJ9n+9bq1atIi8vL6677rpYtWpV5OfnxzHHHBO1a9cu93pOOumkGDlyZPTp0ycOP/zwmDt3btx3332bve/L7rvvHkcccUT06dMnli9fHuPGjYumTZvGeeedFxFffybvuOOO6Nq1axx00EHRp0+f2HvvveODDz6IGTNmRI0aNeL//b//V+71AgDwwxFBAAAol/Hjx0deXl7cd9998eWXX0aHDh3iySefLDkLIBvHHHNM3HnnnXHttddG//79o3HjxnHdddfF4sWLS0WQiIiRI0dG48aN46abboorr7wyqlWrFgcffHCcffbZm91/bm5uTJ06NcaOHRv33HNPPPzww1GtWrXYd99949JLLy25NFT37t1j8eLFMWnSpFi5cmXUqlUrOnbsGCNGjCi5OXQ2LrjggqhWrVpMnz49/vrXv8bq1atjzz33jOOPPz4GDx4chx56aMnYLl26xIwZM2LEiBFxww03RHFxcTRp0qTkh7IREZ07d47HHnsshg0bFkOHDo3KlStHx44d47rrrtvkJuSbM2jQoNhvv/1i7NixMWLEiIj4+p4Xxx9/fHTv3n2Lc1u3bh2PPvpoXHbZZTFkyJCoX79+jBw5MubPn7/JmTH33HNP1K1bN/74xz/Gww8/HJ07d44pU6bE/vvvHwUFBSXj9thjj3j44YdjwIABcfnll0fjxo1j9OjRsWDBgm0SQSIizj///CgqKorrr78+/r//7/+Lli1bxv/+7//GkCFDSo3r0KFDPPfcc3HHHXfExx9/HLvssku0bds2Jk2aVOpSWuU1ceLEWLNmTfTp0yeqV68e//Vf//W991lRvs/3rW7dujFx4sQYPXp0/PKXv4yioqKYMWPG94ogV1xxRaxduzbuv//+mDJlShx22GExbdq0zV5+7IorrojXX389Ro8eHZ9//nkce+yxceutt0a1atVKxnTq1ClmzZoVo0aNiptvvjnWrFkTdevWjXbt2sUFF1xQ7rUCAPDDyslszR3zAAAANuPkk0+ON954Y5P7LHzbnDlz4tBDD43/+Z//ibPOOusHWt3mderUKSIiZs6cWaHrAAAAth/3BAEAALbaunXrSn29YMGCeOSRR0qCwubGRUSMGzcucnNz46ijjtqeSwQAACjhclgAAMBW23fffaN3796x7777xnvvvRcTJkyIKlWqxOWXX15q3O9+97uYPXt2HH300VGpUqV49NFH49FHH43zzz8/6tevv03W8sknn5S62fm35eXlxZ577rlNjrXRunXrYtWqVVscs/vuu3/n/UM2bNgQn3zyyRbHFBYWRtWqVbNeIwAA8B8iCAAAsNVOOOGE+OMf/xjLli2L/Pz8aN++fVxzzTXRrFmzUuMOP/zweOKJJ2LUqFGxZs2aaNCgQQwfPjyuvPLKbbaWU045JZ566qnNPt+wYcNYvHjxNjteRMSUKVOiT58+WxwzY8aMTc6M+bbnnnsujj766C2Oueuuu6J3795ZrhAAAPgm9wQBAAB2SrNnz45PP/10s89XrVo1OnTosE2PuXTp0njjjTe2OKZ169ZRs2bNLY759NNPY/bs2Vscc9BBB0W9evWyXiMAAPAfIggAAAAAAJCkneJyWMXFxfHhhx9G9erVIycnp6KXAwAAAAAAVKBMJhOff/557LXXXpGbm7vZcTtFBPnwww+32c0TAQAAAACANLz//vuxzz77bPb5nSKCVK9ePSK+fjE1atSo4NUAAAAAAAAVafXq1VG/fv2SfrA5O0UE2XgJrBo1aoggAAAAAABARMR33kJj8xfKAgAAAAAA2ImJIAAAAAAAQJJEEAAAAAAAIEk7xT1BAACA7BQVFcW///3vil4GWahcuXLk5eVV9DIAACApIggAACQkk8nEsmXL4rPPPqvopVAOu+22W9StW/c7b+4IAABsHREEAAASsjGA1K5dO6pVq+aH6TuJTCYTX3zxRaxYsSIiIurVq1fBKwIAgDSIIAAAkIiioqKSALLHHntU9HLIUtWqVSMiYsWKFVG7dm2XxgIAgG3AjdEBACARG+8BUq1atQpeCeW18Xvnfi4AALBtiCAAAJAYl8DaefneAQDAtiWCAAAAAAAASRJBAAAAAACAJLkxOgAA/Ag0GjTtBz3e4mtPzGp8JpOJCy64IB566KH49NNP49VXX41WrVptn8UBAAA/Gs4EAQAAKtxjjz0WkydPjr/97W+xdOnSWL16dXTr1i322muvyMnJialTp1b0EgEAgJ2QCAIAAFS4hQsXRr169eLwww+PunXrxtq1a+OQQw6JW265paKXBgAA7MRcDgsAAKhQvXv3jrvvvjsiInJycqJhw4axePHi6Nq1awWvDAAA2NmJIAAAQIUaP358NGnSJG677bZ46aWXIi8vr6KXBAAAJEIEAQAAKlRhYWFUr1498vLyom7duhW9HAAAICHuCQIAAAAAACRJBAEAAAAAAJIkggAAAAAAAElyTxAAAGCHs2bNmnjnnXdKvl60aFHMmTMndt9992jQoEEFrgwAANiZiCAAAPAjsPjaEyt6CVl5+eWX4+ijjy75esCAARER0atXr5g8eXIFrQoAANjZ5GQymUxFL+K7rF69OgoLC2PVqlVRo0aNil4OAADskL788stYtGhRNG7cOAoKCip6OZSD7yEAAGydre0G7gkCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACSpUkUvAAAA+AEML/yBj7fqhz1eOcycOTOOPvro+PTTT2O33XbbZmMBAIAdhzNBAACAH6XDDz88li5dGoWF3x2IshkLAADsOEQQAABgp7Nhw4bvvY8qVapE3bp1IycnZ5uOBQAAdhwiCAAAUOE6deoU/fr1i379+kVhYWHUqlUrhgwZEplMJiIiGjVqFKNGjYqePXtGjRo14vzzz4+IiGeeeSaOPPLIqFq1atSvXz8uueSSWLt2bcl+169fHwMHDoz69etHfn5+NG3aNO68886I+PoSVzk5OfHZZ59FRMR7770X3bp1i5o1a8Yuu+wSBx10UDzyyCNljo2I+POf/xwHHXRQ5OfnR6NGjeKGG24o9ZoaNWoU11xzTZxzzjlRvXr1aNCgQdx2223b6y0EAADKIIIAAAA7hLvvvjsqVaoUL774YowfPz7GjBkTd9xxR8nzv//97+OQQw6JV199NYYMGRILFy6ME044IX7+85/H66+/HlOmTIlnnnkm+vXrVzKnZ8+e8cc//jFuvPHGmD9/fvzhD3+IXXfdtczjX3zxxbF+/fp4+umnY+7cuXHddddtduzs2bPjtNNOizPOOCPmzp0bw4cPjyFDhsTkyZNLjbvhhhuiTZs28eqrr8ZFF10UF154Ybz11lvf/80CAAC2ihujAwAAO4T69evH2LFjIycnJ/bff/+YO3dujB07Ns4777yIiDjmmGPiN7/5Tcn4c889N84666zo379/REQ0a9YsbrzxxujYsWNMmDAhlixZEg8++GA88cQT0blz54iI2HfffTd7/CVLlsTPf/7zaNmy5XeOHTNmTBx77LExZMiQiIjYb7/9Yt68eXH99ddH7969S8b99Kc/jYsuuigiIgYOHBhjx46NGTNmxP7775/9GwQAAGTNmSAAAMAO4Sc/+Umpe260b98+FixYEEVFRRER0aZNm1LjX3vttZg8eXLsuuuuJY8uXbpEcXFxLFq0KObMmRN5eXnRsWPHrTr+JZdcEldddVV06NAhhg0bFq+//vpmx86fPz86dOhQaluHDh1KrTci4uCDDy7555ycnKhbt26sWLFiq9YDAAB8fyIIAACwU9hll11Kfb1mzZq44IILYs6cOSWP1157LRYsWBBNmjSJqlWrZrX/c889N9599904++yzY+7cudGmTZu46aabvteaK1euXOrrnJycKC4u/l77BAAAtp4IAgAA7BBeeOGFUl8///zz0axZs8jLyytz/GGHHRbz5s2Lpk2bbvKoUqVKtGzZMoqLi+Opp57a6jXUr18/+vbtG3/5y1/iN7/5Tdx+++1ljmvevHk8++yzpbY9++yzsd9++212vQAAwA9PBAEAAHYIS5YsiQEDBsRbb70Vf/zjH+Omm26KSy+9dLPjBw4cGM8991z069cv5syZEwsWLIi//vWvJTdGb9SoUfTq1SvOOeecmDp1aixatChmzpwZDz74YJn769+/fzz++OOxaNGieOWVV2LGjBnRvHnzMsf+5je/ienTp8eoUaPi7bffjrvvvjtuvvnmuOyyy77/GwEAAGwzbowOAAA/BsNXVfQKvlPPnj1j3bp10bZt28jLy4tLL700zj///M2OP/jgg+Opp56KK6+8Mo488sjIZDLRpEmTOP3000vGTJgwIa644oq46KKL4uOPP44GDRrEFVdcUeb+ioqK4uKLL47/+7//ixo1asQJJ5wQY8eOLXPsYYcdFg8++GAMHTo0Ro0aFfXq1YuRI0eWuik6AABQ8XIymUymohfxXVavXh2FhYWxatWqqFGjRkUvBwAAdkhffvllLFq0KBo3bhwFBQUVvZysdOrUKVq1ahXjxo2r6KVUqJ35ewgAAD+kre0GLocFAAAAAAAkKesI8vTTT0e3bt1ir732ipycnJg6dep3zpk5c2YcdthhkZ+fH02bNo3JkyeXY6kAAAAAAABbL+sIsnbt2jjkkEPilltu2arxixYtihNPPDGOPvromDNnTvTv3z/OPffcePzxx7NeLAAAkKaZM2f+6C+FBQAAbHtZ3xi9a9eu0bVr160eP3HixGjcuHHccMMNERHRvHnzeOaZZ2Ls2LHRpUuXbA8PAAAAAACwVbb7PUFmzZoVnTt3LrWtS5cuMWvWrM3OWb9+faxevbrUAwAAAAAAIBtZnwmSrWXLlkWdOnVKbatTp06sXr061q1bF1WrVt1kzujRo2PEiBHbe2nlM7ywHHNWbft1AN9feT7PET7T3+Z93Db8+wUAAODHw/9Ll9Jo0LRyzVtc0CPrOS0bNyjXseb2mluueVS87X4mSHkMHjw4Vq1aVfJ4//33K3pJAAAAAADATma7nwlSt27dWL58ealty5cvjxo1apR5FkhERH5+fuTn52/vpQEAAAAAAAnb7meCtG/fPqZPn15q2xNPPBHt27ff3ocGAAAAAAB+xLI+E2TNmjXxzjvvlHy9aNGimDNnTuy+++7RoEGDGDx4cHzwwQdxzz33RERE37594+abb47LL788zjnnnPjHP/4RDz74YEybVr7rvAEAANlreXfLH/R4O8M1k4cPHx5Tp06NOXPmRERE796947PPPoupU6dW6LoAAIBtJ+szQV5++eU49NBD49BDD42IiAEDBsShhx4aQ4cOjYiIpUuXxpIlS0rGN27cOKZNmxZPPPFEHHLIIXHDDTfEHXfcEV26dNlGLwEAAAAAAGBTWZ8J0qlTp8hkMpt9fvLkyWXOefXVV7M9FAAA8CO1YcOGqFKlSkUvAwAA2Mlt93uCAAAAfJdOnTpFv379on///lGrVq3o0qVL/Otf/4quXbvGrrvuGnXq1Imzzz47Vq5cWTKnuLg4fve730XTpk0jPz8/GjRoEFdffXXJ8wMHDoz99tsvqlWrFvvuu28MGTIk/v3vf1fEywMAACqICAIAAOwQ7r777qhSpUo8++yzce2118YxxxwThx56aLz88svx2GOPxfLly+O0004rGT948OC49tprY8iQITFv3ry4//77o06dOiXPV69ePSZPnhzz5s2L8ePHx+233x5jx46tiJcGAABUkKwvhwUAALA9NGvWLH73u99FRMRVV10Vhx56aFxzzTUlz0+aNCnq168fb7/9dtSrVy/Gjx8fN998c/Tq1SsiIpo0aRJHHHFEyfjf/va3Jf/cqFGjuOyyy+KBBx6Iyy+//Ad6RQAAQEUTQQAAgB1C69atS/75tddeixkzZsSuu+66ybiFCxfGZ599FuvXr49jjz12s/ubMmVK3HjjjbFw4cJYs2ZNfPXVV1GjRo3tsnYAAGDHJIIAAAA7hF122aXkn9esWRPdunWL6667bpNx9erVi3fffXeL+5o1a1acddZZMWLEiOjSpUsUFhbGAw88EDfccMM2XzcAALDjEkEAAIAdzmGHHRZ//vOfo1GjRlGp0qb/29KsWbOoWrVqTJ8+Pc4999xNnn/uueeiYcOGceWVV5Zse++997brmgEAgB2PG6MDAAA7nIsvvjg++eSTOPPMM+Oll16KhQsXxuOPPx59+vSJoqKiKCgoiIEDB8bll18e99xzTyxcuDCef/75uPPOOyPi60iyZMmSeOCBB2LhwoVx4403xsMPP1zBrwoAAPihORMEAAB+BOb2mlvRS8jKXnvtFc8++2wMHDgwjj/++Fi/fn00bNgwTjjhhMjN/fp3uYYMGRKVKlWKoUOHxocffhj16tWLvn37RkRE9+7d49e//nX069cv1q9fHyeeeGIMGTIkhg8fXoGvCgAA+KHlZDKZTEUv4rusXr06CgsLY9WqVRV/I8PhheWYs2rbrwP4/srzeY7wmf427+O24d8vwDbw5ZdfxqJFi6Jx48ZRUFBQ0cuhHHwPAeBHwv9Ll9Jo0LRyzVtc0CPrOS0bNyjXsXa2Xyr6MdjabuByWAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAiSkuLq7oJVBOvncAALBtVaroBQAAANtGlSpVIjc3Nz788MPYc889o0qVKpGTk1PRy2IrZDKZ2LBhQ3z00UeRm5sbVapUqeglAQBAEkQQAABIRG5ubjRu3DiWLl0aH374YUUvh3KoVq1aNGjQIHJznbQPAADbgggCAAAJqVKlSjRo0CC++uqrKCoqqujlkIW8vLyoVKmSs3cAAGAbEkEAACAxOTk5Ubly5ahcuXJFLwUAAKBCOccaAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSKlX0AgAAANhOhheWY86qbb8OAACoID/aCNJo0LRyzVtcsI0XAgAAAAAAbBcuhwUAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJFWq6AUAbAuNBk3Les7igu2wEAAAAABgh+FMEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkKRKFb0AAAAAtqzRoGnlmre4YBsvBAAAdjLOBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJJUqaIXAMCOodGgaeWat7hgGy8EAAAAALYRZ4IAAAAAAABJEkEAAAAAAIAkuRzWD6Dl3S3LNW9ur7nbeCUAAAAAAPDj4UwQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSKlX0AgAAANhxtLy7Zbnmze01dxuvBAAAvj9nggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSZUqegEAO5uWd7cs17y5veZu45UAAAAAAFviTBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSe4LwwxteWM55q7btOgAAAAAASJozQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJLKFUFuueWWaNSoURQUFES7du3ixRdf3OL4cePGxf777x9Vq1aN+vXrx69//ev48ssvy7VgAAAAAACArZF1BJkyZUoMGDAghg0bFq+88koccsgh0aVLl1ixYkWZ4++///4YNGhQDBs2LObPnx933nlnTJkyJa644orvvXgAAAAAAIDNyTqCjBkzJs4777zo06dPHHjggTFx4sSoVq1aTJo0qczxzz33XHTo0CF69OgRjRo1iuOPPz7OPPPMLZ49sn79+li9enWpBwAAAAAAQDayiiAbNmyI2bNnR+fOnf+zg9zc6Ny5c8yaNavMOYcffnjMnj27JHq8++678cgjj8RPf/rTzR5n9OjRUVhYWPKoX79+NssEAAAAAACIStkMXrlyZRQVFUWdOnVKba9Tp068+eabZc7p0aNHrFy5Mo444ojIZDLx1VdfRd++fbd4OazBgwfHgAEDSr5evXq1EAIAAAAAAGSlXDdGz8bMmTPjmmuuiVtvvTVeeeWV+Mtf/hLTpk2LUaNGbXZOfn5+1KhRo9QDAAAAAAAgG1mdCVKrVq3Iy8uL5cuXl9q+fPnyqFu3bplzhgwZEmeffXace+65ERHRsmXLWLt2bZx//vlx5ZVXRm7udu8wAAAAAADAj1BWBaJKlSrRunXrmD59esm24uLimD59erRv377MOV988cUmoSMvLy8iIjKZTLbrBQAAAAAA2CpZnQkSETFgwIDo1atXtGnTJtq2bRvjxo2LtWvXRp8+fSIiomfPnrH33nvH6NGjIyKiW7duMWbMmDj00EOjXbt28c4778SQIUOiW7duJTEEAAAAAABgW8s6gpx++unx0UcfxdChQ2PZsmXRqlWreOyxx0pulr5kyZJSZ3789re/jZycnPjtb38bH3zwQey5557RrVu3uPrqq7fdqwAAAAAAAPiWrCNIRES/fv2iX79+ZT43c+bM0geoVCmGDRsWw4YNK8+hAAAAAAAAysVdyQEAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJlSp6AQD8OLW8u2XWc+b2mrsdVgIAAABAqpwJAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJJUqaIXAAAAAAAAVIDhheWct2rbrmM7ciYIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASFKlil4AAKSk0aBp5Zq3uGAbLwQAAIDtzv8Dwo7PmSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJJUrgtxyyy3RqFGjKCgoiHbt2sWLL764xfGfffZZXHzxxVGvXr3Iz8+P/fbbLx555JFyLRgAAAAAAGBrVMp2wpQpU2LAgAExceLEaNeuXYwbNy66dOkSb731VtSuXXuT8Rs2bIjjjjsuateuHQ899FDsvffe8d5778Vuu+22LdYPAAAAAABQpqwjyJgxY+K8886LPn36RETExIkTY9q0aTFp0qQYNGjQJuMnTZoUn3zySTz33HNRuXLliIho1KjRFo+xfv36WL9+fcnXq1evznaZAAAAAADAj1xWl8PasGFDzJ49Ozp37vyfHeTmRufOnWPWrFllzvnf//3faN++fVx88cVRp06daNGiRVxzzTVRVFS02eOMHj06CgsLSx7169fPZpkAAAAAAADZRZCVK1dGUVFR1KlTp9T2OnXqxLJly8qc8+6778ZDDz0URUVF8cgjj8SQIUPihhtuiKuuumqzxxk8eHCsWrWq5PH+++9ns0wAAAAAAIDsL4eVreLi4qhdu3bcdtttkZeXF61bt44PPvggrr/++hg2bFiZc/Lz8yM/P397Lw0AAAAAAEhYVhGkVq1akZeXF8uXLy+1ffny5VG3bt0y59SrVy8qV64ceXl5JduaN28ey5Ytiw0bNkSVKlXKsWwAAAAAAIAty+pyWFWqVInWrVvH9OnTS7YVFxfH9OnTo3379mXO6dChQ7zzzjtRXFxcsu3tt9+OevXqCSAAAAAAAMB2k1UEiYgYMGBA3H777XH33XfH/Pnz48ILL4y1a9dGnz59IiKiZ8+eMXjw4JLxF154YXzyySdx6aWXxttvvx3Tpk2La665Ji6++OJt9yoAAAAAAAC+Jet7gpx++unx0UcfxdChQ2PZsmXRqlWreOyxx0pulr5kyZLIzf1PW6lfv348/vjj8etf/zoOPvjg2HvvvePSSy+NgQMHbrtXAQAAAAAA8C3lujF6v379ol+/fmU+N3PmzE22tW/fPp5//vnyHAoAAAAAAKBcsr4cFgAAAAAAwM5ABAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkipV9AIAAAAAAIDvp9GgaVnPWVywHRayg3EmCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASapU0QsAAAAAAAB2Hi3vbpn1nLm95m6HlXw3Z4IAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECS3BMEAHZS5bn+ZkTFXYMTAAC2lUaDppVr3uKCHlnPadm4QbmO5b+72R52pvswwI7CmSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASapU0QsAAGDbazRoWrnmLS7okfWclo0blOtYc3vNLdc8AAAA2FrOBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASSpXBLnllluiUaNGUVBQEO3atYsXX3xxq+Y98MADkZOTEyeffHJ5DgsAAAAAALDVKmU7YcqUKTFgwICYOHFitGvXLsaNGxddunSJt956K2rXrr3ZeYsXL47LLrssjjzyyO+1YEhNo0HTyjVvcUGPrOe0bNygXMea22tuueYBAAAAAFSkrM8EGTNmTJx33nnRp0+fOPDAA2PixIlRrVq1mDRp0mbnFBUVxVlnnRUjRoyIfffd93stGAAAAAAAYGtkFUE2bNgQs2fPjs6dO/9nB7m50blz55g1a9Zm540cOTJq164dv/zlL7fqOOvXr4/Vq1eXegAAAAAAAGQjqwiycuXKKCoqijp16pTaXqdOnVi2bFmZc5555pm488474/bbb9/q44wePToKCwtLHvXr189mmQAAAAAAAOW7MfrW+vzzz+Pss8+O22+/PWrVqrXV8wYPHhyrVq0qebz//vvbcZUAAAAAAECKsroxeq1atSIvLy+WL19eavvy5cujbt26m4xfuHBhLF68OLp161ayrbi4+OsDV6oUb731VjRp0mSTefn5+ZGfn5/N0gAAAAAAAErJ6kyQKlWqROvWrWP69Okl24qLi2P69OnRvn37TcYfcMABMXfu3JgzZ07Jo3v37nH00UfHnDlzXOYKAAAAAADYbrI6EyQiYsCAAdGrV69o06ZNtG3bNsaNGxdr166NPn36REREz549Y++9947Ro0dHQUFBtGjRotT83XbbLSJik+0AAAAAAADbUtYR5PTTT4+PPvoohg4dGsuWLYtWrVrFY489VnKz9CVLlkRu7na91QgAAAAAAMB3yjqCRET069cv+vXrV+ZzM2fO3OLcyZMnl+eQAAAAAAAAWXHKBgAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQpEoVvQB2bo0GTct6zuKC7bAQAAAAAAD4FmeCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkipV9AIAAIB0NRo0rVzzFhf0KNe8lo0bZD1nbq+55ToWAACw43MmCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJIqVfQCAAAAgJ1Do0HTyjVvcUGPrOe0bNygXMea22tuueYBAGlyJggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJqlTRCwAA+LZGg6ZlPWdxQY9yHatl4wZZz5nba265jgUAAAD8sJwJAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJCkShW9ANhaLe9umfWcub3mboeVAAAAAACwM3AmCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJJEEAAAAAAAIEkiCAAAAAAAkCQRBAAAAAAASJIIAgAAAAAAJEkEAQAAAAAAkiSCAAAAAAAASRJBAAAAAACAJIkgAAAAAABAkkQQAAAAAAAgSSIIAAAAAACQJBEEAAAAAABIkggCAAAAAAAkSQQBAAAAAACSJIIAAAAAAABJEkEAAAAAAIAkiSAAAAAAAECSRBAAAAAAACBJIggAAAAAAJAkEQQAAAAAAEiSCAIAAAAAACRJBAEAAAAAAJIkggAAAAAAAEkSQQAAAAAAgCSJIAAAAAAAQJLKFUFuueWWaNSoURQUFES7du3ixRdf3OzY22+/PY488sioWbNm1KxZMzp37rzF8QAAAAAAANtC1hFkypQpMWDAgBg2bFi88sorccghh0SXLl1ixYoVZY6fOXNmnHnmmTFjxoyYNWtW1K9fP44//vj44IMPvvfiAQAAAAAANifrCDJmzJg477zzok+fPnHggQfGxIkTo1q1ajFp0qQyx993331x0UUXRatWreKAAw6IO+64I4qLi2P69OmbPcb69etj9erVpR4AAAAAAADZyCqCbNiwIWbPnh2dO3f+zw5yc6Nz584xa9asrdrHF198Ef/+979j99133+yY0aNHR2FhYcmjfv362SwTAAAAAAAguwiycuXKKCoqijp16pTaXqdOnVi2bNlW7WPgwIGx1157lQop3zZ48OBYtWpVyeP999/PZpkAAAAAAABR6Yc82LXXXhsPPPBAzJw5MwoKCjY7Lj8/P/Lz83/AlQEAAAAAAKnJKoLUqlUr8vLyYvny5aW2L1++POrWrbvFub///e/j2muvjSeffDIOPvjg7FcKAAAAAACQhawuh1WlSpVo3bp1qZuab7zJefv27Tc773e/+12MGjUqHnvssWjTpk35VwsAAAAAALCVsr4c1oABA6JXr17Rpk2baNu27f/f3n3HR1Hnfxx/74YuCQSll9AJ5UBAgchPIJQDRDhEaqQGRRFUQBEUAYXjbEgTxUKKqEAA0QOllxABcwgIARKKUoJKqEGKoSTs7w8f7N0CgWx2k9kZXs/Hw8eP2RnOt/Pb2e/MfL5F06ZN08WLFzVgwABJUt++fVW2bFm9+eabkqS3335b48aN09y5c1WxYkXn2iGFCxdW4cKFvfifAgAAAAAAAAAA8F9uF0F69OihkydPaty4cUpJSdH999+vFStWOBdLT05Olt3+3wEms2bN0pUrV9S1a1eX/53x48fr9ddf9yw9AAAAAAAAAABAJrK1MPrQoUM1dOjQW+6LjY112T58+HB2/hUAAAAAAAAAAAAecWtNEAAAAAAAAAAAALOgCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACyJIggAAAAAAAAAALAkiiAAAAAAAAAAAMCSKIIAAAAAAAAAAABLoggCAAAAAAAAAAAsiSIIAAAAAAAAAACwJIogAAAAAAAAAADAkiiCAAAAAAAAAAAAS6IIAgAAAAAAAAAALIkiCAAAAAAAAAAAsCSKIAAAAAAAAAAAwJIoggAAAAAAAAAAAEuiCAIAAAAAAAAAACwpW0WQDz74QBUrVlSBAgXUuHFjbdmy5bbHL1y4UMHBwSpQoID+9re/admyZdkKCwAAAAAAAAAAkFVuF0FiYmI0YsQIjR8/Xtu3b1e9evXUtm1bnThx4pbHb968Wb169dLAgQP1008/qXPnzurcubN2797tcXgAAAAAAAAAAIDM5HH3L0yZMkVPPfWUBgwYIEn66KOP9N133ykyMlKjR4++6fjp06erXbt2GjlypCRp4sSJWr16tWbOnKmPPvrolv+Oy5cv6/Lly87tP/74Q5J07tw5d+Nm6trlP7P1987ZHG7/nYy0jOz9u7z435tTsnMes3MOpeydR6ueQ4nv4o18/bso+f55zM3vosQ1fSOuaVe+fk1b9RxKfBfhfbQv3sE1DV/BdxG+gu8ifAX3Ot7BNe0dd9uz9PX/PYfj9v8NNsedjvgfV65cUaFChbRo0SJ17tzZ+Xm/fv109uxZ/fvf/77p71SoUEEjRozQsGHDnJ+NHz9e33zzjXbu3HnLf8/rr7+uN954I6uxAAAAAAAAAADAXejo0aMqV65cpvvdGgly6tQpZWRkqGTJki6flyxZUnv37r3l30lJSbnl8SkpKZn+e1555RWNGDHCuX3t2jWdOXNG9957r2w2mzuRc825c+dUvnx5HT16VAEBAUbHMSXOoXdwHj3HOfQOzqPnOIfewXn0Ds6j5ziH3sF59Bzn0Ds4j57jHHoH59E7OI+e4xx6B+fRc5xD7zDDeXQ4HDp//rzKlClz2+Pcng4rN+TPn1/58+d3+axo0aLGhHFTQECAz34pzIJz6B2cR89xDr2D8+g5zqF3cB69g/PoOc6hd3AePcc59A7Oo+c4h97BefQOzqPnOIfewXn0HOfQO3z9PBYpUuSOx7i1MPp9990nPz8/HT9+3OXz48ePq1SpUrf8O6VKlXLreAAAAAAAAAAAAG9wqwiSL18+NWzYUGvXrnV+du3aNa1du1YhISG3/DshISEux0vS6tWrMz0eAAAAAAAAAADAG9yeDmvEiBHq16+fHnjgATVq1EjTpk3TxYsXNWDAAElS3759VbZsWb355puSpBdeeEHNmzfXe++9pw4dOmj+/PnaunWrPvnkE+/+lxgsf/78Gj9+/E3TeCHrOIfewXn0HOfQOziPnuMcegfn0Ts4j57jHHoH59FznEPv4Dx6jnPoHZxH7+A8eo5z6B2cR89xDr3DSufR5nA4HO7+pZkzZ+rdd99VSkqK7r//fs2YMUONGzeWJLVo0UIVK1ZUdHS08/iFCxfqtdde0+HDh1WtWjW98847euSRR7z2HwEAAAAAAAAAAHCjbBVBAAAAAAAAAAAAfJ1ba4IAAAAAAAAAAACYBUUQAAAAAAAAAABgSRRBAAAAAAAAAACAJVEEAQAAAAAAAAAAlpTH6AAAAPM6d+6c238nICAgB5KYG+cRviIhIcHtv1OrVi3lycMtJeBruJ69gzbaO/g+AtbB9QzAjGwOh8NhdAizGDFihNt/57XXXlOxYsVyII05denSxe2/89FHH6lEiRI5kMa8ZsyY4fbfGTBggPz9/XMgjTktWbLE7b/Tpk0bFSxYMAfSmJfdbpfNZsvy8TabTfv371flypVzMJX5cB49RxvtHde/i1m9PbTb7XwXb+Dud8pms2n79u0KCgrKoUTmxD2j57ievYM22jv4PnqO5xfv4Dx6juvZO7hn9FyDBg3cOt5ms2nJkiUqW7ZsDiWCL6MI4ga73a6QkBDly5cvS8dv3LhR+/bt44f+f9jtdnXv3j3LNxBz585VUlIS5/AGdrtd5cqVk5+fX5aOP3r0KDcdN7Db3ZsN0Gaz6cCBA5zDG9jtdn311VdZuoFzOBx65JFHtHv3bs7jDTiPnqON9g673a4tW7aoePHidzzW4XCoTp06SkhI4Dz+D7vdrmnTpqlIkSJ3PNbhcOjZZ5/ler4F7hk9x/XsHbTR3sH30XM8v3gH59FzXM/ewT2j5+x2u1588UUVLlz4jsc6HA699dZbSkxM5BzeQUZGhqZOnaoFCxYoOTlZV65ccdl/5swZg5J5hrFobvr666+z3MOMXve3NmPGjCyfw0WLFuVwGvPaunUr30UPpaSkcA49FBQUpGbNmunee+/N0vGVK1dW3rx5cziV+XAevYM22nPNmzdX1apVVbRo0Swd36xZM3pG3kLPnj2z/F187rnncjiNeXHP6BmuZ++gjfYOvo/ewfOLd3AePcP17D3cM3pu5MiRWT6H7733Xg6nsYY33nhDs2fP1osvvqjXXntNY8aM0eHDh/XNN99o3LhxRsfLNoogboiKispShfa6jz/+WCVLlszBROazfv16t4b8LV++nGFqtzB+/PgsVbqve/XVV5ny5Qb9+vVz60asd+/ezO98C4cOHXLr+N27d+dQEnPjPHqONto71q9f79bxy5Yty6Ek5nXt2jW3jj9//nwOJTE37hk9x/XsHbTR3sH30XM8v3gH59FzXM/ewT2j5w4dOpSlEUnXJSYmqkyZMjmYyBq+/PJLffrpp+rQoYNef/119erVS1WqVFHdunUVHx+v559/3uiI2cJ0WAAAAAAAAAAA3OXuueceJSUlqUKFCipdurS+++47NWjQQAcPHlT9+vX1xx9/GB0xWxgJAgDwyJw5c1y2+/bta1ASc+M8wldMmDDBZdvMQ56NEhcX57LdrFkzg5Lgbsf17B200d7B9xGwDq5n7+Ce0XPJycku2xUqVDAoiXWUK1dOx44dU4UKFVSlShWtWrVKDRo00I8//qj8+fMbHS/bKIK4qVKlSrLZbM7tgwcPGpjGnOx2u2w2mxwOh2w2mzIyMoyOZEqhoaEu38V169YZmMacwsPDXbYjIyMNSmJuUVFRzj/bbDZeDGQT59FztNHe8b9Tv/zv+UTW9evXz/lnm83GdzGbuGf0HNezd9BGewffR8/x/OIdnEfPcT17B/eMnqtYsSL3i1722GOPae3atWrcuLGee+459e7dWxEREUpOTtbw4cONjpdtFEHcFB0dbXQE03N3Xl3cWv/+/Y2OYHpBQUFGR7AEd+eExa1xHj1HG+0d//uyD9nDvY53cB49x/XsHbTR3sH30XM8v3gH59FzXM/ewb2O59xdWwV39tZbbzn/3KNHDwUFBWnz5s2qVq2aOnbsaGAyz7AmCHLdhAkT9NJLL6lQoUJGRwHgBdeuXZPdbjc6xl1h69ateuCBB4yOgbuIw+HQ6dOnZbPZdO+99xodBwB8Fm20e2hfAOvgegasJS4uTg899JDy5HEdO5Genq7Nmzebdto2iiDIdX5+fjp27JhKlChhdBQAXnDjNT1y5Ei98sorKlasmMHJzOnChQvy8/NTwYIFnZ/t2LFDY8eO1bJlyxjei1yRkpKil19+WUuWLNH58+clSQEBAXrsscf05ptvqmTJkgYn9G1paWlau3atHn30UUnSK6+8osuXLzv3+/n5aeLEiSpQoIBREU3h1KlTunjxokuP3T179mjy5Mm6ePGiOnfurLCwMAMT+r4bp3zJDFPBZA1ttOdoX7zn1KlTOnz4sGw2mypWrMjLZy+6dOmSZs6cqZdeesnoKD6N69k7rl27pujoaC1evNh5TVeqVEldu3ZVnz59mG7sDm5cVyUzZn1xb4TM3tuePn1aJUqUMO39DkUQN/n5+WXpOLN+IXKD3W5XSkoKRRAPBQYGZqkxPHPmTC6kMadffvlFkyZNcj74V6hQQRcuXHDu9/Pz08aNG1WjRg2jIprCjdd0QECAduzYocqVKxuczFyOHj2q7t27a8uWLfLz89PQoUP1z3/+U88884xiYmL02GOPafjw4WrcuLHRUX0WbbR3nDt3Tvfff78uXLigJ554QsHBwXI4HEpMTNS8efMUGBio7du3q3DhwkZH9VkfffSRvvvuOy1dulSS5O/vr9q1aztfnO7du1cvv/yyqefUzQ29evVSmTJl9N5770mSTpw4oeDgYJUpU0ZVqlTR8uXLFRERoT59+hic1HfZ7XYFBQWpfv36ut1j39dff52LqcyHNto7aF+8Y8+ePRo8eLA2bdrk8nnz5s01a9Ysnl2y6OTJk/rPf/6jfPnyqVWrVvLz89PVq1f14Ycf6s0331R6erpOnTpldEyfxfXsHQ6HQx07dtSyZctUr14953lMSkrSrl271KlTJ33zzTdGx/Rpt5uV4vo7M5vNpvT09NyKZHp2u13Hjx9X8eLFXT7fv3+/HnjgAZ07d86gZJ5hTRA3ORwOBQUFqV+/fqpfv77RcUyLSrbnpk2b5vyzw+HQ4MGDNWHCBIpLbnj//fddeqekpqZq3LhxznMYExOjqVOn6qOPPjIqoilRW8+ekSNH6tKlS5o+fboWL16s6dOn6/vvv1fjxo31yy+/qFy5ckZH9Hm00d4xffp0+fn5ac+ePTfd+L722mtq2rSpZsyYoVdffdWghL7vyy+/1Msvv+zy2dy5c53F4S+++EIffPABRZA7iI+Pd1nrZ86cOSpWrJh27NihPHnyaPLkyfrggw8ogtzG4MGDNW/ePB06dEgDBgxQ7969GamZDbTR3kH74rmUlBQ1b95cxYsX15QpU1xePH/66ad6+OGHtXv3bp4J72Djxo169NFHde7cOdlsNj3wwAOKiopS586dlSdPHr3++usuC1bjZlzP3hEdHa24uDitXbtWoaGhLvvWrVunzp07a86cOerbt69BCX1famrqLT//888/NX36dM2YMYMOmlnUpUsXSX+9s+3fv7/y58/v3JeRkaGEhAQ99NBDRsXznANu+fHHHx3PPPOMo2jRoo769es73n//fceZM2eMjmUqNpvNUbRoUUdgYOBt/4F7Chcu7Pjll1+MjmEqderUcfznP/9xbt94DmNjYx1Vq1Y1Ipqp2Gw2x/Hjx53bfBezp3Tp0o4ffvjB4XA4HMePH3fYbDbH1KlTjQ1lMrTR3tG4cWNHZGRkpvsjIiIcTZo0ycVE5lOqVCnHoUOHnNv33Xefy/a+ffscAQEBuR/MZAoUKOA4fPiwc7t9+/aOkSNHOrf37dvnKFasmBHRTOXSpUuOuXPnOlq3bu0oVKiQo1u3bo4VK1Y4rl27ZnQ006CN9g7aF8+9/PLLjgYNGjjS0tJu2vfnn386GjRo4Bg9erQBycylefPmjl69ejl27drleOmllxw2m81RvXp1x8KFC42OZhpcz97Rpk0bx5tvvpnp/kmTJjn+/ve/52Ii88vIyHB8+umnjnLlyjkqVKjgiIyMdGRkZBgdyxT69+/v6N+/v8Nmszl69Ojh3O7fv79j0KBBjn/961+OkydPGh0z25gOK5suXbqkRYsWKSoqSvHx8erYsaMGDhyoNm3aGB3N59ntdk2bNk1FihS57XH0vHCPv7+/du7cSYXbDf7+/kpKSnL23hs+fLhee+0153y6R44cUXBwsNLS0oyM6fPsdrsGDRqkQoUKSZI++OAD9e7d+6ZrfMqUKUbEMw0/Pz/9/vvvztFJhQsX1rZt25jSIBtooz1TrFgx/fDDD5l+9/bu3auHHnqI6RZvo2DBgtqxY8dtz+H999+vS5cu5XIycylZsqRWrVqlevXqSZLuu+8+ffzxx3r88cclSQcOHFD9+vVdprLE7R05ckTR0dGaM2eO0tPTtWfPHqYqyQLaaO+gffFcgwYNNHr0aHXv3v2W++fPn6933nlH27dvz+Vk5nLvvffq+++/V61atZSWlqbChQtr8eLF+sc//mF0NNPgevaOUqVKacWKFbr//vtvuf+nn35S+/btlZKSkrvBTGrx4sV69dVXdfLkSb3yyit67rnnXEYzIGveeOMNvfTSS7rnnnuMjuJVTIeVTQUKFFDv3r3Vu3dvHTp0SAMHDlS7du108uRJhphnQc+ePRmiC8PZ7Xb9/vvvziLI1KlTXfYfP35cefPmNSKaqTRr1kz79u1zbj/00EM6ePCggYnM63/nM7Xb7cqXL5+BacyLNtoz586dU9GiRTPdX7RoUdPOA5tbypUrp927d2f6YiAhIYHpc7KgSZMmmjFjhj799FMtXrxY58+fV8uWLZ379+/fr/LlyxuY0HzsdrtsNpscDgfrI7mJNtpztC+eO3jwoBo0aJDp/gceeID78CxITU3VfffdJ+mvjguFChVSnTp1DE5lLlzP3nHmzJnbLiBfsmTJTKd7wn9t2LBBo0aN0q5du/TCCy9o1KhRd+x4jcyNHz/e6Ag5giKIB3799VdFR0crOjpaf/75p0aOHKmAgACjY/k81gOBr6hdu7bWrFmjRo0a3XL/ypUruRnOgtjYWKMjWILD4VD16tWdv5EXLlxQ/fr1b1rojd5UWUMbnX0Oh+OOCwwykPj2HnnkEY0bN04dOnRQgQIFXPalpaXpjTfeUIcOHQxKZx4TJ05Uq1at9MUXXyg9PV2vvvqqAgMDnfvnz5+v5s2bG5jQHC5fvqzFixcrMjLSOQ/+zJkz1a5du9te6/gv2mjvoH3x3Pnz5297P+Pv78/ouCxKTEx09q53OBzat2+fLl686HJM3bp1jYhmClzP3pGRkaE8eTJ/Nevn58eC3nfwyCOPaM2aNQoPD9c333yjUqVKGR3JEhYtWqQFCxYoOTlZV65ccdln1tGGFEHcdOXKFX399deKiIjQ999/r/bt22vatGlq3769/Pz8jI5nCrdrCM+dO6cvv/xSERER2rp1ay6mMp8RI0a4bF+5ckWTJk1iCiI3DBgwQMOGDVO9evVuehm1dOlSvfXWWy4L0CN7kpKSFBERocmTJxsdxadFRUUZHcH0aKO948aXfbfaj9t79dVXtWDBAtWoUUNDhw5V9erVJUn79u3TzJkznS/0cXt169ZVUlKSNm3apFKlSqlx48Yu+3v27KlatWoZlM4cnn32Wc2fP1/ly5dXeHi45s2b5+z9jKyjjfYO2hfvOH/+/E0F9uvOnTvHecyiVq1auZyrRx99VNJ/X97bbDZGzN0G17N3OByOmxag/l+XL1/O5UTms2LFCuXJk0cxMTFasGBBpsfRUSHrZsyYoTFjxqh///7697//rQEDBuiXX37Rjz/+qCFDhhgdL9tYE8RN9957r/z9/dWvXz/16dMn0ymd6G3qnvXr1ysyMlKLFy9WkSJF9Nhjj+mDDz4wOpZPCw0NzdJx69evz+Ek5tarVy/FxMQoODjYOW3Jvn37tG/fPj3++OO3bUSRuYsXL2r+/PmKiIhQfHy8atWqpd27dxsdy/QyMjJ4mX8btNHe8dlnn2XpONbuur1Dhw5p8ODBWr16tfNFgM1mU5s2bfThhx+yhhdyhd1uV4UKFVS/fv3bjsZevHhxLqayJtroO6N98dz1Ke0yw8v7rDly5EiWjgsKCsrhJObF9ewd/fv3z9JsKRTjM8d30fuCg4M1fvx49erVy2X94XHjxunMmTOaOXOm0RGzhSKIm/53uN+tfqi46ci63377TdHR0YqKitLZs2eVmpqquXPnqnv37kyZhVw1f/58zZ8/X/v375ckVatWTb169VLPnj0NTmY+mzZtUkREhBYsWKC0tDQNHz5cTz75pIKDg42OZmr79+9XRESE5syZo2PHjhkdx2fRRsMXnTlzRj///LMkqWrVqqxL46b09HRNnTpV8+bNc7bT1atXV1hYmF544QXW7roDXq7kPNpo5KYNGzZk6TimCvTc7t27mRoZsAg6KrinUKFCSkpKUlBQkEqUKKHVq1erXr16OnDggJo0aaLTp08bHTFbmA7LTfSq99xXX32liIgIxcXFqX379nrvvffUvn173XPPPfrb3/5GAcRLmIIo63r27EnBwwMnTpxQdHS0IiMj9ccff6hXr16KjY1VSEiIwsPDKYBk059//qmYmBhFRkbqhx9+0AMPPHDTNHhwRRvtHZGRkXriiScyHZYP9xQrVizTtadwe2lpaWrTpo1++OEHtW7dWs2aNZP01z3OqFGjtGTJEq1atSrTaWEgRUdHGx3Bkmijs4f2xXMUN3LW+fPnNW/ePM2ePVvbtm2j48xtcD17x8GDB1WpUiXeg+UQOipkT6lSpXTmzBkFBQWpQoUKio+PV7169XTo0CFTT3VHEcRNR44cUY8ePfih90CPHj00atQoxcTEyN/f3+g4lnKrKYgogmTu3LlzWTqOqXNuLygoSF27dtX06dPVpk0bFln1UHx8vGbPnq2FCxeqQoUKSkpK0vr16/Xwww8bHc3n0UZ7x1NPPaVHH33UOZ1YmTJltHnzZlWsWNHYYCbSpUuXLB3HFES399Zbb+no0aP66aefblqcdufOnerUqZPeeustvf7668YENAE/Pz8dO3Ys0+kB4R7aaM/Qvnhu3LhxGj16tAoVKiRJSk1NVWBgoMGpzC8uLk4RERH66quvVKZMGXXp0oXpue+A69k7qlWr5tJO9+jRQzNmzFDJkiUNTmZedFTwXMuWLbVkyRLVr19fAwYM0PDhw7Vo0SJt3bo1y885vog3VW4aMGCA/vjjD6NjmNrAgQP1wQcfqF27dvroo4+UmppqdCTT27Rpk8LDw1WyZEkNGjRIDz30kBITE1mD4Q6KFi2qwMDATP+5vh+3FxQUpI0bNyouLs45VQnc995776l27drq2rWrAgMDFRcXp127dslms+nee+81Op4p0EZ7x429e86fP69r164ZlMacihQp4vLPd999J7vdftPnuL358+drypQpNxVAJKlevXqaPHmy5s6da0Ay8zBzbz1fQhvtHbQvnps0aZIuXLjg3A4KCtLBgwcNTGReKSkpeuutt1StWjV169ZNAQEBunz5sr755hu99dZbevDBB42O6NO4nr3jxvO4bNkyXbx40aA05hYfH68nn3xSpUuX1pQpU/TDDz9o/fr1io+P18iRI42OZyqffPKJxowZI0kaMmSIIiMjVbNmTU2YMEGzZs0yOF32MRLETTxIeO7jjz/WtGnTtGDBAkVGRmrYsGFq27atHA4HjaYbmILIc/87dY7D4dAjjzyi2bNnq2zZsgamMp+9e/c61wJ58MEHVb16dfXu3VvSrddlwK2NGjVKo0aN0oQJE5ivNJtoo+ErblxfYdGiRXrnnXdYDN1NR44cue1UYk2aNFFycnIuJsLdijYavuLGex3ufbKnY8eOiouLU4cOHTRt2jS1a9dOfn5++uijj4yOBsBN7733nst7sbi4ONWrV0958+alo0I22e12lxk+rDKFPEWQbOClnucKFiyofv36qV+/fjpw4ICioqK0detWNW3aVB06dFDXrl1NPcQqNzAFkedunFPXz89PTZo04SVVNjRt2lRNmzbVjBkzNG/ePEVFRSkjI0PPPvuswsLC1LlzZxUvXtzomD5t4sSJioqK0ueff65evXqpT58+LMaYDbTRnrPZbC7n8cZtILcEBAToxIkTKl++/C33p6SkMLVqFsyePVuFCxe+7THPP/98LqUxJ9po76B9ga9Yvny5nn/+eQ0ePFjVqlUzOo4pcT17x63OG+fRPXRU8I6EhIQsH3urUdpmYHPQdcAtdrtdderUUZ48t68fbd++PZcSWce1a9f03XffKSIiQsuXL9fly5eNjuTTgoODdfnyZYWFhalPnz7OkR958+bVzp07VatWLYMTmo+/v7927txJEcRLkpKSFBERoc8//1xnzpzR1atXjY5kChs2bFBkZKQWLVqkqlWras+ePdqwYYOaNm1qdDSfRxvtHdenbbr+AHb27FkFBATcVGw/c+aMEfFMifYle3r06KH09HR99dVXt9z/+OOPy8/PTwsWLMjlZOZht9tVrly5274UsNlsTKeTRbTRnqF98Zyfn5/279+v4sWLy+FwqHz58tq4ceNN6zCwpuHtxcfHKyIiQjExMapZs6b69Omjnj17qnTp0jxLZxHXs3fY7Xa1b9/euabh0qVL1bJlS91zzz0ux7GOXObefPNNRUVF6dKlSy4dFXg35h673S6bzSaHw3HHQlxGRkYupfIuiiBustvtevHFF+/Ym2r8+PG5lMiaTpw4wQKOWXB9CqKFCxc6pyB6+eWXlZCQoJo1axodz3R4SZUz0tPTtWTJEkZ3uen8+fOaO3euIiMjtW3bNjVq1Ehdu3ZlUbfboI32js8++yxLx/Xr1y+Hk1gH7Uv2JCYmqnHjxqpdu7ZGjBih4OBgORwOJSUlaerUqUpMTFR8fLxq165tdFSfZbfblZKSwn21l9FGZw/ti+euv6S67saXVde3zfqCKrddvHjRuYDyli1blJGRoSlTpig8PJyRhnfA9ewdAwYMyNJxN061ipvRUcEzR44ccf75p59+0ksvvaSRI0cqJCREkvTDDz/ovffe0zvvvKPOnTsblNIzFEHcxIOE5+Li4ly2mzVrZlAS67hw4YJzCqL4+Hg1b96cKYiywd/fXwkJCapUqZLRUQAXu3btUkREhObOnasTJ04YHcdn0UbDVyxZssRlu1evXpo2bZpKlizp8nmnTp1yM5YpxcfHa+DAgUpKSnK+6HM4HAoODlZERITzwQy35ufnp2PHjvG7mINoo5GbNmzYkKXjbpz2F3e2b98+5yj2s2fPqk2bNje15wB8Hx0VPNeoUSO9/vrreuSRR1w+X7ZsmcaOHatt27YZlMwzFEHcxIOE5/73BTPD772PKYiy7saRCQw9zZ4bh03S8yznXL16VXnz5jU6hs+ijYavyMo6XfxeumfHjh3av3+/JKl69eq6//77jQ1kEhSHcw9tNGANGRkZWrp0qSIjIymCACZHR4XsKViwoLZv337TDDNJSUlq0KCB0tLSDErmGYogbuJBAmbBFER3xtBT7/jfYZOSFBQUZFASc5szZ47Ldt++fQ1KYl600d5RqVIll6k16KwAI507d06FCxe+qbB07do1XbhwgXnv7+CNN97QyJEjVahQIaOjmBpttHfQvgDWwfXsHQMGDHA5j5GRkQamsS46KrinQYMGqlOnjmbPnq18+fJJkq5cuaInn3xSu3fvNu0amxRB3HTkyBFVqFDhpkVi0tPTdenSpTvOQw4AwK2EhoY6/2yz2bRu3ToD05gTbbR33DjVBlNqwChff/21Ro0apR07dtz0Ev/ixYtq0KCBJk+erI4dOxqU0PedOnVKFy9edOmgsGfPHk2ePFkXL15U586dFRYWZmBCc6CN9g7aF88xAts7wsPDXbZ58ew+rmfveOONN1y2WbvQfXRU8L4tW7aoY8eOcjgcqlu3riQpISFBNptNS5cuVaNGjQxOmD0UQdy0dOlSnT59Wv3793d+NmnSJE2cOFHp6elq2bKlYmJiFBgYaFxI3BW4AQYAV7TR8BWsf+Ydf//739W9e3c9+eSTt9wfGRmpmJgYrVy5MpeTmUevXr1UpkwZvffee5KkEydOKDg4WGXKlFGVKlW0fPlyRUREqE+fPgYnBZAVjMD2Dl48A9ZBR4WccfHiRX355Zfau3evJKlmzZoKCwu7afp4M6EI4qYWLVqoW7duGjJkiCRp8+bNevjhhzVhwgTVrFlTY8aMUfv27TVlyhSDk8LquAH2HENPAWuhjYavYP0z7yhTpozi4uJUtWrVW+7/+eef1axZM/3++++5nMw8KlWqpOjoaGcP3cmTJ+ujjz7S3r17lSdPHk2ePFmLFi1SfHy8wUkBAABgJh06dNDs2bNVunRpo6NkSR6jA5hNYmKiHnroIef2okWL1KZNG40ZM0aSVKBAAb3wwgu8YEGOo+jhuYoVKxodAYAX0UbDVxw6dMjoCJaQmpqq9PT0TPdfvXpVqampuZjIfFJSUlzud9atW6cuXbooT56/HgM7deqkN99806B0AAAAMKu4uDhTLZJOEcRN58+f17333uvc3rhxo7p16+bcrl27Nr3RAJNg2DNgLbTRgLVUrFhRW7duVXBw8C33b926lU4hdxAQEKCzZ886z9OWLVs0cOBA536bzabLly8bFQ8AAADIFXajA5hN2bJllZSUJEm6cOGCdu7c6dLr9PTp0zct3AgAAHIebTRgLV26dNGYMWN0/Pjxm/alpKTotdde0+OPP25AMvNo0qSJZsyYoWvXrmnRokU6f/68WrZs6dy/f/9+lS9f3sCEAAAAQM6jCOKmbt26adiwYfr888/11FNPqVSpUmrSpIlz/9atW1WjRg0DE1qD3W5Xy5YttW3bNqOjAPCCli1bauLEifrzzz+NjgILo40GrGX06NHy9/dXtWrV9Oyzz2r69OmaPn26Bg8erOrVq6tw4cIaPXq00TF92sSJE7VkyRIVLFhQPXr00Msvv6zAwEDn/vnz5zvXCwEAAACsiumw3DRu3Dj99ttvev7551WqVCl98cUX8vPzc+6fN2+eOnbsaGBCa4iMjNThw4c1ZMgQFmoELKBChQpau3atPv30UyUnJxsdx7SSk5NVtmxZl3YH/0UbnXvCw8MVGhqqPn36GB0FFubv769NmzbplVdeUUxMjHP9j6JFi6p3796aNGmS/P39DU7p2+rWraukpCRt2rRJpUqVUuPGjV329+zZU7Vq1TIonbXQRnsH7QtgHVzP3jFnzhw1bdpUVapUMToKYGo2h8PhMDoEAODucO7cOQUEBBgdw7TsdruqVaumN998U126dDE6Du5iLVq00OHDh1W0aFHt2LHD6Di4CzgcDp06dUoOh0PFixeXzWYzOhLggjbaO2hfPNeyZUuFhobqxRdfZBpQD/Di2XNcz95ht9uVN29eDRo0SO+//77RcUyJjgo5w9/fXzt37lTlypWNjpIlFEEAC+MGGLCWDRs26ODBg1qxYoViYmKMjgMoMTGRXuSAD5szZ47Ldt++fQ1KYn200d5F+5J9/fv31+HDh3Xw4EFGYHuAF8/ew/XsuUOHDmn58uV69tlnjY5iSnRUyBlvvvmmBg8erKJFixodJUsogrgpNDTUpefZunXrDExjTuHh4S7bkZGRBiWxPm6APUcPoDvjBQt8BW00zMRut6tFixZ699131bBhQ6Pj+CSuac+FhoY6/2yz2TiHwF2EEdie48UzYA10VLizJUuWZPnYTp065WCSnEMRxE2fffaZy3a/fv0MSmJeb7zxhsv2+PHjDUpy9+AGOPvoAXRnvGCBr6CN9o64uDiX7WbNmhmUxNqio6N1+PBhrVixgvXPMsE1DVgL7QtgHVzPgLXY7fYsHWez2ZSRkZHDaXIGRRAAuAN6ACE30OMZvqJSpUrOP9tsNh08eNDANABgPNpo76B98RwjsOEruJ69w263y2azyeFwmPrlMmAGFEFgqISEBO3fv1/58uVT9erVFRwcbHQk0+AGGL4uPT1dly5dUuHChY2OYgr0eAYAwDfRRsNXMALbO3jxDF9x5MgRl+2goCCDkpgXHRWQVRRBYIgtW7Zo4MCBSkxM1PWvoM1m04MPPqjPPvtMNWrUkCSdOXNGxYoVMzKqz+IGGL5i6dKlOn36tPr37+/8bNKkSZo4caLS09PVsmVLxcTEKDAw0LiQALzqt99+U9myZY2O4bNY/wwAAN/Fi+ecd/bsWX3xxRcaOnSo0VFgcXRU8I4ZM2Zk+djnn38+B5PkHIogyHWJiYlq3LixatasqeHDh6tmzZrOz6dOnapffvlFu3fv1jfffKMzZ87otddeMzgxrIoeQN4RGhqqrl27asiQIZKkzZs36+GHH9aECRNUs2ZNjRkzRu3bt9eUKVMMTgrAUykpKZo0aZIiIiL0559/Gh3HZ7H+GWA9MTExWrJkia5cuaJWrVrpmWeeMTqS6fzwww86ffq0Hn30Uednc+bM0fjx43Xx4kV17txZ77//vvLnz29gSt927dq1LM/bDhhh7dq1ioiI0Ndff61ChQrp9OnTRkfyef+7huuyZcuUnp7u3Ofn56cOHToYFc0U4uLi9NBDDylPnjxGRzG1/53i7nbMPP0dRRDkuu7duys9PV1fffWVy5A1SXI4HOrSpYsSExN19OhRLV++XM2bNzcoqbklJSUpIiJCkydPNjqKz6IHkHeUKFFCK1euVP369SVJI0aMUGJiolasWCHprxu5F154QQcOHDAyps8LDAy86TfxVs6cOZMLaXA3S01N1bPPPqvVq1crX758Gj16tIYOHarXX39dkydPVt26dTV8+HD16NHD6KgAkCtmzZqlIUOGqFq1aipYsKB27dqlESNG6N133zU6mqm0b99eLVq00KhRoyRJu3btUoMGDdS/f3/VrFlT7777rp5++mm9/vrrxgb1YX5+fjp27JhKlCghSRo5cqReeeUVZk/IBl48e8/Ro0cVFRWlqKgoJScnq2fPnurTp49atWqlvHnzGh3Pp3377bcaO3asfvrpJ0mSv7+/Ll686Nxvs9kUExOjrl27GhXR5934uwhkhiIIcl3x4sW1fPlyPfDAA7fc/+OPP6px48Zavny52rZtm8vpzO3ixYuaP3++IiIiFB8fr1q1amn37t1GxzK13bt3q06dOkbH8GkFCxbUvn37VKFCBUlSo0aN1K1bN40cOVLSX8WmWrVqudzM4Wb/O4zX4XBo8ODBmjBhwk03cwzvRU57+umntWLFCnXr1k0rV65UYmKi2rZtK7vdrtdee01NmjQxOqLPu3TpklatWqXQ0FD5+/u77Dt37pxiY2PVtm1bejvDJyQnJ6ts2bLy8/MzOorPql27trp37+4c1fXFF1/o6aef5t7GTaVLl9bSpUudz4FjxozRhg0btHHjRknSwoULNX78eCUmJhoZ06fZ7XalpKQ47w8DAgK0Y8cOVa5c2eBk5sKLZ89dvXpV33zzjWbPnq3vv/9e7dq1U1hYmHr16qWdO3eqVq1aRkc0hU6dOqlz587OqVT9/f21c+dO5zX9zjvvKDY2VsuWLTMypk+78XcRyAxjhXLAhAkTFBoaqocfftjoKD7p/PnzKlmyZKb7S5Uqpbx581IAccOmTZsUERGhBQsWKC0tTcOHD1dkZCQLzWfT+fPnNW/ePM2ePVvbtm1jmqw7KFu2rJKSklShQgVduHBBO3fu1NSpU537T58+rUKFChmY0BxuLG4899xzevzxx3mo9TLa6Dtbvny5oqOj1bJlSw0dOlSVK1fW/fffr3/9619GRzONjz/+WEuWLFGnTp1u2hcQEKAZM2bo6NGjzmkEkX1c056rWLGiqlWrpjfffFNdunQxOo5POnjwoEs7HRYWpoEDB+rYsWMqXbq0gcnMJTU11eU5cMOGDWrfvr1z+8EHH9TRo0eNiGZa9GnNnk8++UTPPfecy2c///yzy4vnyMhIiiC3UbZsWQUHB6t3796aP3++c/3HXr16GZzMXHbt2nXbUYXt27dndo8syMqMCnDPr7/+qiVLlig5OVlXrlxx2WfWqc6ZTDIHREVFqW3bturYsaPRUXxSUFCQtmzZkun+//znP0xLlAUnTpzQO++8o+DgYHXt2lVFixZVbGys7Ha7wsPDKYBkQ1xcnPr166fSpUtr8uTJatmypeLj442O5fO6deumYcOG6fPPP9dTTz2lUqVKufQU37p1q2rUqGFgQuC/aKPv7Pfff3eu11WxYkUVKFBAvXv3NjiVuXz55ZcaNmxYpvuHDRt20yKOyB6uac+tX79eo0ePVkxMjNFRfNbly5d1zz33OLftdrvy5cuntLQ0A1OZT8mSJXXo0CFJ0pUrV7R9+3aXe8bz588zdQ5yxa5du9S0adNM97dv315bt27NxUTmk56eLpvNJpvNxkhCDxw7dsxlZPD69etVvnx553bhwoX1xx9/GBHNVPr3768uXbrc9h9k3dq1a1WjRg3NmjVL7733ntavX6+oqChFRkZqx44dRsfLNkaC5IBDhw4pLS1N69evNzqKT+rZs6dGjBihGjVq3DTN0K5du/TSSy+pb9++BqUzj6CgIHXt2lXTp09XmzZtWCAvm1JSUhQdHa2IiAidO3dO3bt31+XLl/XNN98whDeLxo0bp99++03PP/+8SpUqpS+++MLlRnjevHm8nILPoI2+M4fD4bKwoJ+fnwoWLGhgIvM5cOCA6tWrl+n+unXrsk6Sl3BNe6558+Zq3ry5BgwYYHQUnzZ27FiXka1XrlzRpEmTVKRIEednZu0ZmVseeeQRjR49Wm+//ba++eYbFSpUyGUUV0JCgqpUqWJgQnMYN26c87t4q++hxHfxTnjx7Lnff/9dX331lSIiIvTCCy+offv26t27Nz3y3VSsWDH9/PPPqlixoiTdNG38gQMHWPMnC/z9/Xle8aJXXnlFL730kt544w35+/vrq6++UokSJfTEE0+oXbt2RsfLNtYEQa67dOmSWrVqpf/85z9q06aNatasKYfDoaSkJK1Zs0aNGjXS2rVr+QG7g+DgYF2+fFlhYWHq06ePc+RH3rx5mYMzizp27Ki4uDh16NDB+WPu5+fHOYRPuHE+WCC32O121alTx1kISUhIUHBwsPLly+dy3Pbt242IZwr+/v6KjY1Vw4YNb7l/27ZtatGihc6fP5/LyQBkR4sWLbL0Yo9i3O2dOnVKXbp00caNG1W4cGFFR0e79M5t1aqVmjRpokmTJhmY0rfxXfSOMmXKaM6cOWrduvUt969atUr9+vXTsWPHcjmZOf3yyy+KiorSZ599pt9++029evVS//791bJlS0aJ3EHPnj31559/asmSJbfc/+ijj+qee+5htOZtsCaI9/n7+2vHjh2qUqWKAgMDtXHjRtWuXVs7d+7UP/7xDx0+fNjoiNnCSBA3JScnu2xfXwgYWVegQAGtX79eU6dO1bx587RhwwZJUrVq1fTPf/5Tw4cPZ6HQLNi7d69zLZAHH3xQ1atXd05XQu+LrFm+fLmef/55DR48WNWqVTM6juVs2LBBFy9eVEhIiHOOWGRuxIgRLtv07HMfbbR3XF/497p//OMfBiUxr9q1a2vNmjWZFkFWrVql2rVr53Iq8+Ga9lxoaKjLfeG6desMTGNesbGxRkewhPvuu09xcXH6448/VLhw4Zteji5cuFCFCxc2KJ058F30jmbNmmnGjBmZFkFmzJihZs2a5XIq86pSpYr++c9/asKECVq5cqUiIiL06KOPqnDhwjp9+rTR8XzaqFGjFBISom7duunll19W9erVJUn79u3T22+/rTVr1mjz5s0Gp/RtvP/yvnvuuce5Dkjp0qX1yy+/OJ9dTp06ZWQ0jzASxE12u102m00Oh0M2m40Fk+ETLly4oHnz5ikqKkrx8fFq3ry5wsLC1LlzZxUvXtzoeD4rPj5eERERiomJUc2aNdWnTx/17NlTpUuXZiSIG95++21duHBBEydOlPTXVDrt27fXqlWrJEklSpTQ2rVreeF3B6GhoVk6jp59maONhq/45JNPNGLECM2fP1+PPvqoy76lS5eqV69emjJligYNGmRQQnPgmvbcjWvP/O/i3vCepKQkRUREsHitG86ePauff/5ZklS1alUVLVrU2EAWwXcxa3766SeFhISoY8eOt3zx/N1332nz5s1q0KCBwUnN6+TJk/r8889v6uiFm/373//Wk08+qTNnzrh8HhgYqNmzZ6tz587GBDOJO40E4XfRfZ07d1aHDh301FNP6aWXXtK///1v9e/fX4sXL1ZgYKDWrFljdMRsoQiCXLdlyxY1bNgw02GRly9f1r///W917949l5NZw/Uf+M8//1xnzpzR1atXjY7k8y5evKiYmBhFRkZqy5YtysjI0JQpUxQeHi5/f3+j4/m8Bg0aaNSoUerRo4ekv3rx9evXT6tXr1bNmjXVt29fFSpUSAsWLDA4KYCsuHTpklatWqXQ0NCbfgPPnTun2NhYtW3bllGbd9C7d2/NnTtXwcHBqlGjhqS/RnHu379f3bt317x58wxOCMATFy9e1Pz58xUREaH4+HjVqlVLu3fvNjqWzzt8+LCGDBmilStX6vqrCJvNpnbt2mnmzJnOefGRdXwXs4cXzzmLF8/u+fPPP7Vy5UrnmnHVqlXT3//+d91zzz0GJ/N9GzZsUNOmTV3WNOR30TMHDx7UhQsXVLduXV28eFEvvviiNm/erGrVqmnKlCkKCgoyOmK2UATJAWlpaaxncRt+fn46duyYs0obEBCgHTt2OOe9P378uMqUKUNvPw+lp6dryZIlLvPs4s727dvnLCKdPXtWbdq0yXR+TvwlMDBQmzdvVs2aNSVJAwYMUEZGhubMmSPprxE33bp109GjR42MaXo8SHju2rVrWrZs2U098+Fq+vTpWrJkidauXXvL/a1bt1bnzp01dOjQXE5mPgsWLNDcuXN14MABORwOVa9eXWFhYXT08BKu6ayJiYnRkiVLdOXKFbVq1UrPPPOM0ZFM7fp0tAsWLFBaWpqGDx+uJ5980rk+HzJ39OhRPfjgg8qbN6+effZZ571jYmKiZs2apfT0dP34448qV66cwUnNge+i53jx7F28eIbR+F3MXfPmzVOnTp1M85tJEcSLLl++rJkzZ+rdd99VSkqK0XF81o1D1W5c/Pf48eMqXbq0rl27ZmRM09mzZ49L4cjPz4/phzyQkZGhpUuXKjIykiLIHdx4DQcHB2vYsGHOlyzJycmqUaOG0tLSjIxpSjxIeMfPP/+syMhIRUdH6+TJk4yQu4NGjRpp7Nix6tix4y33f/vtt5owYYK2bNmSy8mAv3BNZ92sWbM0ZMgQVatWTQULFtSuXbs0YsQIvfvuu0ZHM5UTJ04oOjpakZGR+uOPP9SrVy+FhYUpJCSEKVTdMHDgQP38889auXKlChQo4LIvLS1N7dq1U7Vq1TR79myDEvo+vovwRbx4zr4ZM2Zk6bjnn38+h5OYF7+LxrmxU7uvY2F0N12+fFmvv/66Vq9erXz58unll19W586dFRUVpTFjxsjPz0/Dhw83OqbpsbDRnX3//fcaMWKEfvzxR0lSkyZN9Oeff7oMK1+5cmWmi73h9vz8/NS5c2eGQWdBlSpVFBcXp8qVKys5OVn79+93WUjw119/1b333mtgQvO51YNEZGQkDxJuSEtL08KFCzV79mxt2rRJDz/8sMaNG6fHHnvM6Gg+78CBA6pXr16m++vWrevsMYlbGzdunEaPHq1ChQpJklJTUxUYGGhwKnPjms6emTNnavz48Ro/frwk6YsvvtDTTz9NEcRNQUFB6tq1q6ZPn642bdrIbrcbHcmUVqxYoZiYmJsKIJJUsGBBTZw4UT179jQgmXnwXfQOXjx77lYvnmNjYxUSEqLw8HCeW7Jo6tSpLttHjx5V6dKlXaZ2stlsfBdvg99F45htXAVFEDeNGzdOH3/8sVq3bq3NmzerW7duGjBggOLj4zVlyhR169Yt07UuAG/68MMP1adPH5fP1q9fr6CgIDkcDs2YMUOzZs2iCHIbx44d08yZMzVp0iRJ0v/93//pzz//dO7PkyePvv76a5UtW9aoiKYwZMgQDR06VN9//73i4+MVEhLi0tti3bp1ql+/voEJzYEHCe/48ccfNXv2bM2fP19VqlTRE088oc2bN+vDDz+kF1AWpaen6+TJk6pQocIt9588eVLp6em5nMpcJk2apKFDhzqLIEFBQabqJeVLuKY9c/DgQZfF0MPCwjRw4EAdO3ZMpUuXNjCZuQQFBWnjxo2qUKGCgoKCaJOz6dSpU7dd86Ny5co3rc8AV3wXvYMXz57jxbN3HDp0yGXb399fGzZs4J7RDfwuIqsogrhp4cKFmjNnjjp16qTdu3erbt26Sk9P186dOxm94IbExETnlGEOh0N79+7VhQsXJP11c4w727p1q8aMGePyWbly5ZwLFPXp00cdOnQwIpppfPjhh0pNTXVu79y5U+Hh4SpWrJgkafny5Zo6dSprMNzBU089JT8/Py1dulTNmjVz9ja97vfff1d4eLhB6cyDBwnP1a1bV+fOnVNYWJg2b97snBJw9OjRBiczl9q1a2vNmjVq2LDhLfevWrWK6Rbv4MZeUWbrJeUruKY9d/nyZZd5mu12u/Lly8cUlW7au3evc5Tmgw8+qOrVq6t3796SGMHujtKlSysxMTHTNT92796tUqVK5XIqc+G76B28ePYcL57hK/hdRFZRBHHTr7/+6nwpUKdOHeXPn1/Dhw/nwnJTq1atXF4IXF/Q0mazyeFwcD6z4Ndff1WRIkWc25999pnLQ0OxYsV0+vRpI6KZxrfffnvTUOgXXnjBefPbpEkTjRgxgiJIFoSHh2da6Pjwww9zOY058SDhuX379qlHjx4KDQ2lh7gHwsPDNWLECNWuXfumBaeXLl2qSZMmacqUKQalw92Ea9o7xo4d6xyVJElXrlzRpEmTXO4juabvrGnTpmratKlmzJihefPmKSoqShkZGXr22WcVFhamzp07q3jx4kbH9GmdO3fWSy+9pLVr1950rk6cOKFRo0YxFW0W8F2EL+DFM3wJv4vIChZGd5Ofn59SUlKcF4+/v78SEhJUqVIlg5OZx5EjR7J03PURDbi1EiVKaMGCBWrRosUt98fGxqpbt246efJk7gYzkcDAQO3atcvZG61Lly6aNWuWSpYsKUk6fPiwatWq5TJFFpCTrj9ILFy40Pkg8fLLLyshIUE1a9Y0Op7P++233xQdHa2oqCilpaWpV69eeuKJJ9S4cWPt2LGDl6hu6N27t+bOnavg4GDVqFFD0l8Pu/v371f37t01b948gxP6Nj8/P+3fv1/FixeXw+FQ+fLltXHjxpumgQkICDAmoElwTXuuRYsWd3wZZbPZtG7dulxKZC1JSUmKiIjQ559/rjNnzujq1atGR/Jpqampaty4sVJSUtS7d28FBwfL4XAoKSlJc+fOValSpRQfH+8clY2s47voGX9/f+3cuZORINl04cIF54vn+Ph4NW/enBfP2cR30Xv4Xcx5Zvu+UgRxk91uV/v27ZU/f35Jf/WIbNmypcswc0lavHixEfFwF+nYsaOKFy+uyMjIW+7v37+/Tp06pW+//TaXk5lH4cKF9f3332e6XsVPP/2khx9+2DlVG27Nbre7jOLKyMgwOpLp8SDhuXXr1ikyMlKLFy/WpUuX9NJLL+nJJ59U9erVjY5mGgsWLNDcuXN14MABORwOVa9eXWFhYerevbvR0Xze9d/F624c5crvpfu4puHL0tPTtWTJEnXp0sXoKD4vNTVVr776qmJiYnT27FlJUtGiRdW9e3f961//ogDiIb6L2WO2F3m+jBfP7jl37pzLdrly5eg442X8LuacOnXqaPny5SpfvrzRUbKEIoibBgwYkKXjoqKicjiJeSUnJ7tsZ7bwKm5v/fr1at26tUaMGKGRI0eqRIkSkv4aSv72229r+vTpWrVqlVq2bGlwUt/VsGFDhYeHa8iQIbfcP2PGDEVHR2v79u25nMxcbhzdxSgu7+JBwjN//PGHvvzyS0VGRmr79u2qU6eOEhISjI4Fi9uwYUOWjmvevHkOJ7Eermn3rF+/Xk2bNlW+fPmMjgK4cDgczhHrxYsXZ/oc5CpePOc8XjxnDR1n4KvOnj2rRYsW6ZdfftHIkSNVrFgxbd++XSVLllTZsmWNjpctFEGQ6+g17j0ffvihhg8frvT0dAUEBMhms+mPP/5Qnjx59N5772no0KFGR/Rp7777rt566y2tX79edevWddm3c+dOtWrVSqNGjdLIkSMNSgj8Fw8SdzZu3DiNHj3aOfd9amqqAgMDnft37NihyMjIm9YCAmBOXNN3ZrfbVaBAATVp0kShoaEKDQ1VkyZNlCcPS0O6g+cX70hLS9Pq1asVGhoqf39/l33nzp1TbGys2rZt65x1ATfju+gdvHiGr6DjjOf4XfS+hIQEtW7dWkWKFNHhw4e1b98+Va5cWa+99pqSk5M1Z84coyNmC0UQL3I4HFqxYoUiIiK0aNEio+PgLnH06FEtWrRIBw4ckCRVq1ZNXbt2Nc1wNCNdvXpVrVu31ubNm9WmTRvnvPf79u3T6tWrFRISorVr1ypv3rwGJ/VdCQkJqlOnjux2e5aO37Nnj2rUqMHLF+QIPz8/HTt2zDkyLiAgQDt27GBqAzfxIAFYx5EjR7Ru3Tpt2LBBsbGxSk5OVqFChdS0aVNnUeTBBx/Mcjt+t2LUq3dMnz5dS5Ys0dq1a2+5v3Xr1nrssccyHaUNvovewotnz3G/CF/B76L3tW7dWg0aNNA777zjMl3g5s2bFRYWpsOHDxsdMVsognjBoUOHFBkZqejoaJ08eVKtW7dmHQbAJK5cuaIpU6Zo/vz52r9/v6S/Ckm9evXS8OHD6Yl2B35+fkpJScnyOhW8lM4cDxKes9vtSklJcRZBmN85e3iQ8BzXs3f88ssvmjRpknP9swoVKris05UnTx59//33zk4MuLODBw8qNjZWsbGx2rBhg3799Vf5+/s712YAclKjRo00duxYdezY8Zb7v/32W02YMEFbtmzJ5WQA3MX9ImBdRYoU0fbt21WlShWXZ+ojR46oRo0aunTpktERs4WuuNl0+fJlLVq0SBEREdq4caMyMjI0efJkDRw4kHkjkSvi4uJctps1a2ZQEnPLly+fRo8erdGjRxsdxZQcDofGjh3rnH7oTq5cuZLDiczr0KFDRkcAJPEQ6w1cz97x/vvvq2TJks7t1NRUjRs3zlnojImJ0dSpU/XRRx8ZFdF0KleuLD8/P9lsNtlsNn3zzTe0zXfAqFfvOXDggOrVq5fp/rp16zpHtwPwbdwvegcdZzxDG50z8ufPf9PaSZK0f//+LHeA9UX8f91N27ZtU0REhObNm6eqVauqT58+mjdvnsqVK6e2bdtSAEGu6devn/PPNptNBw8eNDAN7lbNmjXTvn37snx8SEiIChYsmIOJzIsHCc/ZbDadP39eBQoUcD5IXLhw4aYbONrqzPEg4R1cz96xdu1aRUREuHz2+OOPO0d3VaxYUU8++aQR0UwlOTlZsbGxWr9+vWJjY3Xq1Ck99NBDevjhh/Xtt9+qcePGRkf0afXr13dr1GtISAijXjORnp6ukydPqkKFCrfcf/LkSaWnp+dyKtyNePHsGe4XvYeOM56hjc4ZnTp10oQJE7RgwQJJfz1nJycna9SoUXr88ccNTpd9/AK5qXHjxnruuecUHx/P0HsYisbSc5UqVXJZAI9CkvtiY2ONjmAJPEh4h8PhUPXq1V2269ev77LNg+7t8SDhOa5n7zl8+LDKlCnj3H7yySdVpEgR53bFihX166+/GhHNNCpXrqzU1FQ1bdpUzZo109NPP60HHniA75sbGPXqPbVr19aaNWvUsGHDW+5ftWqVateuncupcDfiWdoz3C96Dx1nPEMbnTPee+89de3aVSVKlFBaWpqaN2+ulJQUhYSEaNKkSUbHyzbuft3UqlUrRURE6MSJE+rTp4/atm3r8hIVgHlER0cbHQGQxIOEt6xfv97oCKbHg4TnuJ69x2636/fff1e5cuUkSVOnTnXZf/z4ceXNm9eIaKaRlpYm6a9zmSdPHuXNm1d+fn4GpzIXRr16T3h4uEaMGKHatWvr0Ucfddm3dOlSTZo0SVOmTDEoHe4mvHj2DPeL3kHHGc/RRueMIkWKaPXq1dq0aZN27typCxcuqEGDBmrdurXR0TzCleOmlStX6ujRo4qKitLgwYOVlpamHj16SBLFEMBkmjdvbnQEQBIPEt7CNe05HiQ8x/XsPdd7jTdq1OiW+1euXKk6derkcipzOXbsmPbu3eucDuudd97RpUuX9H//939q0aKFmjdvroYNG2b5BczdiFGv3jNo0CDFxcWpU6dOCg4Ods6ssHfvXu3fv1/du3fXoEGDDE4Jq+PFs+e4X/QOOs54jjba+65evaqCBQtqx44datq0qZo2bWp0JK+xORwOh9EhzGz16tWKiorS119/rfLly6tr167q2rWrGjRoYHQ0U6tUqZJatmypiRMnukyDAABW1KJFC7cL6XPnzlXp0qVzKBGA7OJ69p5PP/1Uw4YN04IFC9ShQweXfUuXLlXPnj01bdo0PfXUUwYlNKekpCTn+iCrVq2SJJ09e9bYULirLFiwQHPnztWBAwecU1mGhYWpe/fuRkezhJYtWyo0NFQvvvhilgvydxM/Pz+3XjwHBATw4hk5wm63a9CgQVm+Tj/88EMlJibyXUSOq1y5sr7++mvVq1fP6CheRRHES1JTU/XFF18oMjJSCQkJzDfuoddff12HDx/Whg0bmK8TAJAlLHIJWE+vXr0UExPj0mt837592rdvnx5//HHngo3ImuPHjztHhaxfv14HDhxQ/vz5ndNmATC//v376/Dhwzp48KCSk5ONjuNzePEMX0HHGfiqiIgILV68WJ9//rmKFStmdByvoQiSA7Zv385IEAAActmRI0dctpnvGbCG+fPna/78+dq/f78kqVq1aurVq5d69uxpcDLfd+LECcXGxjoLH/v371fevHnVqFEjhYaGKjQ0VCEhIcqfP7/RUQF42blz5xQQEGB0DJ/Di2cAuL369evr559/1tWrVxUUFKR77rnHZf/27dsNSuYZiiBuurEnRYUKFQxKAgAAAACZs9vtyps3rx544AFn0eOhhx5ibnYYghGbAAD4vjfeeOO2+8ePH59LSbyLIoibuHHz3IQJE1y2x40bZ1AS67Pb7WrRooXeffddNWzY0Og4AJBjWOQSAG62cuVK/d///d9NPfiuS0pKUkREhCZPnpzLyXA3YsSm5+bMmeOy3bdvX4OSAABgLhRBkOsGDBjg/LPNZlNkZKSBaawtOjpahw8f1ooVKxQfH290HFMKDw9XaGio+vTpY3QUALfBIpeA9dD5KGdcvHhR8+fPV0REhOLj41WrVi3t3r3b6FgAsiA0NNT5Z5vNpnXr1hmYBgBgZdu2bVNSUpIkqXbt2qpfv77BiTxDEQQAbqNFixY6fPiwihYtqh07dhgdB0AmWOQSsB56jXvXpk2bFBERoQULFigtLU3Dhw/Xk08+qeDgYKOj4S7AiE0AAMzhxIkT6tmzp2JjY1W0aFFJ0tmzZxUaGqr58+dnueOhr6EIAp/icDi0YsUKRUREaNGiRUbHAZwSExNVq1Yto2MAyASLXALAzU6cOKHo6GhFRkbqjz/+UK9evRQWFqaQkBDt3LmTexvkGkZs5qz09HRdunRJhQsXNjoKAMDkevTooYMHD2rOnDmqWbOmpL/eifXr109Vq1bVvHnzDE6YPXSrgE84dOiQIiMjFR0drZMnT6p169ZGR/J54eHhLttMK5azeEkA+LbY2FijIwDwInqNe0dQUJC6du2q6dOnq02bNlk+n4C3ORwOjR07NssjNq9cuZLDicxp6dKlOn36tPr37+/8bNKkSZo4caLS09PVsmVLxcTEKDAw0LiQAABTW7FihdasWeMsgEh/vRP74IMP9Pe//93AZJ7hKQGGuXz5shYtWqSIiAht3LhRGRkZmjx5sgYOHKiAgACj4/k8poTwXFxcnMt2s2bNDEoCAAD+V/369d3qNR4SEkKv8VsICgrSxo0bVaFCBQUFBTH1FQzTrFkz7du3L8vHh4SEqGDBgjmYyJymTJmirl27Orc3b96scePGacKECapZs6bGjBmjiRMnasqUKQamBACY2bVr15Q3b96bPs+bN6+uXbtmQCLvYDos5Lpt27YpIiJC8+bNU9WqVdWnTx/16NFD5cqVY1g+clWlSpWcf7bZbDp48KCBaQAAwHWs8+M919cCWbhwoapXr67evXvr5ZdfVkJCgksPPwC+r0SJElq5cqVzcdoRI0YoMTFRK1askCQtW7ZML7zwgg4cOGBkTACAif3jH//Q2bNnNW/ePJUpU0aS9Ntvv+mJJ55QYGCgvv76a4MTZg9FEOS6PHny6LnnntMzzzyjGjVqOD/PmzcvRRA3XLp0SatWrVJoaKj8/f1d9p07d06xsbFq27at8ufPb1BCAACA7GGdH++7cOGC5s2bp6ioKMXHx6t58+YKCwtT586dTbvAJXC3KViwoPbt26cKFSpIkho1aqRu3bpp5MiRkqQjR46oVq1aunjxopExAQAmdvToUXXq1El79uxR+fLlnZ/VqVNHS5YsUbly5QxOmD0UQXJApUqV1LJlS02cONFZMcN/tW3bVj/88IM6duyoPn36qG3btrLZbBRB3DR9+nQtWbJEa9euveX+1q1b67HHHtOQIUNyORkAAAB8WVJSkiIiIvT555/rzJkzunr1qtGRAGRB1apV9cEHH6ht27a6cOGC7r33Xq1bt05NmzaVJG3fvl1t27bVyZMnDU4KADAzh8OhNWvWaO/evZKkmjVrmn79ZlbGywH9+vVTRkaG80YErlauXOlcvHLw4MEqXbq0XnjhBUlyu8ff3ezLL7/UsGHDMt0/bNgwffbZZ7kXyIR++OEHffvtty6fzZkzR5UqVVKJEiU0aNAgXb582aB0AAAAOaNmzZqaPHmyfvvtN8XExBgdB0AWdevWTcOGDdPnn3+up556SqVKlVKTJk2c+7du3eoy2wIAANlhs9nUpk0bPffcc3ruuedMXwCRGAkCH7B69WpFRUXp66+/Vvny5dW1a1d17dpVDRo0MDqaTwsMDNTOnTudQ6FvlJycrHr16ik1NTWXk5lH+/bt1aJFC40aNUqStGvXLjVo0ED9+/dXzZo19e677+rpp5/W66+/bmxQAAAAAHe9tLQ0Pf3001q6dKlKlSqlTz75RA8//LBzf2hoqNq1a+d8vgEAwF3PP/+8qlatqueff97l85kzZ+rnn3/WtGnTjAnmIYog8Bmpqan64osvFBkZqYSEBGVkZBgdyaf5+/srNjZWDRs2vOX+bdu2qUWLFjp//nwuJzOP0qVLa+nSpXrggQckSWPGjNGGDRu0ceNGSdLChQs1fvx4JSYmGhkTAAAgW+x2u2w2mxwOh2w2G/fXAAAAuK2yZctqyZIlN71v3L59uzp16qRff/3VoGSeyWN0ALOZMGGCy/a4ceMMSmJeFSpU0E8//aR7771X0l+VxL59+yowMNA5zGr79u0Gp/R9tWvX1po1azItgqxatUq1a9fO5VTmkpqaqpIlSzq3N2zYoPbt2zu3H3zwQR09etSIaAAAAB47dOiQ0REA5IINGzbo4sWLCgkJUWBgoNFxAAAmdvr0aRUpUuSmzwMCAnTq1CkDEnkHa4K46dChQ85/Dh8+bHQcU/r1119deqG9+uqrN11ETIV1Z+Hh4Zo4ceJNa1pI0tKlSzVp0iSFh4cbkMw8SpYs6Xw5cOXKFW3fvt1lTt3z588rb968RsUDAADwSFBQkMs/AMzt7bff1tixY53bDodD7dq1U2hoqB599FHVrFlTe/bsMTAhAMDsqlatqhUrVtz0+fLly1W5cmUDEnkHI0HcFBUVZXQEy2FGtuwZNGiQ4uLi1KlTJwUHBzsXwNu7d6/279+v7t27a9CgQQan9G2PPPKIRo8erbffflvffPONChUq5DKnbkJCgqpUqWJgQgAAgOxJSEhQnTp1ZLdnrd/bnj17VKNGDeXJwyMi4KtiYmJc1vtYtGiR4uLi9P3336tmzZrq27ev3njjDS1YsMDAlAAAMxsxYoSGDh2qkydPqmXLlpKktWvXavLkyZo+fbrB6bKPNUE8cPbsWf3888/Kly+fKlWqJH9/f6MjmYLdbldKSopKlCgh6a+1LXbu3GnqaqKRFixYoLlz5+rAgQNyOByqXr26wsLC1L17d6Oj+bxTp06pS5cu2rhxowoXLqzo6Gh16dLFub9Vq1Zq0qSJJk2aZGBKAAAA9/n5+SklJUXFixfP0vEBAQHasWMH9+SADwsMDNTmzZtVs2ZNSdKAAQOUkZGhOXPmSJLi4+PVrVs3pvQFAHhk1qxZmjRpkn7//XdJUqVKlTR+/Hj17dvX4GTZRzefbDh8+LCGDBmilStXOkcx5MmTR126dNG0adOcawxcvnxZ+fPnNzKqz5o9e7YKFy4sSUpPT1d0dLTuu+8+l2Oef/55I6KZTvfu3Sl4ZNN9992nuLg4/fHHHypcuLD8/Pxc9i9cuND5PQUAADATh8OhsWPHqlChQlk6/sqVKzmcCICn0tPTXd4x/PDDDxo2bJhzu0yZMqaerx0AYLy0tDT169dPgwcP1smTJ3X8+HGtXr3aZU1dM2IkiJuOHj2qBx98UHnz5tWzzz7r7IGRmJioWbNmKU+ePPrpp58UFxenpKQkl6Gq+EvFihVls9lue4zNZtPBgwdzKZE5jRs3TqNHj3Y+2KamprIIngeuj+yS/pr/sGjRosYGAgAA8ECLFi3ueM99o7lz56p06dI5lAiAp+6//34NGzZM/fv3V3JysipWrKjdu3erVq1akqTNmzere/fu+vXXXw1OCgAwq7///e/q0qWLnnnmGZ09e1bBwcHKmzevTp06pSlTpmjw4MFGR8wWiiBuGjhwoH7++WetXLlSBQoUcNmXlpamdu3a6dq1a9q6davmz5+vf/zjHwYlhdX5+fnp2LFjzmnFmMIge241sstms6ldu3aaOXOmKlasaGxAAAAAAJD06aefavjw4erRo4fi4+NVtGhRbdq0ybn/n//8p/7zn/9o6dKlBqYEAJjZfffdpw0bNqh27dqaPXu23n//ff3000/66quvNG7cOCUlJRkdMVuYDstNK1asUExMzE0FEEkqWLCgJk6cqBYtWmj27NkUQJCjbqxfUs9039GjR9WkSRPlzZtXEydOvGlkV0hIiH788UeVK1fO4KQAAAAA7nZPPfWU/Pz8tHTpUjVr1kzjx4932f/7778rPDzcoHQAACv4888/neter1q1Sl26dJHdbleTJk105MgRg9NlHyNB3JQ/f3798ssvmb4U/fXXX1W5cmXm1L2N64u2XWfmRXWMxALznsvKyK5q1app9uzZBiUEAAAAAAAAckfdunX15JNP6rHHHlOdOnW0YsUKhYSEaNu2berQoYNSUlKMjpgtjARxU+nSpZWYmJhpEWT37t0qU6ZMLqcyl6ioKOefbTYbRZBsstlsOn/+vAoUKCCHwyGbzaYLFy7o3LlzLscFBAQYlND3ZWVkV8+ePQ1IBgAAAAAAAOSucePGKSwsTMOHD1erVq0UEhIi6a9RIfXr1zc4XfYxEsRNw4YN07p167R27VoVL17cZd+JEyfUpk0bhYaGatq0acYExF3Dbre7LHZ5vRBy43ZGRoYR8UwhKyO7qlatqkuXLuVyMgAAAABwdf0ZkGc9AEBOSklJ0bFjx1SvXj3Z7XZJ0pYtWxQQEKDg4GCD02UPI0HcNH78eC1btkxVqlRR7969FRwcLIfDoaSkJM2dO1elSpXSuHHjjI6Ju8D69euNjmB6WRnZVapUqVxOBQAAAAA3O3TokNERAAB3gVKlSt30PqxRo0YGpfEORoJkQ2pqql599VXFxMTo7NmzkqSiRYuqe/fumjRpku69915jAwLIEkZ2AQAAAAAAANZGEcQDDodDJ0+elCQVL17cZSoiAL4vNTVVjRs3VkpKSqYju+Lj41WsWDGjowIAAAC4iyUkJKhOnTrOaUnuZM+ePapRo4by5GECEAAAKIK46cSJEypRokSm+9PT07V9+3bTDxGC72M+WO+43ciuf/3rXxRAAAAAABjOz89PKSkpN41gz0xAQIB27NihypUr53AyAAB8H0UQN/n5+enYsWPOQsjf/vY3LVu2TOXLl5ckHT9+XGXKlOGFNHLckSNHXLaDgoIMSmINjOwCAAAA4KvsdrsGDRqkQoUKZen4Dz/8UImJiRRBAAAQC6O77caa0eHDh3X16tXbHgP3JScnq2zZsvLz8zM6is+i6OG5tLQ0rV69WqGhofL393cZ5XXu3DnFxsaqbdu2yp8/v4EpAQAAANztmjVrpn379mX5+JCQEBUsWDAHEwEAYB4UQXIAPcg9V7FiRVWrVk1vvvmmunTpYnQcn8N8sN7xySefaMmSJerUqdNN+wICAjRjxgwdPXpUQ4YMMSAdAAAAAPwlNjbW6AgAAJhW1t6gArls/fr1Gj16tGJiYoyO4pPq16+v06dPZ/n4kJAQJScn52Aic/ryyy81bNiwTPcPGzZMn332We4FAgAAAAAAAOBVdAt3k81m0/nz51WgQAHngtQXLlzQuXPnJMn5f+GZ5s2bq3nz5howYIDRUXySw+HQ2LFjszwf7JUrV3I4kTkdOHBA9erVy3R/3bp1deDAgVxMBAAAAAAAAMCbKIK4yeFwqHr16i7b9evXd9lmOizkNOaD9Y709HSdPHlSFSpUuOX+kydPKj09PZdTAQAAAAAAAPAWiiBuWr9+vdERTC80NNSlULRu3ToD05gT88F6R+3atbVmzRo1bNjwlvtXrVql2rVr53IqAAAAAAAAAN5CEcRNzZs3NzqC6fXv39/oCIAkKTw8XCNGjFDt2rX16KOPuuxbunSpJk2apClTphiUDgAAAAAAAICnbA6Hw2F0CAAwSu/evTV37lwFBwerRo0akqS9e/dq//796t69u+bNm2dwQgAAAAAAAADZRRHETXa7XTabzbn2R0ZGhtGRAHhowYIFmjt3rg4cOOBc9ycsLEzdu3c3OhoAAAAAAAAAD1AEcdORI0dctoOCggxKYl6VKlW64+LxNptNv/zySy4lAgAAAAAAAABYEUUQ5Lrp06dnuu/w4cP6+OOPdfnyZUbZAAAAAAAAAAA8QhHEDQkJCapTp47sdnuWjt+zZ49q1KihPHlYf/5Ozpw5o4kTJ2rWrFlq3Lix3n77bTVp0sToWLA4prcDAAAAAAAArI0iiBv8/PyUkpKi4sWLZ+n4gIAA7dixQ5UrV87hZOaVlpamKVOmaPLkyQoKCtK//vUvPfLII0bHwl2C6e0AAAAAAAAAa2OIghscDofGjh2rQoUKZen4K1eu5HAi88rIyNCnn36qN954QwUKFNCMGTPUu3fvO64VAngTRQ8AAAAAAADA2iiCuKFZs2bat29flo8PCQlRwYIFczCROS1YsECvvfaazp49qzFjxmjw4MHKly+f0bFwl2F6OwAAAAAAAMD6mA4Luc5ut6tgwYLq1auXAgICMj1uypQpuZgKdxumtwMAAAAAAACsjy7NyHXNmjWTzWbTL7/8YnQU3MWY3g4AAAAAAACwPkaCALgrtWjRwu01aObOnavSpUvnUCIAAAAAAAAA3kYRBD4nKSlJERERmjx5stFRAAAAAAAAAAAmlrUVgYEcdvHiRUVEROihhx5S7dq1tWLFCqMjAQAAAAAAAABMjiIIDLVp0yaFh4erZMmSGjRokB566CElJiZq9+7dRkcDAAAAAAAAAJgcRRDkuhMnTuidd95RcHCwunbtqqJFiyo2NlZ2u13h4eEKDg42OiIAAAAAAAAAwALyGB0Ad5+goCB17dpV06dPV5s2bWS3U4sDAAAAAAAAAHgfb5+R64KCgrRx40bFxcVp//79RscBAAAAAAAAAFgURRDkur179+qLL77QsWPH9OCDD6phw4aaOnWqJMlmsxmcDgAAAAAAAABgFTaHw+EwOgTuXhcuXNC8efMUFRWl+Ph4NW/eXGFhYercubOKFy9udDwAAAAAAAAAgIlRBIHPSEpKUkREhD7//HOdOXNGV69eNToSAAAAAAAAAMDEKILA56Snp2vJkiXq0qWL0VEAAAAAAAAAACZGEQQAAAAAAAAAAFhSHqMD4O5jt9tls9nkcDhks9mUkZFhdCQAAAAAAAAAgAVRBEGuO3TokNERAAAAAAAAAAB3AabDAgAAAAAAAAAAlmQ3OgDuLgkJCbp27VqWj9+zZ4/S09NzMBEAAAAAAAAAwKoYCYJc5efnp5SUFBUvXjxLxwcEBGjHjh2qXLlyDicDAAAAAAAAAFgNa4IgVzkcDo0dO1aFChXK0vFXrlzJ4UQAAAAAAAAAAKuiCIJc1axZM+3bty/Lx4eEhKhgwYI5mAgAAAAAAAAAYFVMhwUAAAAAAAAAACyJhdEBAAAAAAAAAIAlUQQBAAAAAAAAAACWRBEEAAAAAAAAAABYEkUQAAAAAAAAAABgSRRBAAAAAAAAAACAJVEEAQAAAAAAAAAAlkQRBAAAAAAAAAAAWBJFEAAAAAAAAAAAYEn/D82mDXyFuY0xAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2000x1400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.fallacies import change_lbl\n",
    "from src.metrics import get_metrics\n",
    "from src.plot import plot_metric\n",
    "\n",
    "metric_single, metric_multi = get_metrics(change_lbl, df_res)\n",
    "plot_metric(\n",
    "    metric=metric_single,\n",
    "    title=f'Fallacies: Score gguf_{quant} single label',\n",
    "    file_plot=f'./img/{task}/test_gguf_{quant}_metric_single.png',\n",
    "    file_metric=f'./test_res/{task}/test_gguf_{quant}_metric_single.csv'\n",
    ")\n",
    "plot_metric(\n",
    "    metric=metric_multi,\n",
    "    title=f'Fallacies: Score gguf_{quant} multi label',\n",
    "    file_plot=f'./img/{task}/test_gguf_{quant}_metric_multi.png',\n",
    "    file_metric=f'./test_res/{task}/test_gguf_{quant}_metric_multi.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "# paths = {\n",
    "#     'cocolofa': './Data_jsonl/cocolofa.jsonl',\n",
    "#     'mafalda': './Data_jsonl/mafalda.jsonl'\n",
    "# }\n",
    "# SYSTEM_PROMPT = 'You are an expert in argumentation. Your task is to determine the type of fallacy in the given [SENTENCE]. The fallacy would be in the [FALLACY] Set. Utilize the [TITLE] and the [FULL TEXT] as context to support your decision.\\nYour answer must be in the following format with only the fallacy in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.'\n",
    "# n_sample = 4000\n",
    "# e = 1.5\n",
    "# n_eval = 8\n",
    "# n_eval_step = np.floor((n_sample / 32) / n_eval)\n",
    "\n",
    "# model_name = 'Llama3.18BInstruct'\n",
    "# spl_name = 'spl2'\n",
    "# task_name = 'fallacies'\n",
    "# train_resp = '_train_resp'\n",
    "\n",
    "# train_spl_file = f'./sampling/sample/{task_name}/{spl_name}_train.csv'\n",
    "# val_spl_file = f'./sampling/sample/{task_name}/{spl_name}_val.csv'\n",
    "# test_spl_file = f'./sampling/sample/{task_name}/{spl_name}_test.csv'\n",
    "\n",
    "# outputs_dir = f'./outputs/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}'\n",
    "\n",
    "# test_result_file = f'./test_res/{task_name}/test_res_{model_name}_{e}e{n_sample}{spl_name}{train_resp}.csv'\n",
    "# result_file = f'./test_res/{task_name}/test_res_{model_name}_{e}e{n_sample}{spl_name}{train_resp}.csv'\n",
    "\n",
    "# file_stat_train = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}_stat_train.png'\n",
    "# file_stat_val = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}_stat_val.png'\n",
    "# file_stat_test = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}_stat_test.png'\n",
    "\n",
    "# file_plot_single = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_res_single.png'\n",
    "# file_plot_multi = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_res_multi.png'\n",
    "\n",
    "# file_metric_single = f'./test_res/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_metric_single.csv'\n",
    "# file_metric_multi = f'./test_res/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_metric_multi.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name= \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=0.6\n",
    ")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompt(tokenizer, d):\n",
    "    t = d['prompt']\n",
    "    texts = [tokenizer.apply_chat_template(txt, tokenize=False, add_generation_prompt=False) for txt in t]\n",
    "    return { 'text': texts, }\n",
    "time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{OUTPUT}<|eot_id|>\"\"\"\n",
    "savefile = get_savefile(\n",
    "    task_name='fallacies',\n",
    "    spl_name='spl2',\n",
    "    m_name='meta-Llama-3.1-8B-Instruct',\n",
    "    n_sample=4000,\n",
    "    epoch=2,\n",
    "    train_resp='_train_resp',\n",
    "    outputs_dir=f'./outputs/test_fallacies',\n",
    "    time=time\n",
    ")\n",
    "fallacies, prt_train, prt_val, prt_test = get_data(savefile)\n",
    "data_train, data_val, data_test = get_datasets(tokenizer, prt_train, prt_val, prt_test)\n",
    "prt_train = prt_train.rename(columns={\"prompt\": \"conversations\"})\n",
    "train = apply_chat_template(\n",
    "    Dataset.from_pandas(prt_train),\n",
    "    tokenizer,\n",
    "    chat_template\n",
    ")\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template='llama-3.1'\n",
    ")\n",
    "d_train = Dataset.from_pandas(prt_train).map(\n",
    "    lambda x: formatting_prompt(tokenizer, x),\n",
    "    batched=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, fallacies = load_all_dataset(paths)\n",
    "# spl_data = get_all_spl(data, fallacies, n_sample)\n",
    "# prt_train, prt_val, prt_test = get_prt(spl_data, fallacies, SYSTEM_PROMPT)\n",
    "\n",
    "# prt_train.to_csv(train_spl_file, index=False)\n",
    "# prt_val.to_csv(val_spl_file, index=False)\n",
    "# prt_test.to_csv(test_spl_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = {'prompt': literal_eval, 'answer': literal_eval}\n",
    "prt_train = pd.read_csv(\n",
    "    train_spl_file,\n",
    "    converters=converter\n",
    ")\n",
    "prt_val = pd.read_csv(\n",
    "    val_spl_file,\n",
    "    converters=converter\n",
    ")\n",
    "prt_test = pd.read_csv(\n",
    "    test_spl_file,\n",
    "    converters=converter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompt(data: dict):\n",
    "    text = tokenizer.apply_chat_template(data.get('prompt'),tokenize = False, add_generation_prompt = False)\n",
    "    return { 'text': text, }\n",
    "\n",
    "SAMPLING_PARAMS = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 128,\n",
    ")\n",
    "data_train = Dataset.from_pandas(prt_train).map(\n",
    "    formatting_prompt,\n",
    "    batched=True,\n",
    ")\n",
    "data_val = Dataset.from_pandas(prt_val).map(\n",
    "    formatting_prompt,\n",
    "    batched=True,\n",
    ")\n",
    "data_test = Dataset.from_pandas(prt_test).map(\n",
    "    formatting_prompt,\n",
    "    batched=True\n",
    ").shuffle(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen(txt, model, sampling_params):\n",
    "#     output = model.fast_generate(\n",
    "#         txt,\n",
    "#         sampling_params = sampling_params,\n",
    "#     )[0].outputs[0].text\n",
    "    \n",
    "#     return output\n",
    "\n",
    "# def format_output(answer: str, fallacies: set) -> list:\n",
    "#     s = '<[|]ANSWER[|]>'\n",
    "#     tmp = re.split(s, answer)\n",
    "#     pred= [i for i in tmp if i in fallacies]\n",
    "#     return pred\n",
    "\n",
    "# def zero_shot_gen(data: list[str], model, fallacies: set, sampling_params) -> list:\n",
    "#     res = []\n",
    "#     for i in data:\n",
    "#         out = gen(i, model, sampling_params)\n",
    "#         pred = format_output(out, fallacies)\n",
    "#         res.append(pred)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(p, model, text_streamer):\n",
    "    txt = tokenizer.apply_chat_template(\n",
    "        p,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "    output = model.generate(\n",
    "        txt,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=128,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def format_output(answer: list, labels: set) -> list:\n",
    "    s = '<[|]ANSWER[|]>'\n",
    "    tmp = re.split(s, answer[0])\n",
    "    pred = [i for i in tmp if i in labels]\n",
    "    return pred\n",
    "\n",
    "def zero_shot_gen(\n",
    "    data:Dataset,\n",
    "    model,\n",
    "    labels:set,\n",
    "    text_streamer: TextStreamer\n",
    ") -> list:\n",
    "    res= []\n",
    "    prompt = data['prompt']\n",
    "    for prt in prompt:\n",
    "        out = gen(prt, model, text_streamer)\n",
    "        decoded_out = tokenizer.batch_decode(out)\n",
    "        pred = format_output(decoded_out, labels)\n",
    "        res.append(pred)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class custom_validation_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies, n_step=10):\n",
    "#         super().__init__()\n",
    "#         self.val_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#         self.n_step=n_step\n",
    "#     def on_step_end(self, args, state, control, **kwargs):\n",
    "#         if state.global_step % self.n_step == 0 and state.global_step > 0 :\n",
    "#             model.save_lora('sft_save_lora')\n",
    "#             FastLanguageModel.for_inference(model)\n",
    "#             pred = zero_shot_gen(\n",
    "#                 data=self.val_dataset['text'],\n",
    "#                 model=model,\n",
    "#                 fallacies=self.fallacies,\n",
    "#                 sampling_params=self.sampling_params\n",
    "#             )\n",
    "#             tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#             d = pd.DataFrame().from_records(tmp_pred)\n",
    "#             d['truth_label'] = self.val_dataset['answer']\n",
    "#             d['step'] = np.full((len(d['truth_label']),), state.global_step)\n",
    "#             try:\n",
    "#                 d.to_csv(\n",
    "#                     './validation_res.csv',\n",
    "#                     index=False,\n",
    "#                     mode='a',\n",
    "#                     header=['pred', 'truth_label', 'step']\n",
    "#                 )\n",
    "#             except FileNotFoundError:\n",
    "#                 d.to_csv('./validation_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_step_end(args, state, control, **kwargs)\n",
    "\n",
    "# class custom_test_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies):\n",
    "#         super().__init__()\n",
    "#         self.test_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#     def on_train_end(self, args, state, control, **kwargs):\n",
    "#         model.save_lora('sft_save_lora')\n",
    "#         FastLanguageModel.for_inference(model)\n",
    "#         pred = zero_shot_gen(\n",
    "#             data=self.test_dataset['text'],\n",
    "#             model=model,\n",
    "#             fallacies=self.fallacies,\n",
    "#             sampling_params=self.sampling_params\n",
    "#         )\n",
    "#         tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#         d = pd.DataFrame().from_records(tmp_pred)\n",
    "#         d['truth_label'] = self.test_dataset['answer']\n",
    "#         try:\n",
    "#             d.to_csv('./test_res.csv', index=False, mode='a', header=['pred','truth_label'])\n",
    "#         except FileNotFoundError:\n",
    "#             d.to_csv('./test_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_train_end(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 4, #2\n",
    "    per_device_eval_batch_size= 4,\n",
    "    gradient_accumulation_steps = 8, #4\n",
    "    eval_accumulation_steps= 8,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = e, # Set this for 1 full training run.\n",
    "    # max_steps = 60,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = outputs_dir,\n",
    "    report_to = \"tensorboard\", # Use this for WandB etc\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=n_eval_step,\n",
    ")   \n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = data_train,\n",
    "    eval_dataset=data_val,\n",
    "    # formatting_func=formatting_prompt,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args,\n",
    ")\n",
    "# trainer.get_train_dataloader().shuffle = False\n",
    "# trainer.get_eval_dataloader().shuffle = False\n",
    "# trainer.train()\n",
    "if train_resp == '_train_resp':\n",
    "    trainer = train_on_responses_only(\n",
    "        trainer,\n",
    "        instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "        response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(data):\n",
    "    try:\n",
    "        tmp = data.apply(\n",
    "            lambda x: x['pred'] in x['lbl'],\n",
    "            axis=1\n",
    "        )\n",
    "        tp = tmp.value_counts().loc[True]\n",
    "        score = tp / len(tmp)\n",
    "        fp_fn = len(tmp) - tp\n",
    "        return score, tp, fp_fn\n",
    "    except (KeyError, ZeroDivisionError):\n",
    "        tp = 0\n",
    "        score = 0\n",
    "        fp_fn = len(tmp) - tp\n",
    "        return score, tp, fp_fn\n",
    "\n",
    "def get_recall_multi(data, label):\n",
    "    n_lbl = len(label)\n",
    "    ratio = []\n",
    "    for l in label:\n",
    "        c = data['pred'].value_counts()\n",
    "        try:\n",
    "            ratio.append(min((1/n_lbl) * (c[l] / (len(data)/n_lbl)), 1/n_lbl))\n",
    "        except:\n",
    "            ratio.append(min((1/n_lbl) * (0 / (len(data)/n_lbl)), 1/n_lbl))\n",
    "    rec = sum(ratio)\n",
    "    tp = np.round(len(data)*rec)\n",
    "    fn = len(data) - tp\n",
    "    return rec, tp, fn\n",
    "\n",
    "def get_f1(precision, recall):\n",
    "    try:\n",
    "        f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "    return f1\n",
    "\n",
    "def get_metrics_single(data, labels):\n",
    "    tp_preci = 0\n",
    "    tp_rec = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    res = {}\n",
    "    for l in labels:\n",
    "        df_on_pred = data[data['pred'] == l[0]]\n",
    "        df_on_label = data.apply(\n",
    "            lambda x: x if l[0] in x['lbl'] else np.nan,\n",
    "            result_type='broadcast',\n",
    "            axis = 1\n",
    "        ).dropna()\n",
    "        precision, tp_p, fp_p = get_precision_recall(df_on_pred)\n",
    "        recall, tp_r, fn_r = get_precision_recall(df_on_label)\n",
    "        f1 = get_f1(precision, recall)\n",
    "        n_lbl = change_lbl(l)\n",
    "        res.update({str(n_lbl): (f1, precision, recall)})\n",
    "        fn += fn_r\n",
    "        fp += fp_p\n",
    "        tp_preci += tp_p\n",
    "        tp_rec += tp_r\n",
    "    precision_all_data = tp_preci / (tp_preci + fp)\n",
    "    recall_all_data = tp_rec / (tp_rec + fn)\n",
    "    f1_all_data = get_f1(precision_all_data, recall_all_data)\n",
    "    res.update({\n",
    "        'score_all_data': (f1_all_data, precision_all_data, recall_all_data)\n",
    "    })\n",
    "    return res\n",
    "\n",
    "def get_metrics_multi(data, labels):\n",
    "    tp_preci = 0\n",
    "    tp_rec = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    res = {}\n",
    "    for l in labels:\n",
    "        df_pred_lbl = data.apply(\n",
    "            lambda x: x if x['lbl'] == l else np.nan,\n",
    "            result_type='broadcast',\n",
    "            axis=1\n",
    "        ).dropna()\n",
    "        precision, tp_p, fp_p = get_precision_recall(df_pred_lbl)\n",
    "        recall, tp_r, fn_r = get_recall_multi(df_pred_lbl, l)\n",
    "        f1 = get_f1(precision, recall)\n",
    "        n_lbl = change_lbl(l)\n",
    "        res.update({str(n_lbl): (f1, precision, recall)})\n",
    "        fn += fn_r\n",
    "        fp += fp_p\n",
    "        tp_preci += tp_p\n",
    "        tp_rec += tp_r\n",
    "    precision_all_data = tp_preci / (tp_preci + fp)\n",
    "    recall_all_data = tp_rec / (tp_rec + fn)\n",
    "    f1_all_data = get_f1(precision_all_data, recall_all_data)\n",
    "    res.update({\n",
    "        'score_all_data': (f1_all_data, precision_all_data, recall_all_data)\n",
    "    })\n",
    "    return res\n",
    "\n",
    "def change_lbl(labels: list) -> list:\n",
    "    new_lbl = []\n",
    "    replace_lbl = {\n",
    "        'appeal to nature': 'AN',\n",
    "        'straw man': 'STM',\n",
    "        'false dilemma': 'FD',\n",
    "        'appeal to tradition': 'AT',\n",
    "        'causal oversimplification': 'COS',\n",
    "        'appeal to majority': 'AM',\n",
    "        'ad hominem': 'AH',\n",
    "        'appeal to ridicule': 'AR',\n",
    "        'circular reasoning': 'CR',\n",
    "        'false analogy': 'FA',\n",
    "        'false causality': 'FC',\n",
    "        'appeal to fear': 'AF',\n",
    "        'appeal to worse problems': 'AWP',\n",
    "        'none': 'NONE',\n",
    "        'guilt by association': 'GA',\n",
    "        'equivocation': 'EQ',\n",
    "        'appeal to authority': 'AA',\n",
    "        'hasty generalization': 'HG',\n",
    "        'slippery slope': 'SS',\n",
    "        'ad populum': 'AP'\n",
    "    }\n",
    "    for l in labels:\n",
    "        new_lbl.append(replace_lbl.get(l))\n",
    "    return new_lbl\n",
    "    \n",
    "\n",
    "def get_metrics(data_single: pd.DataFrame, data_multi: pd.DataFrame):\n",
    "    labels_single = data_single['lbl'].value_counts().index\n",
    "    labels_multi = data_multi['lbl'].value_counts().index\n",
    "    scores_single = get_metrics_single(data_single, labels_single)\n",
    "    scores_multi = get_metrics_multi(data_multi, labels_multi)\n",
    "    return scores_single, scores_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "pred = zero_shot_gen(\n",
    "    data=data_test,\n",
    "    model=model,\n",
    "    labels=fallacies,\n",
    "    text_streamer=text_streamer\n",
    ")\n",
    "\n",
    "names_dataset = data_test['datasets']\n",
    "true_labels = data_test['answer']\n",
    "\n",
    "tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "pred_flat = list(itertools.chain.from_iterable(tmp_pred))\n",
    "\n",
    "d_res = {'names': names_dataset, 'pred': pred_flat, 'lbl': true_labels}\n",
    "df_res = pd.DataFrame(data=d_res)\n",
    "df_res.to_csv(test_result_file, index=False)\n",
    "# df_res = pd.read_csv(\n",
    "#     result_file,\n",
    "#     converters={'lbl': literal_eval}\n",
    "# )\n",
    "#metric = get_metrics(df_res, fallacies)\n",
    "df_single = df_res.apply(\n",
    "    lambda x: x if len(x['lbl']) <= 1 else np.nan,\n",
    "    axis=1,\n",
    "    result_type='broadcast'\n",
    ").dropna()\n",
    "df_multi = df_res.apply(\n",
    "    lambda x: x if len(x['lbl']) > 1 else np.nan,\n",
    "    axis=1,\n",
    "    result_type='broadcast',\n",
    ").dropna()\n",
    "metric_single, metric_multi = get_metrics(df_single, df_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stat_sample(\n",
    "    sample: pd.DataFrame,\n",
    "    lst_labels: set,\n",
    "    title: str,\n",
    "    savefile=None,\n",
    ") -> None:\n",
    "    width = 0.3\n",
    "    fig, ax = plt.subplots()\n",
    "    d = {}\n",
    "    spl_len = {}\n",
    "    spl_over = {}\n",
    "    under = []\n",
    "    over = []\n",
    "    x = np.arange(len(lst_labels))\n",
    "    b = 0\n",
    "    bar_label = True\n",
    "    dataset_names = sample['datasets'].value_counts().index.to_list()\n",
    "    for name in dataset_names:\n",
    "        spl_dataset = sample[sample['datasets'] == name]\n",
    "        spl = spl_dataset[spl_dataset['spl'] == 'sample']\n",
    "        over_spl = spl_dataset[spl_dataset['spl'] == 'oversample']\n",
    "        len_s = {\n",
    "            lbl: len(spl[spl['single_ans'] == lbl])\n",
    "            for lbl in lst_labels\n",
    "        }\n",
    "            \n",
    "        len_o = {\n",
    "            lbl: len(over_spl[over_spl['single_ans'] == lbl])\n",
    "            for lbl in lst_labels\n",
    "        }\n",
    "        spl_len.update({name: len_s})\n",
    "        spl_over.update({name: len_o})\n",
    "    df_spl_len = pd.DataFrame().from_dict(spl_len) # .sort_index()\n",
    "    df_oversample = pd.DataFrame().from_dict(spl_over) # .sort_index()\n",
    "    df_spl_len.index = change_lbl(df_spl_len.index)\n",
    "    df_oversample.index = change_lbl(df_oversample.index)\n",
    "    df_spl_len = df_spl_len.sort_index()\n",
    "    df_oversample = df_oversample.sort_index()\n",
    "    for name in dataset_names:\n",
    "        under.append((name, df_spl_len[name]))\n",
    "        over.append((name,df_oversample[name]))\n",
    "    d.update({\n",
    "        'under': under,\n",
    "        'over': over,\n",
    "    })\n",
    "    for _,v in d.items():\n",
    "        if bar_label:\n",
    "            lbl = 'sample'\n",
    "        else:\n",
    "            lbl = 'oversample'\n",
    "        p = ax.bar(x, v[0][1], width, label=f'{lbl} {v[0][0]}', bottom=b)\n",
    "        # ax.bar_label(p, label_type='center')\n",
    "        p = ax.bar(\n",
    "            x=x,\n",
    "            height=v[1][1],\n",
    "            width=width,\n",
    "            label=f'{lbl} {v[1][0]}',\n",
    "            bottom=b + v[0][1]\n",
    "        )\n",
    "        # ax.bar_label(p, label_type='center')\n",
    "        bar_label=False\n",
    "        b = b + v[0][1] + v[1][1]\n",
    "    # ax.set_yticks(np.arange(0, max(df_spl_len.max().values) + 2, step=1))\n",
    "    lst_labels = change_lbl(lst_labels)\n",
    "    ax.set_xticks(x, sorted(lst_labels), rotation=90)\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_title(title)\n",
    "    fig.set_size_inches(20, 10)\n",
    "    # plt.ylim(0, max(df_spl_len.max().values) + 1)\n",
    "    plt.show()\n",
    "    if savefile is not None:\n",
    "        fig.savefig(savefile, format='png')\n",
    "\n",
    "def plot_metric(\n",
    "    metric: dict,\n",
    "    columns: list[str]=['f1', 'precision', 'recall'],\n",
    "    title='',\n",
    "    file_plot=None,\n",
    "    file_metric=None,\n",
    "):\n",
    "    # rand_mark = pd.Series(np.full((len(metric),), 1/(len(metric)-1)))\n",
    "    df_metric = pd.DataFrame().from_dict(\n",
    "        metric,\n",
    "        orient='index',\n",
    "        columns=columns\n",
    "    )\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    df_metric.plot(\n",
    "        ax=ax,\n",
    "        kind='bar',\n",
    "        figsize=(20,14),\n",
    "        title=title,\n",
    "    )\n",
    "    # rand_mark.plot(ax=ax, color='red', linestyle='dashed')\n",
    "    # ax.set_yticks(np.arange(0, 1.1, step=0.05))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    if file_plot is not None:\n",
    "        fig.savefig(file_plot, format='png')\n",
    "    if file_metric is not None:\n",
    "        df_metric.to_csv(file_metric, header=['F1', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['f1', 'precision', 'recall']\n",
    "\n",
    "# plot_stat_sample(\n",
    "#     sample=prt_train,\n",
    "#     lst_labels=fallacies, \n",
    "#     savefile=file_stat_train,\n",
    "#     title=f'sample {spl_name} train'\n",
    "# )\n",
    "# plot_stat_sample(\n",
    "#     sample=prt_val,\n",
    "#     lst_labels=fallacies,\n",
    "#     savefile=file_stat_val,\n",
    "#     title=f'sample {spl_name} val'\n",
    "# )\n",
    "# plot_stat_sample(\n",
    "#     sample=prt_test,\n",
    "#     lst_labels=fallacies,\n",
    "#     savefile=file_stat_test,\n",
    "#     title=f'sample {spl_name} test'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(\n",
    "    metric=metric_single,\n",
    "    title=f'Scores {n_sample} sample single fallacies',\n",
    "    file_plot=file_plot_single,\n",
    "    file_metric=file_metric_single\n",
    ")\n",
    "plot_metric(\n",
    "    metric=metric_multi,\n",
    "    title=f'Scores {n_sample} sample multi fallacies',\n",
    "    file_plot=file_plot_multi,\n",
    "    file_metric=file_metric_multi\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
