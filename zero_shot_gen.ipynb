{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import unsloth_train\n",
    "\n",
    "from datasets import Dataset\n",
    "from vllm import SamplingParams\n",
    "from transformers import TrainingArguments, TrainerCallback\n",
    "from transformers import TextStreamer\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sampling.sampling_fallacies_detection import load_all_dataset, get_prt, get_spl, get_nb_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "paths = {\n",
    "    'cocolofa': './Data_jsonl/cocolofa.jsonl',\n",
    "    'mafalda': './Data_jsonl/mafalda.jsonl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name= \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=0.6\n",
    ")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = 'You are an expert in argumentation. Your task is to determine the type of fallacy in the given [SENTENCE]. The fallacy would be in the [FALLACY] Set. Utilize the [TITLE] and the [FULL TEXT] as context to support your decision.\\nYour answer must be in the following format with only the fallacy in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.'\n",
    "\n",
    "n_sample = 1000\n",
    "data, fallacies = load_all_dataset(paths)\n",
    "df_train, df_val, df_test = get_prt(data, fallacies, SYSTEM_PROMPT)\n",
    "sample_train, over_train = get_spl(df_train, fallacies, n_sample=n_sample)\n",
    "sample_val, over_val = get_spl(df_val, fallacies, n_sample=n_sample*0.2)\n",
    "sample_test, over_test = get_spl(df_test, fallacies, n_sample=n_sample*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompt(data: dict):\n",
    "    text = tokenizer.apply_chat_template(data.get('prompt'),tokenize = False, add_generation_prompt = False)\n",
    "    return { 'text': text, }\n",
    "\n",
    "SAMPLING_PARAMS = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 128,\n",
    ")\n",
    "data_train = Dataset.from_pandas(sample_train).map(\n",
    "    formatting_prompt,\n",
    "    batched=True,\n",
    ")\n",
    "data_val = Dataset.from_pandas(sample_val).map(\n",
    "    formatting_prompt,\n",
    "    batched=True,\n",
    ")\n",
    "data_test = Dataset.from_pandas(sample_test).map(\n",
    "    formatting_prompt,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen(txt, model, sampling_params):\n",
    "#     output = model.fast_generate(\n",
    "#         txt,\n",
    "#         sampling_params = sampling_params,\n",
    "#     )[0].outputs[0].text\n",
    "    \n",
    "#     return output\n",
    "\n",
    "# def format_output(answer: str, fallacies: set) -> list:\n",
    "#     s = '<[|]ANSWER[|]>'\n",
    "#     tmp = re.split(s, answer)\n",
    "#     pred= [i for i in tmp if i in fallacies]\n",
    "#     return pred\n",
    "\n",
    "# def zero_shot_gen(data: list[str], model, fallacies: set, sampling_params) -> list:\n",
    "#     res = []\n",
    "#     for i in data:\n",
    "#         out = gen(i, model, sampling_params)\n",
    "#         pred = format_output(out, fallacies)\n",
    "#         res.append(pred)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(p, model, text_streamer):\n",
    "    txt = tokenizer.apply_chat_template(\n",
    "        p,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "    output = model.generate(\n",
    "        txt,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=128,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def format_output(answer: list, labels: set) -> list:\n",
    "    s = '<[|]ANSWER[|]>'\n",
    "    tmp = re.split(s, answer[0])\n",
    "    pred = [i for i in tmp if i in labels]\n",
    "    return pred\n",
    "\n",
    "def zero_shot_gen(\n",
    "    data:Dataset,\n",
    "    model,\n",
    "    labels:set,\n",
    "    text_streamer: TextStreamer\n",
    ") -> list:\n",
    "    res= []\n",
    "    prompt = data['prompt']\n",
    "    for prt in prompt:\n",
    "        out = gen(prt, model, text_streamer)\n",
    "        decoded_out = tokenizer.batch_decode(out)\n",
    "        pred = format_output(decoded_out, labels)\n",
    "        res.append(pred)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class custom_validation_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies, n_step=10):\n",
    "#         super().__init__()\n",
    "#         self.val_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#         self.n_step=n_step\n",
    "#     def on_step_end(self, args, state, control, **kwargs):\n",
    "#         if state.global_step % self.n_step == 0 and state.global_step > 0 :\n",
    "#             model.save_lora('sft_save_lora')\n",
    "#             FastLanguageModel.for_inference(model)\n",
    "#             pred = zero_shot_gen(\n",
    "#                 data=self.val_dataset['text'],\n",
    "#                 model=model,\n",
    "#                 fallacies=self.fallacies,\n",
    "#                 sampling_params=self.sampling_params\n",
    "#             )\n",
    "#             tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#             d = pd.DataFrame().from_records(tmp_pred)\n",
    "#             d['truth_label'] = self.val_dataset['answer']\n",
    "#             d['step'] = np.full((len(d['truth_label']),), state.global_step)\n",
    "#             try:\n",
    "#                 d.to_csv(\n",
    "#                     './validation_res.csv',\n",
    "#                     index=False,\n",
    "#                     mode='a',\n",
    "#                     header=['pred', 'truth_label', 'step']\n",
    "#                 )\n",
    "#             except FileNotFoundError:\n",
    "#                 d.to_csv('./validation_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_step_end(args, state, control, **kwargs)\n",
    "\n",
    "# class custom_test_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies):\n",
    "#         super().__init__()\n",
    "#         self.test_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#     def on_train_end(self, args, state, control, **kwargs):\n",
    "#         model.save_lora('sft_save_lora')\n",
    "#         FastLanguageModel.for_inference(model)\n",
    "#         pred = zero_shot_gen(\n",
    "#             data=self.test_dataset['text'],\n",
    "#             model=model,\n",
    "#             fallacies=self.fallacies,\n",
    "#             sampling_params=self.sampling_params\n",
    "#         )\n",
    "#         tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#         d = pd.DataFrame().from_records(tmp_pred)\n",
    "#         d['truth_label'] = self.test_dataset['answer']\n",
    "#         try:\n",
    "#             d.to_csv('./test_res.csv', index=False, mode='a', header=['pred','truth_label'])\n",
    "#         except FileNotFoundError:\n",
    "#             d.to_csv('./test_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_train_end(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dir = './outputs/1e1000spl'\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 4, #2\n",
    "    per_device_eval_batch_size= 4,\n",
    "    gradient_accumulation_steps = 8, #4\n",
    "    eval_accumulation_steps= 8,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 1.5, # Set this for 1 full training run.\n",
    "    # max_steps = 60,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = outputs_dir,\n",
    "    report_to = \"tensorboard\", # Use this for WandB etc\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=4,\n",
    ")   \n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = data_train,\n",
    "    eval_dataset=data_val,\n",
    "    # formatting_func=formatting_prompt,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args,\n",
    ")\n",
    "# trainer.get_train_dataloader().shuffle = False\n",
    "# trainer.get_eval_dataloader().shuffle = False\n",
    "# trainer.train()\n",
    "unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(data):\n",
    "    try:\n",
    "        tmp = data.apply(\n",
    "            lambda x: x['pred'] in x['lbl'],\n",
    "            axis=1\n",
    "        )\n",
    "        tp = tmp.value_counts().loc[True]\n",
    "        score = tp / len(tmp)\n",
    "        fp_fn = len(tmp) - tp\n",
    "        return score, tp, fp_fn\n",
    "    except (KeyError, ZeroDivisionError):\n",
    "        tp = 0\n",
    "        score = 0\n",
    "        fp_fn = len(tmp) - tp\n",
    "        return score, tp, fp_fn\n",
    "\n",
    "def get_f1(precision, recall):\n",
    "    try:\n",
    "        f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "    return f1\n",
    "\n",
    "def get_metrics(data, labels):\n",
    "    tp_preci = 0\n",
    "    tp_rec = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    res = {}\n",
    "    for l in labels:\n",
    "        df_on_pred = data[data['pred'] == l]\n",
    "        df_on_label = data.apply(\n",
    "            lambda x: x if l in x['lbl'] else np.nan,\n",
    "            result_type='broadcast',\n",
    "            axis = 1\n",
    "        ).dropna()\n",
    "        precision, tp_p, fp_p = get_precision_recall(df_on_pred)\n",
    "        recall, tp_r, fn_r = get_precision_recall(df_on_label)\n",
    "        f1 = get_f1(precision, recall)\n",
    "        res.update({l: (f1, precision, recall)})\n",
    "        fn += fn_r\n",
    "        fp += fp_p\n",
    "        tp_preci += tp_p\n",
    "        tp_rec += tp_r\n",
    "    precision_all_data = tp_preci / (tp_preci + fp)\n",
    "    recall_all_data = tp_rec / (tp_rec + fn)\n",
    "    f1_all_data = get_f1(precision_all_data, recall_all_data)\n",
    "    res.update({\n",
    "        'score_all_data': (f1_all_data, precision_all_data, recall_all_data)\n",
    "    })\n",
    "    return res\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "pred = zero_shot_gen(\n",
    "    data=data_test[:10],\n",
    "    model=model,\n",
    "    labels=fallacies,\n",
    "    text_streamer=text_streamer\n",
    ")\n",
    "\n",
    "names_dataset = data_test[:10]['datasets']\n",
    "true_labels = data_test[:10]['answer']\n",
    "\n",
    "tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "pred_flat = list(itertools.chain.from_iterable(tmp_pred))\n",
    "\n",
    "d_res = {'names': names_dataset, 'pred': pred_flat, 'lbl': true_labels}\n",
    "df_res = pd.DataFrame(data=d_res)\n",
    "metric = get_metrics(df_res, fallacies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stat_sample(\n",
    "    sample: pd.DataFrame,\n",
    "    over: dict,\n",
    "    lst_labels: set[str],\n",
    "    n_sample: int,\n",
    "    savefile=None\n",
    ") -> None:\n",
    "    width = 0.3\n",
    "    nb_per_lbl = {}\n",
    "    fig, ax = plt.subplots()\n",
    "    d = {}\n",
    "    x = np.arange(len(lst_labels))\n",
    "    mult = 0\n",
    "    dataset_names = sample['datasets'].value_counts().index.to_list()\n",
    "    for name in dataset_names:\n",
    "        data = sample[sample['datasets'] == name]['answer'] \n",
    "        nb_per_lbl.update({\n",
    "            name: get_nb_element(data.to_list())\n",
    "        })\n",
    "    df_nb_lbl = pd.DataFrame().from_dict(nb_per_lbl).fillna(0).sort_index()\n",
    "    df_over = pd.DataFrame().from_dict(over).fillna(0).sort_index()\n",
    "    print(nb_per_lbl)\n",
    "    print(df_over)\n",
    "    for name in dataset_names:\n",
    "        nb_element = n_sample / len(lst_labels)\n",
    "        df_nb_lbl.index.names = [None]\n",
    "        tmp = [\n",
    "            (x, nb_element) \n",
    "            if y > 15 else (x, y)\n",
    "            for x, y in zip(df_over[name].values, df_nb_lbl[name].values)\n",
    "        ]\n",
    "        nb_lbl = [\n",
    "            nb_element - x[0] \n",
    "            if x[0] > 0 else x[1]\n",
    "            for x in tmp\n",
    "        ]\n",
    "        nb_lbl = pd.Series(data=nb_lbl, index=df_over[name].index)\n",
    "        # nb_lbl_list = df_nb_lbl[name].values - df_over[name].values - nb_element\n",
    "        nb_lbl_list = df_nb_lbl[name].values - df_over[name].values - nb_lbl.values\n",
    "        # nb_lbl_list[nb_lbl_list < 0] = 0\n",
    "        sr_lbl_list = pd.Series(data=nb_lbl_list, index=df_over[name].index)\n",
    "        d.update({\n",
    "            name: (\n",
    "                nb_lbl,\n",
    "                df_over[name],\n",
    "                sr_lbl_list\n",
    "            )\n",
    "        })\n",
    "    for k,v in d.items():\n",
    "        p = ax.bar(x + width * mult, v[0], width, label=k)\n",
    "        ax.bar_label(p, label_type='center')\n",
    "        p = ax.bar(\n",
    "            x + width * mult, v[1], width, label=f'oversample {k}', bottom=v[0]\n",
    "        )\n",
    "        ax.bar_label(p, label_type='center')\n",
    "        p = ax.bar(\n",
    "            x + width * mult,\n",
    "            v[2],\n",
    "            width,\n",
    "            label=f'labels in list {k}',\n",
    "            bottom=v[1]+v[0]\n",
    "        )\n",
    "        ax.bar_label(p, label_type='center')\n",
    "        mult += 1\n",
    "    ax.set_xticks(x + width, sorted(lst_labels), rotation=90)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title('Sampled data')\n",
    "    fig.set_size_inches(20, 10)\n",
    "    plt.show()\n",
    "    if savefile is not None:\n",
    "        plt.savefig(savefile, format='png')\n",
    "\n",
    "def plot_metric(\n",
    "    metric: dict,\n",
    "    columns: list[str]=['f1', 'precision', 'recall'],\n",
    "    title='',\n",
    "    savefile=None,\n",
    "):\n",
    "    # rand_mark = pd.Series(np.full((len(metric),), 1/(len(metric)-1)))\n",
    "    df_metric = pd.DataFrame().from_dict(\n",
    "        metric,\n",
    "        orient='index',\n",
    "        columns=columns\n",
    "    )\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    df_metric.plot(\n",
    "        ax=ax,\n",
    "        kind='bar',\n",
    "        figsize=(20,10),\n",
    "        title=title,\n",
    "    )\n",
    "    # rand_mark.plot(ax=ax, color='red', linestyle='dashed')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    if savefile is not None:\n",
    "        plt.savefig(savefile, format='png')\n",
    "    \n",
    "columns = ['f1', 'precision', 'recall']\n",
    "plot_stat_sample(sample_train, over_train, fallacies, n_sample)\n",
    "plot_metric(metric, title='Scores on all sampled data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
