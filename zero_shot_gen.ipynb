{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import unsloth_train\n",
    "from unsloth.chat_templates import train_on_responses_only, get_chat_template \n",
    "from unsloth import apply_chat_template\n",
    "\n",
    "from datasets import Dataset\n",
    "from vllm import SamplingParams\n",
    "from transformers import TrainingArguments, TrainerCallback\n",
    "from transformers import TextStreamer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from src.utils import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training\n",
    "model, tokenizer = run('evidence_type', do_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_savefile\n",
    "from src.prompting import get_datasets\n",
    "from src.metrics import get_metrics\n",
    "from src.plot import plot_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.aduc import get_data\n",
    "# from src.aduc import change_lbl\n",
    "\n",
    "# from src.claim_detect import get_data\n",
    "# from src.claim_detect import change_lbl\n",
    "\n",
    "# from src.evidence_detect import get_data\n",
    "# from src.evidence_detect import change_lbl\n",
    "\n",
    "# from src.evidence_type import get_data\n",
    "# from src.evidence_type import change_lbl\n",
    "\n",
    "# from src.fallacies import get_data\n",
    "# from src.fallacies import change_lbl\n",
    "\n",
    "# from src.quality import get_data\n",
    "# from src.quality import change_lbl\n",
    "\n",
    "# from src.relation import get_data\n",
    "# from src.relation import change_lbl\n",
    "\n",
    "# from src.stance_detect import get_data\n",
    "# from src.stance_detect import change_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'aduc': f'./outputs/aduc/2025-05-07_17:29:07_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'claim_detection': f'./outputs/claim_detection/2025-05-09_14:57:11_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'evidence_detection': f'./outputs/evidence_detection/2025-05-09_15:51:24_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'evidence_type': f'./outputs/evidence_type/2025-05-12_10:47:06_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'fallacies': f'./outputs/fallacies/2025-05-09_16:41:44_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-340',\n",
    "    'relation': f'./outputs/relation/2025-05-09_18:41:38_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'stance_detection': f'./outputs/stance_detection/2025-05-12_08:59:05_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'quality': f'./outputs/quality/2025-05-12_09:44:16_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Parameters\\n\",\n",
    "task = 'evidence_type'\n",
    "task_title = 'Evidence Type'\n",
    "ckpt_names = checkpoint.get(task)\n",
    "# gguf_file = f'./gguf_model/{task}'\n",
    "max_seq_lenght = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "gpu_mem_use = 0.6\n",
    "system_prompt = \"\"\n",
    "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{OUTPUT}<|eot_id|>\"\"\"\n",
    "time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "s_file = get_savefile(\n",
    "    task_name=task,\n",
    "    spl_name='spl2',\n",
    "    m_name='meta-Llama-3.1-8B-Instruct',\n",
    "    n_sample=4000,\n",
    "    epoch=2,\n",
    "    train_resp=f'_train_resp',\n",
    "    outputs_dir=f'./outputs/test_{task}',\n",
    "    time=time\n",
    ")\n",
    "labels, tr_d, val_d, test_d = get_data(s_file)\n",
    "print(ckpt_names)\n",
    "print(gguf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Load Model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=ckpt_names,\n",
    "    max_seq_length=max_seq_lenght,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=gpu_mem_use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Run On test_set\n",
    "from src.training import test\n",
    "train_set, val_set, test_set = get_datasets(\n",
    "    tokenizer=tokenizer,\n",
    "    train=tr_d,\n",
    "    val=val_d,\n",
    "    test=test_d, \n",
    "    chat_template=chat_template,\n",
    "    sys_prt=system_prompt\n",
    ")\n",
    "res_test_chpt = test(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_test=test_set,\n",
    "    labels=labels,\n",
    ")\n",
    "# metric, _ = get_metrics(change_lbl, res_test_chpt, is_multi_lbl=False)\n",
    "# plot_metric(\n",
    "#     metric=metric,\n",
    "#     title=f'{task_title}: Scores ckpt{nbckpt}',\n",
    "#     file_plot=f'./img/{task}/scores_ckpt{nbckpt}_metric_single.png',\n",
    "#     file_metric=f'./test_res/{task}/scores_ckpt{nbckpt}_metric_single.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Method\n",
    "model.save_pretrained_gguf(\n",
    "    gguf_file,\n",
    "    tokenizer,\n",
    "    quantization_method = \"q8_0\", # choose quant method: q8_0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "res = []\n",
    "s = '<[|]ANSWER[|]>'\n",
    "quant = 'q8_0'\n",
    "names_dataset = test_d['datasets']\n",
    "true_labels = test_d['answer']\n",
    "# print(len(test_d['conversations']))\n",
    "for i in test_d['conversations']:\n",
    "    response = chat(model='unsloth_model', messages=[i[1]])\n",
    "    print(response['message']['content'])\n",
    "    tmp = re.split(s, response['message']['content'])\n",
    "    res.append(tmp[1])\n",
    "d_res = {'names': names_dataset, 'pred': res, 'lbl': true_labels}\n",
    "df_res = pd.DataFrame(data=d_res)\n",
    "df_res.to_csv(f'./test_res/{task}/test_result_gguf_{quant}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metric_single, metric_multi = get_metrics(change_lbl, df_res)\n",
    "metric_single, _ = get_metrics(change_lbl, df_res, is_multi_lbl=False)\n",
    "plot_metric(\n",
    "    metric=metric_single,\n",
    "    title=f'{task_title}: Score gguf_{quant} single label',\n",
    "    file_plot=f'./img/{task}/scores_gguf_{quant}_metric_single.png',\n",
    "    file_metric=f'./test_res/{task}/scores_gguf_{quant}_metric_single.csv'\n",
    ")\n",
    "# For Fallacies Task Only\n",
    "# plot_metric(\n",
    "#     metric=metric_multi,\n",
    "#     title=f'{task_title}: Score gguf_{quant} multi label',\n",
    "#     file_plot=f'./img/{task}/scores_gguf_{quant}_metric_multi.png',\n",
    "#     file_metric=f'./test_res/{task}/scores_gguf_{quant}_metric_multi.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "# paths = {\n",
    "#     'cocolofa': './Data_jsonl/cocolofa.jsonl',\n",
    "#     'mafalda': './Data_jsonl/mafalda.jsonl'\n",
    "# }\n",
    "# SYSTEM_PROMPT = 'You are an expert in argumentation. Your task is to determine the type of fallacy in the given [SENTENCE]. The fallacy would be in the [FALLACY] Set. Utilize the [TITLE] and the [FULL TEXT] as context to support your decision.\\nYour answer must be in the following format with only the fallacy in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.'\n",
    "# n_sample = 4000\n",
    "# e = 1.5\n",
    "# n_eval = 8\n",
    "# n_eval_step = np.floor((n_sample / 32) / n_eval)\n",
    "\n",
    "# model_name = 'Llama3.18BInstruct'\n",
    "# spl_name = 'spl2'\n",
    "# task_name = 'fallacies'\n",
    "# train_resp = '_train_resp'\n",
    "\n",
    "# train_spl_file = f'./sampling/sample/{task_name}/{spl_name}_train.csv'\n",
    "# val_spl_file = f'./sampling/sample/{task_name}/{spl_name}_val.csv'\n",
    "# test_spl_file = f'./sampling/sample/{task_name}/{spl_name}_test.csv'\n",
    "\n",
    "# outputs_dir = f'./outputs/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}'\n",
    "\n",
    "# test_result_file = f'./test_res/{task_name}/test_res_{model_name}_{e}e{n_sample}{spl_name}{train_resp}.csv'\n",
    "# result_file = f'./test_res/{task_name}/test_res_{model_name}_{e}e{n_sample}{spl_name}{train_resp}.csv'\n",
    "\n",
    "# file_stat_train = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}_stat_train.png'\n",
    "# file_stat_val = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}_stat_val.png'\n",
    "# file_stat_test = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}_stat_test.png'\n",
    "\n",
    "# file_plot_single = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_res_single.png'\n",
    "# file_plot_multi = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_res_multi.png'\n",
    "\n",
    "# file_metric_single = f'./test_res/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_metric_single.csv'\n",
    "# file_metric_multi = f'./test_res/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_metric_multi.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name= \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=0.6\n",
    ")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompt(tokenizer, d):\n",
    "    t = d['prompt']\n",
    "    texts = [tokenizer.apply_chat_template(txt, tokenize=False, add_generation_prompt=False) for txt in t]\n",
    "    return { 'text': texts, }\n",
    "time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{OUTPUT}<|eot_id|>\"\"\"\n",
    "savefile = get_savefile(\n",
    "    task_name='fallacies',\n",
    "    spl_name='spl2',\n",
    "    m_name='meta-Llama-3.1-8B-Instruct',\n",
    "    n_sample=4000,\n",
    "    epoch=2,\n",
    "    train_resp='_train_resp',\n",
    "    outputs_dir=f'./outputs/test_fallacies',\n",
    "    time=time\n",
    ")\n",
    "fallacies, prt_train, prt_val, prt_test = get_data(savefile)\n",
    "data_train, data_val, data_test = get_datasets(tokenizer, prt_train, prt_val, prt_test)\n",
    "prt_train = prt_train.rename(columns={\"prompt\": \"conversations\"})\n",
    "train = apply_chat_template(\n",
    "    Dataset.from_pandas(prt_train),\n",
    "    tokenizer,\n",
    "    chat_template\n",
    ")\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template='llama-3.1'\n",
    ")\n",
    "d_train = Dataset.from_pandas(prt_train).map(\n",
    "    lambda x: formatting_prompt(tokenizer, x),\n",
    "    batched=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, fallacies = load_all_dataset(paths)\n",
    "# spl_data = get_all_spl(data, fallacies, n_sample)\n",
    "# prt_train, prt_val, prt_test = get_prt(spl_data, fallacies, SYSTEM_PROMPT)\n",
    "\n",
    "# prt_train.to_csv(train_spl_file, index=False)\n",
    "# prt_val.to_csv(val_spl_file, index=False)\n",
    "# prt_test.to_csv(test_spl_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = {'prompt': literal_eval, 'answer': literal_eval}\n",
    "prt_train = pd.read_csv(\n",
    "    train_spl_file,\n",
    "    converters=converter\n",
    ")\n",
    "prt_val = pd.read_csv(\n",
    "    val_spl_file,\n",
    "    converters=converter\n",
    ")\n",
    "prt_test = pd.read_csv(\n",
    "    test_spl_file,\n",
    "    converters=converter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompt(data: dict):\n",
    "    text = tokenizer.apply_chat_template(data.get('prompt'),tokenize = False, add_generation_prompt = False)\n",
    "    return { 'text': text, }\n",
    "\n",
    "SAMPLING_PARAMS = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 128,\n",
    ")\n",
    "data_train = Dataset.from_pandas(prt_train).map(\n",
    "    formatting_prompt,\n",
    "    batched=True,\n",
    ")\n",
    "data_val = Dataset.from_pandas(prt_val).map(\n",
    "    formatting_prompt,\n",
    "    batched=True,\n",
    ")\n",
    "data_test = Dataset.from_pandas(prt_test).map(\n",
    "    formatting_prompt,\n",
    "    batched=True\n",
    ").shuffle(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen(txt, model, sampling_params):\n",
    "#     output = model.fast_generate(\n",
    "#         txt,\n",
    "#         sampling_params = sampling_params,\n",
    "#     )[0].outputs[0].text\n",
    "    \n",
    "#     return output\n",
    "\n",
    "# def format_output(answer: str, fallacies: set) -> list:\n",
    "#     s = '<[|]ANSWER[|]>'\n",
    "#     tmp = re.split(s, answer)\n",
    "#     pred= [i for i in tmp if i in fallacies]\n",
    "#     return pred\n",
    "\n",
    "# def zero_shot_gen(data: list[str], model, fallacies: set, sampling_params) -> list:\n",
    "#     res = []\n",
    "#     for i in data:\n",
    "#         out = gen(i, model, sampling_params)\n",
    "#         pred = format_output(out, fallacies)\n",
    "#         res.append(pred)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(p, model, text_streamer):\n",
    "    txt = tokenizer.apply_chat_template(\n",
    "        p,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "    output = model.generate(\n",
    "        txt,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=128,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def format_output(answer: list, labels: set) -> list:\n",
    "    s = '<[|]ANSWER[|]>'\n",
    "    tmp = re.split(s, answer[0])\n",
    "    pred = [i for i in tmp if i in labels]\n",
    "    return pred\n",
    "\n",
    "def zero_shot_gen(\n",
    "    data:Dataset,\n",
    "    model,\n",
    "    labels:set,\n",
    "    text_streamer: TextStreamer\n",
    ") -> list:\n",
    "    res= []\n",
    "    prompt = data['prompt']\n",
    "    for prt in prompt:\n",
    "        out = gen(prt, model, text_streamer)\n",
    "        decoded_out = tokenizer.batch_decode(out)\n",
    "        pred = format_output(decoded_out, labels)\n",
    "        res.append(pred)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class custom_validation_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies, n_step=10):\n",
    "#         super().__init__()\n",
    "#         self.val_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#         self.n_step=n_step\n",
    "#     def on_step_end(self, args, state, control, **kwargs):\n",
    "#         if state.global_step % self.n_step == 0 and state.global_step > 0 :\n",
    "#             model.save_lora('sft_save_lora')\n",
    "#             FastLanguageModel.for_inference(model)\n",
    "#             pred = zero_shot_gen(\n",
    "#                 data=self.val_dataset['text'],\n",
    "#                 model=model,\n",
    "#                 fallacies=self.fallacies,\n",
    "#                 sampling_params=self.sampling_params\n",
    "#             )\n",
    "#             tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#             d = pd.DataFrame().from_records(tmp_pred)\n",
    "#             d['truth_label'] = self.val_dataset['answer']\n",
    "#             d['step'] = np.full((len(d['truth_label']),), state.global_step)\n",
    "#             try:\n",
    "#                 d.to_csv(\n",
    "#                     './validation_res.csv',\n",
    "#                     index=False,\n",
    "#                     mode='a',\n",
    "#                     header=['pred', 'truth_label', 'step']\n",
    "#                 )\n",
    "#             except FileNotFoundError:\n",
    "#                 d.to_csv('./validation_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_step_end(args, state, control, **kwargs)\n",
    "\n",
    "# class custom_test_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies):\n",
    "#         super().__init__()\n",
    "#         self.test_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#     def on_train_end(self, args, state, control, **kwargs):\n",
    "#         model.save_lora('sft_save_lora')\n",
    "#         FastLanguageModel.for_inference(model)\n",
    "#         pred = zero_shot_gen(\n",
    "#             data=self.test_dataset['text'],\n",
    "#             model=model,\n",
    "#             fallacies=self.fallacies,\n",
    "#             sampling_params=self.sampling_params\n",
    "#         )\n",
    "#         tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#         d = pd.DataFrame().from_records(tmp_pred)\n",
    "#         d['truth_label'] = self.test_dataset['answer']\n",
    "#         try:\n",
    "#             d.to_csv('./test_res.csv', index=False, mode='a', header=['pred','truth_label'])\n",
    "#         except FileNotFoundError:\n",
    "#             d.to_csv('./test_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_train_end(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 4, #2\n",
    "    per_device_eval_batch_size= 4,\n",
    "    gradient_accumulation_steps = 8, #4\n",
    "    eval_accumulation_steps= 8,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = e, # Set this for 1 full training run.\n",
    "    # max_steps = 60,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = outputs_dir,\n",
    "    report_to = \"tensorboard\", # Use this for WandB etc\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=n_eval_step,\n",
    ")   \n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = data_train,\n",
    "    eval_dataset=data_val,\n",
    "    # formatting_func=formatting_prompt,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args,\n",
    ")\n",
    "# trainer.get_train_dataloader().shuffle = False\n",
    "# trainer.get_eval_dataloader().shuffle = False\n",
    "# trainer.train()\n",
    "if train_resp == '_train_resp':\n",
    "    trainer = train_on_responses_only(\n",
    "        trainer,\n",
    "        instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "        response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(data):\n",
    "    try:\n",
    "        tmp = data.apply(\n",
    "            lambda x: x['pred'] in x['lbl'],\n",
    "            axis=1\n",
    "        )\n",
    "        tp = tmp.value_counts().loc[True]\n",
    "        score = tp / len(tmp)\n",
    "        fp_fn = len(tmp) - tp\n",
    "        return score, tp, fp_fn\n",
    "    except (KeyError, ZeroDivisionError):\n",
    "        tp = 0\n",
    "        score = 0\n",
    "        fp_fn = len(tmp) - tp\n",
    "        return score, tp, fp_fn\n",
    "\n",
    "def get_recall_multi(data, label):\n",
    "    n_lbl = len(label)\n",
    "    ratio = []\n",
    "    for l in label:\n",
    "        c = data['pred'].value_counts()\n",
    "        try:\n",
    "            ratio.append(min((1/n_lbl) * (c[l] / (len(data)/n_lbl)), 1/n_lbl))\n",
    "        except:\n",
    "            ratio.append(min((1/n_lbl) * (0 / (len(data)/n_lbl)), 1/n_lbl))\n",
    "    rec = sum(ratio)\n",
    "    tp = np.round(len(data)*rec)\n",
    "    fn = len(data) - tp\n",
    "    return rec, tp, fn\n",
    "\n",
    "def get_f1(precision, recall):\n",
    "    try:\n",
    "        f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "    return f1\n",
    "\n",
    "def get_metrics_single(data, labels):\n",
    "    tp_preci = 0\n",
    "    tp_rec = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    res = {}\n",
    "    for l in labels:\n",
    "        df_on_pred = data[data['pred'] == l[0]]\n",
    "        df_on_label = data.apply(\n",
    "            lambda x: x if l[0] in x['lbl'] else np.nan,\n",
    "            result_type='broadcast',\n",
    "            axis = 1\n",
    "        ).dropna()\n",
    "        precision, tp_p, fp_p = get_precision_recall(df_on_pred)\n",
    "        recall, tp_r, fn_r = get_precision_recall(df_on_label)\n",
    "        f1 = get_f1(precision, recall)\n",
    "        n_lbl = change_lbl(l)\n",
    "        res.update({str(n_lbl): (f1, precision, recall)})\n",
    "        fn += fn_r\n",
    "        fp += fp_p\n",
    "        tp_preci += tp_p\n",
    "        tp_rec += tp_r\n",
    "    precision_all_data = tp_preci / (tp_preci + fp)\n",
    "    recall_all_data = tp_rec / (tp_rec + fn)\n",
    "    f1_all_data = get_f1(precision_all_data, recall_all_data)\n",
    "    res.update({\n",
    "        'score_all_data': (f1_all_data, precision_all_data, recall_all_data)\n",
    "    })\n",
    "    return res\n",
    "\n",
    "def get_metrics_multi(data, labels):\n",
    "    tp_preci = 0\n",
    "    tp_rec = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    res = {}\n",
    "    for l in labels:\n",
    "        df_pred_lbl = data.apply(\n",
    "            lambda x: x if x['lbl'] == l else np.nan,\n",
    "            result_type='broadcast',\n",
    "            axis=1\n",
    "        ).dropna()\n",
    "        precision, tp_p, fp_p = get_precision_recall(df_pred_lbl)\n",
    "        recall, tp_r, fn_r = get_recall_multi(df_pred_lbl, l)\n",
    "        f1 = get_f1(precision, recall)\n",
    "        n_lbl = change_lbl(l)\n",
    "        res.update({str(n_lbl): (f1, precision, recall)})\n",
    "        fn += fn_r\n",
    "        fp += fp_p\n",
    "        tp_preci += tp_p\n",
    "        tp_rec += tp_r\n",
    "    precision_all_data = tp_preci / (tp_preci + fp)\n",
    "    recall_all_data = tp_rec / (tp_rec + fn)\n",
    "    f1_all_data = get_f1(precision_all_data, recall_all_data)\n",
    "    res.update({\n",
    "        'score_all_data': (f1_all_data, precision_all_data, recall_all_data)\n",
    "    })\n",
    "    return res\n",
    "\n",
    "def change_lbl(labels: list) -> list:\n",
    "    new_lbl = []\n",
    "    replace_lbl = {\n",
    "        'appeal to nature': 'AN',\n",
    "        'straw man': 'STM',\n",
    "        'false dilemma': 'FD',\n",
    "        'appeal to tradition': 'AT',\n",
    "        'causal oversimplification': 'COS',\n",
    "        'appeal to majority': 'AM',\n",
    "        'ad hominem': 'AH',\n",
    "        'appeal to ridicule': 'AR',\n",
    "        'circular reasoning': 'CR',\n",
    "        'false analogy': 'FA',\n",
    "        'false causality': 'FC',\n",
    "        'appeal to fear': 'AF',\n",
    "        'appeal to worse problems': 'AWP',\n",
    "        'none': 'NONE',\n",
    "        'guilt by association': 'GA',\n",
    "        'equivocation': 'EQ',\n",
    "        'appeal to authority': 'AA',\n",
    "        'hasty generalization': 'HG',\n",
    "        'slippery slope': 'SS',\n",
    "        'ad populum': 'AP'\n",
    "    }\n",
    "    for l in labels:\n",
    "        new_lbl.append(replace_lbl.get(l))\n",
    "    return new_lbl\n",
    "    \n",
    "\n",
    "def get_metrics(data_single: pd.DataFrame, data_multi: pd.DataFrame):\n",
    "    labels_single = data_single['lbl'].value_counts().index\n",
    "    labels_multi = data_multi['lbl'].value_counts().index\n",
    "    scores_single = get_metrics_single(data_single, labels_single)\n",
    "    scores_multi = get_metrics_multi(data_multi, labels_multi)\n",
    "    return scores_single, scores_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "pred = zero_shot_gen(\n",
    "    data=data_test,\n",
    "    model=model,\n",
    "    labels=fallacies,\n",
    "    text_streamer=text_streamer\n",
    ")\n",
    "\n",
    "names_dataset = data_test['datasets']\n",
    "true_labels = data_test['answer']\n",
    "\n",
    "tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "pred_flat = list(itertools.chain.from_iterable(tmp_pred))\n",
    "\n",
    "d_res = {'names': names_dataset, 'pred': pred_flat, 'lbl': true_labels}\n",
    "df_res = pd.DataFrame(data=d_res)\n",
    "df_res.to_csv(test_result_file, index=False)\n",
    "# df_res = pd.read_csv(\n",
    "#     result_file,\n",
    "#     converters={'lbl': literal_eval}\n",
    "# )\n",
    "#metric = get_metrics(df_res, fallacies)\n",
    "df_single = df_res.apply(\n",
    "    lambda x: x if len(x['lbl']) <= 1 else np.nan,\n",
    "    axis=1,\n",
    "    result_type='broadcast'\n",
    ").dropna()\n",
    "df_multi = df_res.apply(\n",
    "    lambda x: x if len(x['lbl']) > 1 else np.nan,\n",
    "    axis=1,\n",
    "    result_type='broadcast',\n",
    ").dropna()\n",
    "metric_single, metric_multi = get_metrics(df_single, df_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stat_sample(\n",
    "    sample: pd.DataFrame,\n",
    "    lst_labels: set,\n",
    "    title: str,\n",
    "    savefile=None,\n",
    ") -> None:\n",
    "    width = 0.3\n",
    "    fig, ax = plt.subplots()\n",
    "    d = {}\n",
    "    spl_len = {}\n",
    "    spl_over = {}\n",
    "    under = []\n",
    "    over = []\n",
    "    x = np.arange(len(lst_labels))\n",
    "    b = 0\n",
    "    bar_label = True\n",
    "    dataset_names = sample['datasets'].value_counts().index.to_list()\n",
    "    for name in dataset_names:\n",
    "        spl_dataset = sample[sample['datasets'] == name]\n",
    "        spl = spl_dataset[spl_dataset['spl'] == 'sample']\n",
    "        over_spl = spl_dataset[spl_dataset['spl'] == 'oversample']\n",
    "        len_s = {\n",
    "            lbl: len(spl[spl['single_ans'] == lbl])\n",
    "            for lbl in lst_labels\n",
    "        }\n",
    "            \n",
    "        len_o = {\n",
    "            lbl: len(over_spl[over_spl['single_ans'] == lbl])\n",
    "            for lbl in lst_labels\n",
    "        }\n",
    "        spl_len.update({name: len_s})\n",
    "        spl_over.update({name: len_o})\n",
    "    df_spl_len = pd.DataFrame().from_dict(spl_len) # .sort_index()\n",
    "    df_oversample = pd.DataFrame().from_dict(spl_over) # .sort_index()\n",
    "    df_spl_len.index = change_lbl(df_spl_len.index)\n",
    "    df_oversample.index = change_lbl(df_oversample.index)\n",
    "    df_spl_len = df_spl_len.sort_index()\n",
    "    df_oversample = df_oversample.sort_index()\n",
    "    for name in dataset_names:\n",
    "        under.append((name, df_spl_len[name]))\n",
    "        over.append((name,df_oversample[name]))\n",
    "    d.update({\n",
    "        'under': under,\n",
    "        'over': over,\n",
    "    })\n",
    "    for _,v in d.items():\n",
    "        if bar_label:\n",
    "            lbl = 'sample'\n",
    "        else:\n",
    "            lbl = 'oversample'\n",
    "        p = ax.bar(x, v[0][1], width, label=f'{lbl} {v[0][0]}', bottom=b)\n",
    "        # ax.bar_label(p, label_type='center')\n",
    "        p = ax.bar(\n",
    "            x=x,\n",
    "            height=v[1][1],\n",
    "            width=width,\n",
    "            label=f'{lbl} {v[1][0]}',\n",
    "            bottom=b + v[0][1]\n",
    "        )\n",
    "        # ax.bar_label(p, label_type='center')\n",
    "        bar_label=False\n",
    "        b = b + v[0][1] + v[1][1]\n",
    "    # ax.set_yticks(np.arange(0, max(df_spl_len.max().values) + 2, step=1))\n",
    "    lst_labels = change_lbl(lst_labels)\n",
    "    ax.set_xticks(x, sorted(lst_labels), rotation=90)\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_title(title)\n",
    "    fig.set_size_inches(20, 10)\n",
    "    # plt.ylim(0, max(df_spl_len.max().values) + 1)\n",
    "    plt.show()\n",
    "    if savefile is not None:\n",
    "        fig.savefig(savefile, format='png')\n",
    "\n",
    "def plot_metric(\n",
    "    metric: dict,\n",
    "    columns: list[str]=['f1', 'precision', 'recall'],\n",
    "    title='',\n",
    "    file_plot=None,\n",
    "    file_metric=None,\n",
    "):\n",
    "    # rand_mark = pd.Series(np.full((len(metric),), 1/(len(metric)-1)))\n",
    "    df_metric = pd.DataFrame().from_dict(\n",
    "        metric,\n",
    "        orient='index',\n",
    "        columns=columns\n",
    "    )\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    df_metric.plot(\n",
    "        ax=ax,\n",
    "        kind='bar',\n",
    "        figsize=(20,14),\n",
    "        title=title,\n",
    "    )\n",
    "    # rand_mark.plot(ax=ax, color='red', linestyle='dashed')\n",
    "    # ax.set_yticks(np.arange(0, 1.1, step=0.05))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    if file_plot is not None:\n",
    "        fig.savefig(file_plot, format='png')\n",
    "    if file_metric is not None:\n",
    "        df_metric.to_csv(file_metric, header=['F1', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['f1', 'precision', 'recall']\n",
    "\n",
    "# plot_stat_sample(\n",
    "#     sample=prt_train,\n",
    "#     lst_labels=fallacies, \n",
    "#     savefile=file_stat_train,\n",
    "#     title=f'sample {spl_name} train'\n",
    "# )\n",
    "# plot_stat_sample(\n",
    "#     sample=prt_val,\n",
    "#     lst_labels=fallacies,\n",
    "#     savefile=file_stat_val,\n",
    "#     title=f'sample {spl_name} val'\n",
    "# )\n",
    "# plot_stat_sample(\n",
    "#     sample=prt_test,\n",
    "#     lst_labels=fallacies,\n",
    "#     savefile=file_stat_test,\n",
    "#     title=f'sample {spl_name} test'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(\n",
    "    metric=metric_single,\n",
    "    title=f'Scores {n_sample} sample single fallacies',\n",
    "    file_plot=file_plot_single,\n",
    "    file_metric=file_metric_single\n",
    ")\n",
    "plot_metric(\n",
    "    metric=metric_multi,\n",
    "    title=f'Scores {n_sample} sample multi fallacies',\n",
    "    file_plot=file_plot_multi,\n",
    "    file_metric=file_metric_multi\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
