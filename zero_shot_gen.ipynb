{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import unsloth_train\n",
    "from unsloth.chat_templates import train_on_responses_only, get_chat_template, CHAT_TEMPLATES\n",
    "from unsloth import apply_chat_template\n",
    "\n",
    "from datasets import Dataset\n",
    "from vllm import SamplingParams\n",
    "from transformers import TrainingArguments, TrainerCallback\n",
    "from transformers import TextStreamer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from src.utils import run\n",
    "from src.utils import get_savefile\n",
    "from src.fallacies import get_data\n",
    "from src.prompting import get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST THING\n",
    "model, tokenizer = run('aduc', do_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_gguf(\n",
    "#     './gguf_model/fallacies',\n",
    "#     tokenizer,\n",
    "#     quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "# paths = {\n",
    "#     'cocolofa': './Data_jsonl/cocolofa.jsonl',\n",
    "#     'mafalda': './Data_jsonl/mafalda.jsonl'\n",
    "# }\n",
    "# SYSTEM_PROMPT = 'You are an expert in argumentation. Your task is to determine the type of fallacy in the given [SENTENCE]. The fallacy would be in the [FALLACY] Set. Utilize the [TITLE] and the [FULL TEXT] as context to support your decision.\\nYour answer must be in the following format with only the fallacy in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.'\n",
    "# n_sample = 4000\n",
    "# e = 1.5\n",
    "# n_eval = 8\n",
    "# n_eval_step = np.floor((n_sample / 32) / n_eval)\n",
    "\n",
    "# model_name = 'Llama3.18BInstruct'\n",
    "# spl_name = 'spl2'\n",
    "# task_name = 'fallacies'\n",
    "# train_resp = '_train_resp'\n",
    "\n",
    "# train_spl_file = f'./sampling/sample/{task_name}/{spl_name}_train.csv'\n",
    "# val_spl_file = f'./sampling/sample/{task_name}/{spl_name}_val.csv'\n",
    "# test_spl_file = f'./sampling/sample/{task_name}/{spl_name}_test.csv'\n",
    "\n",
    "# outputs_dir = f'./outputs/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}'\n",
    "\n",
    "# test_result_file = f'./test_res/{task_name}/test_res_{model_name}_{e}e{n_sample}{spl_name}{train_resp}.csv'\n",
    "# result_file = f'./test_res/{task_name}/test_res_{model_name}_{e}e{n_sample}{spl_name}{train_resp}.csv'\n",
    "\n",
    "# file_stat_train = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}_stat_train.png'\n",
    "# file_stat_val = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}_stat_val.png'\n",
    "# file_stat_test = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}_stat_test.png'\n",
    "\n",
    "# file_plot_single = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_res_single.png'\n",
    "# file_plot_multi = f'./img/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_res_multi.png'\n",
    "\n",
    "# file_metric_single = f'./test_res/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_metric_single.csv'\n",
    "# file_metric_multi = f'./test_res/{task_name}/{model_name}_{e}e{n_sample}{spl_name}{train_resp}_metric_multi.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name= \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=0.6\n",
    ")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115f8668bbb243a0b93b5c5501f97b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e00e7b5fde448bb1b8445c8f1820ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d48fc42dc9b4200828252d53c1938bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031e7f8ee56a445db698e915bff7d3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8628254f544ba9a7e771ea765e99f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m\n\u001b[1;32m     26\u001b[0m train \u001b[38;5;241m=\u001b[39m apply_chat_template(\n\u001b[1;32m     27\u001b[0m     Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(prt_train),\n\u001b[1;32m     28\u001b[0m     tokenizer,\n\u001b[1;32m     29\u001b[0m     chat_template\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_chat_template(\n\u001b[1;32m     32\u001b[0m     tokenizer,\n\u001b[1;32m     33\u001b[0m     chat_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama-3.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m d_train \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprt_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/amelia/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/amelia/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3074\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3070\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3071\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3072\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3073\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3074\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3075\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3076\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/amelia/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3516\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3515\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3516\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[1;32m   3517\u001b[0m         num_examples_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(i)\n\u001b[1;32m   3518\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "File \u001b[0;32m~/amelia/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3466\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3466\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/amelia/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3389\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3388\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3389\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[18], line 36\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     26\u001b[0m train \u001b[38;5;241m=\u001b[39m apply_chat_template(\n\u001b[1;32m     27\u001b[0m     Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(prt_train),\n\u001b[1;32m     28\u001b[0m     tokenizer,\n\u001b[1;32m     29\u001b[0m     chat_template\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_chat_template(\n\u001b[1;32m     32\u001b[0m     tokenizer,\n\u001b[1;32m     33\u001b[0m     chat_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama-3.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m d_train \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(prt_train)\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mformatting_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     37\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     38\u001b[0m )\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36mformatting_prompt\u001b[0;34m(tokenizer, d)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformatting_prompt\u001b[39m(tokenizer, d):\n\u001b[0;32m----> 2\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(txt, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m txt \u001b[38;5;129;01min\u001b[39;00m t]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: texts, }\n",
      "File \u001b[0;32m~/amelia/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:280\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 280\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[1;32m    282\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prompt'"
     ]
    }
   ],
   "source": [
    "def formatting_prompt(tokenizer, d):\n",
    "    t = d['prompt']\n",
    "    texts = [tokenizer.apply_chat_template(txt, tokenize=False, add_generation_prompt=False) for txt in t]\n",
    "    return { 'text': texts, }\n",
    "time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{OUTPUT}<|eot_id|>\"\"\"\n",
    "savefile = get_savefile(\n",
    "    task_name='fallacies',\n",
    "    spl_name='spl2',\n",
    "    m_name='meta-Llama-3.1-8B-Instruct',\n",
    "    n_sample=4000,\n",
    "    epoch=2,\n",
    "    train_resp='_train_resp',\n",
    "    outputs_dir=f'./outputs/test_fallacies',\n",
    "    time=time\n",
    ")\n",
    "fallacies, prt_train, prt_val, prt_test = get_data(savefile)\n",
    "data_train, data_val, data_test = get_datasets(tokenizer, prt_train, prt_val, prt_test)\n",
    "prt_train = prt_train.rename(columns={\"prompt\": \"conversations\"})\n",
    "train = apply_chat_template(\n",
    "    Dataset.from_pandas(prt_train),\n",
    "    tokenizer,\n",
    "    chat_template\n",
    ")\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template='llama-3.1'\n",
    ")\n",
    "d_train = Dataset.from_pandas(prt_train).map(\n",
    "    lambda x: formatting_prompt(tokenizer, x),\n",
    "    batched=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, fallacies = load_all_dataset(paths)\n",
    "# spl_data = get_all_spl(data, fallacies, n_sample)\n",
    "# prt_train, prt_val, prt_test = get_prt(spl_data, fallacies, SYSTEM_PROMPT)\n",
    "\n",
    "# prt_train.to_csv(train_spl_file, index=False)\n",
    "# prt_val.to_csv(val_spl_file, index=False)\n",
    "# prt_test.to_csv(test_spl_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = {'prompt': literal_eval, 'answer': literal_eval}\n",
    "prt_train = pd.read_csv(\n",
    "    train_spl_file,\n",
    "    converters=converter\n",
    ")\n",
    "prt_val = pd.read_csv(\n",
    "    val_spl_file,\n",
    "    converters=converter\n",
    ")\n",
    "prt_test = pd.read_csv(\n",
    "    test_spl_file,\n",
    "    converters=converter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompt(data: dict):\n",
    "    text = tokenizer.apply_chat_template(data.get('prompt'),tokenize = False, add_generation_prompt = False)\n",
    "    return { 'text': text, }\n",
    "\n",
    "SAMPLING_PARAMS = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 128,\n",
    ")\n",
    "data_train = Dataset.from_pandas(prt_train).map(\n",
    "    formatting_prompt,\n",
    "    batched=True,\n",
    ")\n",
    "data_val = Dataset.from_pandas(prt_val).map(\n",
    "    formatting_prompt,\n",
    "    batched=True,\n",
    ")\n",
    "data_test = Dataset.from_pandas(prt_test).map(\n",
    "    formatting_prompt,\n",
    "    batched=True\n",
    ").shuffle(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen(txt, model, sampling_params):\n",
    "#     output = model.fast_generate(\n",
    "#         txt,\n",
    "#         sampling_params = sampling_params,\n",
    "#     )[0].outputs[0].text\n",
    "    \n",
    "#     return output\n",
    "\n",
    "# def format_output(answer: str, fallacies: set) -> list:\n",
    "#     s = '<[|]ANSWER[|]>'\n",
    "#     tmp = re.split(s, answer)\n",
    "#     pred= [i for i in tmp if i in fallacies]\n",
    "#     return pred\n",
    "\n",
    "# def zero_shot_gen(data: list[str], model, fallacies: set, sampling_params) -> list:\n",
    "#     res = []\n",
    "#     for i in data:\n",
    "#         out = gen(i, model, sampling_params)\n",
    "#         pred = format_output(out, fallacies)\n",
    "#         res.append(pred)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(p, model, text_streamer):\n",
    "    txt = tokenizer.apply_chat_template(\n",
    "        p,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "    output = model.generate(\n",
    "        txt,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=128,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def format_output(answer: list, labels: set) -> list:\n",
    "    s = '<[|]ANSWER[|]>'\n",
    "    tmp = re.split(s, answer[0])\n",
    "    pred = [i for i in tmp if i in labels]\n",
    "    return pred\n",
    "\n",
    "def zero_shot_gen(\n",
    "    data:Dataset,\n",
    "    model,\n",
    "    labels:set,\n",
    "    text_streamer: TextStreamer\n",
    ") -> list:\n",
    "    res= []\n",
    "    prompt = data['prompt']\n",
    "    for prt in prompt:\n",
    "        out = gen(prt, model, text_streamer)\n",
    "        decoded_out = tokenizer.batch_decode(out)\n",
    "        pred = format_output(decoded_out, labels)\n",
    "        res.append(pred)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class custom_validation_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies, n_step=10):\n",
    "#         super().__init__()\n",
    "#         self.val_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#         self.n_step=n_step\n",
    "#     def on_step_end(self, args, state, control, **kwargs):\n",
    "#         if state.global_step % self.n_step == 0 and state.global_step > 0 :\n",
    "#             model.save_lora('sft_save_lora')\n",
    "#             FastLanguageModel.for_inference(model)\n",
    "#             pred = zero_shot_gen(\n",
    "#                 data=self.val_dataset['text'],\n",
    "#                 model=model,\n",
    "#                 fallacies=self.fallacies,\n",
    "#                 sampling_params=self.sampling_params\n",
    "#             )\n",
    "#             tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#             d = pd.DataFrame().from_records(tmp_pred)\n",
    "#             d['truth_label'] = self.val_dataset['answer']\n",
    "#             d['step'] = np.full((len(d['truth_label']),), state.global_step)\n",
    "#             try:\n",
    "#                 d.to_csv(\n",
    "#                     './validation_res.csv',\n",
    "#                     index=False,\n",
    "#                     mode='a',\n",
    "#                     header=['pred', 'truth_label', 'step']\n",
    "#                 )\n",
    "#             except FileNotFoundError:\n",
    "#                 d.to_csv('./validation_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_step_end(args, state, control, **kwargs)\n",
    "\n",
    "# class custom_test_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies):\n",
    "#         super().__init__()\n",
    "#         self.test_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#     def on_train_end(self, args, state, control, **kwargs):\n",
    "#         model.save_lora('sft_save_lora')\n",
    "#         FastLanguageModel.for_inference(model)\n",
    "#         pred = zero_shot_gen(\n",
    "#             data=self.test_dataset['text'],\n",
    "#             model=model,\n",
    "#             fallacies=self.fallacies,\n",
    "#             sampling_params=self.sampling_params\n",
    "#         )\n",
    "#         tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#         d = pd.DataFrame().from_records(tmp_pred)\n",
    "#         d['truth_label'] = self.test_dataset['answer']\n",
    "#         try:\n",
    "#             d.to_csv('./test_res.csv', index=False, mode='a', header=['pred','truth_label'])\n",
    "#         except FileNotFoundError:\n",
    "#             d.to_csv('./test_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_train_end(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 4, #2\n",
    "    per_device_eval_batch_size= 4,\n",
    "    gradient_accumulation_steps = 8, #4\n",
    "    eval_accumulation_steps= 8,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = e, # Set this for 1 full training run.\n",
    "    # max_steps = 60,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = outputs_dir,\n",
    "    report_to = \"tensorboard\", # Use this for WandB etc\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=n_eval_step,\n",
    ")   \n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = data_train,\n",
    "    eval_dataset=data_val,\n",
    "    # formatting_func=formatting_prompt,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args,\n",
    ")\n",
    "# trainer.get_train_dataloader().shuffle = False\n",
    "# trainer.get_eval_dataloader().shuffle = False\n",
    "# trainer.train()\n",
    "if train_resp == '_train_resp':\n",
    "    trainer = train_on_responses_only(\n",
    "        trainer,\n",
    "        instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "        response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(data):\n",
    "    try:\n",
    "        tmp = data.apply(\n",
    "            lambda x: x['pred'] in x['lbl'],\n",
    "            axis=1\n",
    "        )\n",
    "        tp = tmp.value_counts().loc[True]\n",
    "        score = tp / len(tmp)\n",
    "        fp_fn = len(tmp) - tp\n",
    "        return score, tp, fp_fn\n",
    "    except (KeyError, ZeroDivisionError):\n",
    "        tp = 0\n",
    "        score = 0\n",
    "        fp_fn = len(tmp) - tp\n",
    "        return score, tp, fp_fn\n",
    "\n",
    "def get_recall_multi(data, label):\n",
    "    n_lbl = len(label)\n",
    "    ratio = []\n",
    "    for l in label:\n",
    "        c = data['pred'].value_counts()\n",
    "        try:\n",
    "            ratio.append(min((1/n_lbl) * (c[l] / (len(data)/n_lbl)), 1/n_lbl))\n",
    "        except:\n",
    "            ratio.append(min((1/n_lbl) * (0 / (len(data)/n_lbl)), 1/n_lbl))\n",
    "    rec = sum(ratio)\n",
    "    tp = np.round(len(data)*rec)\n",
    "    fn = len(data) - tp\n",
    "    return rec, tp, fn\n",
    "\n",
    "def get_f1(precision, recall):\n",
    "    try:\n",
    "        f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "    return f1\n",
    "\n",
    "def get_metrics_single(data, labels):\n",
    "    tp_preci = 0\n",
    "    tp_rec = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    res = {}\n",
    "    for l in labels:\n",
    "        df_on_pred = data[data['pred'] == l[0]]\n",
    "        df_on_label = data.apply(\n",
    "            lambda x: x if l[0] in x['lbl'] else np.nan,\n",
    "            result_type='broadcast',\n",
    "            axis = 1\n",
    "        ).dropna()\n",
    "        precision, tp_p, fp_p = get_precision_recall(df_on_pred)\n",
    "        recall, tp_r, fn_r = get_precision_recall(df_on_label)\n",
    "        f1 = get_f1(precision, recall)\n",
    "        n_lbl = change_lbl(l)\n",
    "        res.update({str(n_lbl): (f1, precision, recall)})\n",
    "        fn += fn_r\n",
    "        fp += fp_p\n",
    "        tp_preci += tp_p\n",
    "        tp_rec += tp_r\n",
    "    precision_all_data = tp_preci / (tp_preci + fp)\n",
    "    recall_all_data = tp_rec / (tp_rec + fn)\n",
    "    f1_all_data = get_f1(precision_all_data, recall_all_data)\n",
    "    res.update({\n",
    "        'score_all_data': (f1_all_data, precision_all_data, recall_all_data)\n",
    "    })\n",
    "    return res\n",
    "\n",
    "def get_metrics_multi(data, labels):\n",
    "    tp_preci = 0\n",
    "    tp_rec = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    res = {}\n",
    "    for l in labels:\n",
    "        df_pred_lbl = data.apply(\n",
    "            lambda x: x if x['lbl'] == l else np.nan,\n",
    "            result_type='broadcast',\n",
    "            axis=1\n",
    "        ).dropna()\n",
    "        precision, tp_p, fp_p = get_precision_recall(df_pred_lbl)\n",
    "        recall, tp_r, fn_r = get_recall_multi(df_pred_lbl, l)\n",
    "        f1 = get_f1(precision, recall)\n",
    "        n_lbl = change_lbl(l)\n",
    "        res.update({str(n_lbl): (f1, precision, recall)})\n",
    "        fn += fn_r\n",
    "        fp += fp_p\n",
    "        tp_preci += tp_p\n",
    "        tp_rec += tp_r\n",
    "    precision_all_data = tp_preci / (tp_preci + fp)\n",
    "    recall_all_data = tp_rec / (tp_rec + fn)\n",
    "    f1_all_data = get_f1(precision_all_data, recall_all_data)\n",
    "    res.update({\n",
    "        'score_all_data': (f1_all_data, precision_all_data, recall_all_data)\n",
    "    })\n",
    "    return res\n",
    "\n",
    "def change_lbl(labels: list) -> list:\n",
    "    new_lbl = []\n",
    "    replace_lbl = {\n",
    "        'appeal to nature': 'AN',\n",
    "        'straw man': 'STM',\n",
    "        'false dilemma': 'FD',\n",
    "        'appeal to tradition': 'AT',\n",
    "        'causal oversimplification': 'COS',\n",
    "        'appeal to majority': 'AM',\n",
    "        'ad hominem': 'AH',\n",
    "        'appeal to ridicule': 'AR',\n",
    "        'circular reasoning': 'CR',\n",
    "        'false analogy': 'FA',\n",
    "        'false causality': 'FC',\n",
    "        'appeal to fear': 'AF',\n",
    "        'appeal to worse problems': 'AWP',\n",
    "        'none': 'NONE',\n",
    "        'guilt by association': 'GA',\n",
    "        'equivocation': 'EQ',\n",
    "        'appeal to authority': 'AA',\n",
    "        'hasty generalization': 'HG',\n",
    "        'slippery slope': 'SS',\n",
    "        'ad populum': 'AP'\n",
    "    }\n",
    "    for l in labels:\n",
    "        new_lbl.append(replace_lbl.get(l))\n",
    "    return new_lbl\n",
    "    \n",
    "\n",
    "def get_metrics(data_single: pd.DataFrame, data_multi: pd.DataFrame):\n",
    "    labels_single = data_single['lbl'].value_counts().index\n",
    "    labels_multi = data_multi['lbl'].value_counts().index\n",
    "    scores_single = get_metrics_single(data_single, labels_single)\n",
    "    scores_multi = get_metrics_multi(data_multi, labels_multi)\n",
    "    return scores_single, scores_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "pred = zero_shot_gen(\n",
    "    data=data_test,\n",
    "    model=model,\n",
    "    labels=fallacies,\n",
    "    text_streamer=text_streamer\n",
    ")\n",
    "\n",
    "names_dataset = data_test['datasets']\n",
    "true_labels = data_test['answer']\n",
    "\n",
    "tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "pred_flat = list(itertools.chain.from_iterable(tmp_pred))\n",
    "\n",
    "d_res = {'names': names_dataset, 'pred': pred_flat, 'lbl': true_labels}\n",
    "df_res = pd.DataFrame(data=d_res)\n",
    "df_res.to_csv(test_result_file, index=False)\n",
    "# df_res = pd.read_csv(\n",
    "#     result_file,\n",
    "#     converters={'lbl': literal_eval}\n",
    "# )\n",
    "#metric = get_metrics(df_res, fallacies)\n",
    "df_single = df_res.apply(\n",
    "    lambda x: x if len(x['lbl']) <= 1 else np.nan,\n",
    "    axis=1,\n",
    "    result_type='broadcast'\n",
    ").dropna()\n",
    "df_multi = df_res.apply(\n",
    "    lambda x: x if len(x['lbl']) > 1 else np.nan,\n",
    "    axis=1,\n",
    "    result_type='broadcast',\n",
    ").dropna()\n",
    "metric_single, metric_multi = get_metrics(df_single, df_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stat_sample(\n",
    "    sample: pd.DataFrame,\n",
    "    lst_labels: set,\n",
    "    title: str,\n",
    "    savefile=None,\n",
    ") -> None:\n",
    "    width = 0.3\n",
    "    fig, ax = plt.subplots()\n",
    "    d = {}\n",
    "    spl_len = {}\n",
    "    spl_over = {}\n",
    "    under = []\n",
    "    over = []\n",
    "    x = np.arange(len(lst_labels))\n",
    "    b = 0\n",
    "    bar_label = True\n",
    "    dataset_names = sample['datasets'].value_counts().index.to_list()\n",
    "    for name in dataset_names:\n",
    "        spl_dataset = sample[sample['datasets'] == name]\n",
    "        spl = spl_dataset[spl_dataset['spl'] == 'sample']\n",
    "        over_spl = spl_dataset[spl_dataset['spl'] == 'oversample']\n",
    "        len_s = {\n",
    "            lbl: len(spl[spl['single_ans'] == lbl])\n",
    "            for lbl in lst_labels\n",
    "        }\n",
    "            \n",
    "        len_o = {\n",
    "            lbl: len(over_spl[over_spl['single_ans'] == lbl])\n",
    "            for lbl in lst_labels\n",
    "        }\n",
    "        spl_len.update({name: len_s})\n",
    "        spl_over.update({name: len_o})\n",
    "    df_spl_len = pd.DataFrame().from_dict(spl_len) # .sort_index()\n",
    "    df_oversample = pd.DataFrame().from_dict(spl_over) # .sort_index()\n",
    "    df_spl_len.index = change_lbl(df_spl_len.index)\n",
    "    df_oversample.index = change_lbl(df_oversample.index)\n",
    "    df_spl_len = df_spl_len.sort_index()\n",
    "    df_oversample = df_oversample.sort_index()\n",
    "    for name in dataset_names:\n",
    "        under.append((name, df_spl_len[name]))\n",
    "        over.append((name,df_oversample[name]))\n",
    "    d.update({\n",
    "        'under': under,\n",
    "        'over': over,\n",
    "    })\n",
    "    for _,v in d.items():\n",
    "        if bar_label:\n",
    "            lbl = 'sample'\n",
    "        else:\n",
    "            lbl = 'oversample'\n",
    "        p = ax.bar(x, v[0][1], width, label=f'{lbl} {v[0][0]}', bottom=b)\n",
    "        # ax.bar_label(p, label_type='center')\n",
    "        p = ax.bar(\n",
    "            x=x,\n",
    "            height=v[1][1],\n",
    "            width=width,\n",
    "            label=f'{lbl} {v[1][0]}',\n",
    "            bottom=b + v[0][1]\n",
    "        )\n",
    "        # ax.bar_label(p, label_type='center')\n",
    "        bar_label=False\n",
    "        b = b + v[0][1] + v[1][1]\n",
    "    # ax.set_yticks(np.arange(0, max(df_spl_len.max().values) + 2, step=1))\n",
    "    lst_labels = change_lbl(lst_labels)\n",
    "    ax.set_xticks(x, sorted(lst_labels), rotation=90)\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_title(title)\n",
    "    fig.set_size_inches(20, 10)\n",
    "    # plt.ylim(0, max(df_spl_len.max().values) + 1)\n",
    "    plt.show()\n",
    "    if savefile is not None:\n",
    "        fig.savefig(savefile, format='png')\n",
    "\n",
    "def plot_metric(\n",
    "    metric: dict,\n",
    "    columns: list[str]=['f1', 'precision', 'recall'],\n",
    "    title='',\n",
    "    file_plot=None,\n",
    "    file_metric=None,\n",
    "):\n",
    "    # rand_mark = pd.Series(np.full((len(metric),), 1/(len(metric)-1)))\n",
    "    df_metric = pd.DataFrame().from_dict(\n",
    "        metric,\n",
    "        orient='index',\n",
    "        columns=columns\n",
    "    )\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    df_metric.plot(\n",
    "        ax=ax,\n",
    "        kind='bar',\n",
    "        figsize=(20,14),\n",
    "        title=title,\n",
    "    )\n",
    "    # rand_mark.plot(ax=ax, color='red', linestyle='dashed')\n",
    "    # ax.set_yticks(np.arange(0, 1.1, step=0.05))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    if file_plot is not None:\n",
    "        fig.savefig(file_plot, format='png')\n",
    "    if file_metric is not None:\n",
    "        df_metric.to_csv(file_metric, header=['F1', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['f1', 'precision', 'recall']\n",
    "\n",
    "# plot_stat_sample(\n",
    "#     sample=prt_train,\n",
    "#     lst_labels=fallacies, \n",
    "#     savefile=file_stat_train,\n",
    "#     title=f'sample {spl_name} train'\n",
    "# )\n",
    "# plot_stat_sample(\n",
    "#     sample=prt_val,\n",
    "#     lst_labels=fallacies,\n",
    "#     savefile=file_stat_val,\n",
    "#     title=f'sample {spl_name} val'\n",
    "# )\n",
    "# plot_stat_sample(\n",
    "#     sample=prt_test,\n",
    "#     lst_labels=fallacies,\n",
    "#     savefile=file_stat_test,\n",
    "#     title=f'sample {spl_name} test'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(\n",
    "    metric=metric_single,\n",
    "    title=f'Scores {n_sample} sample single fallacies',\n",
    "    file_plot=file_plot_single,\n",
    "    file_metric=file_metric_single\n",
    ")\n",
    "plot_metric(\n",
    "    metric=metric_multi,\n",
    "    title=f'Scores {n_sample} sample multi fallacies',\n",
    "    file_plot=file_plot_multi,\n",
    "    file_metric=file_metric_multi\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
