{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training\n",
    "model, tokenizer = run('stance_detection', do_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_savefile\n",
    "from src.prompting import get_datasets\n",
    "from src.metrics import get_metrics\n",
    "from src.plot import plot_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.aduc import get_data\n",
    "# from src.aduc import change_lbl\n",
    "import src.aduc as aduc\n",
    "\n",
    "# from src.claim_detect import get_data\n",
    "# from src.claim_detect import change_lbl\n",
    "import src.claim_detect as cd\n",
    "\n",
    "# from src.evidence_detect import get_data\n",
    "# from src.evidence_detect import change_lbl\n",
    "import src.evidence_detect as ed\n",
    "\n",
    "# from src.evidence_type import get_data\n",
    "# from src.evidence_type import change_lbl\n",
    "import src.evidence_type as et\n",
    "\n",
    "# from src.fallacies import get_data\n",
    "# from src.fallacies import change_lbl\n",
    "import src.fallacies as fd\n",
    "\n",
    "# from src.quality import get_data\n",
    "# from src.quality import change_lbl\n",
    "import src.quality as aq\n",
    "\n",
    "# from src.relation import get_data\n",
    "# from src.relation import change_lbl\n",
    "import src.relation as arc\n",
    "\n",
    "# from src.stance_detect import get_data\n",
    "# from src.stance_detect import change_lbl\n",
    "import src.stance_detect as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'aduc': f'./outputs/aduc/2025-05-07_17:29:07_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'claim_detection': f'./outputs/claim_detection/2025-05-09_14:57:11_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'evidence_detection': f'./outputs/evidence_detection/2025-05-09_15:51:24_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'evidence_type': f'./outputs/evidence_type/2025-05-12_10:47:06_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'fallacies': f'./outputs/fallacies/2025-05-09_16:41:44_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-340',\n",
    "    'relation': f'./outputs/relation/2025-05-09_18:41:38_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'stance_detection': f'./outputs/stance_detection/2025-05-15_09:50:35_Meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "    'quality': f'./outputs/quality/2025-05-12_09:44:16_meta-Llama-3.1-8B-Instruct_2e4000spl2_train_resp/checkpoint-250',\n",
    "}\n",
    "st_model = {\n",
    "    'aduc': f'./saved_model/aduc',\n",
    "    'claim_detection': f'./saved_model/claim_detection',\n",
    "    'evidence_detection': f'./saved_model/evidence_detection',\n",
    "    'evidence_type': f'./saved_model/evidence_type',\n",
    "    'fallacies': f'./saved_model/fallacies',\n",
    "    'relation': f'./saved_model/relation',\n",
    "    'stance_detection': f'./saved_model/stance_detection',\n",
    "    'quality': f'./saved_model/quality'\n",
    "}\n",
    "d_sys_prt = {\n",
    "    'aduc': f'You are an expert in argumentation. Your task is to determine whether the given [SENTENCE] is a Claim or a Premise. Utilize the [TOPIC] and the [FULL TEXT] as context to support your decision\\nYour answer must be in the following format with only Claim or Premise in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'claim_detection': f'You are an expert in argumentation. Your task is to determiner whether the given [SENTENCE] is a Claim or Non-claim. Utilize the [TOPIC] and the [FULL TEXT] as context to support your decision\\nYour answer must be in the following format with only Claim or Non-claim in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'evidence_detection': f'You are an expert in argumentation. Your task is to determine whether the given [SENTENCE] is an Evidence or Non-evidence. Utilize the [TOPIC] and the [ARGUMENT] as context to support your decision\\nYour answer must be in the following format with only Evidence or Non-evidence in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'evidence_type': f'You are an expert in argumentation. Your task is to determine the type of evidence of the given [SENTENCE]. The type of evidence would be in the [TYPE] set. Utilize the [TOPIC] and the [CLAIM] as context to support your decision\\nYour answer must be in the following format with only the type of evidence in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'fallacies': f'You are an expert in argumentation. Your task is to determine the type of fallacy in the given [SENTENCE]. The fallacy would be in the [FALLACY] Set. Utilize the [TITLE] and the [FULL TEXT] as context to support your decision.\\nYour answer must be in the following format with only the fallacy in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'relation': f'You are an expert in argumentation. Your task is to determine the type of relation between [SOURCE] and [TARGET]. The type of relation would be in the [RELATION] set. Utilize the [TOPIC] as context to support your decision\\nYour answer must be in the following format with only the type of the relation in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'stance_detection': f'You are an expert in argumentation. Your task is to determine whether the given [SENTENCE] is For or Against. Utilize the [TOPIC] as context to support your decision\\nYour answer must be in the following format with only For or Against in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.',\n",
    "    'quality': f'You are an expert in argumentation. Your task is to determine the quality of the [SENTENCE]. The quality would be in the [QUALITY] set. Utilize the [TOPIC], the [STANCE] and the [DEFINITION] as context to support your decision\\nYour answer must be in the following format with only the quality in the answer section:\\n<|ANSWER|><answer><|ANSWER|>.'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Parameters\\n\",\n",
    "task = 'claim_detection'\n",
    "task_data = 'claim_detection'\n",
    "task_title = 'Claim Detection'\n",
    "ckpt_name = checkpoint.get(task)\n",
    "st_name = ''\n",
    "gguf_file = f'./gguf_model/{task}'\n",
    "max_seq_lenght = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "gpu_mem_use = 0.6\n",
    "system_prompt = d_sys_prt.get(task_data)\n",
    "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{OUTPUT}<|eot_id|>\"\"\"\n",
    "time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "s_file = get_savefile(\n",
    "    task_name=task_data,\n",
    "    spl_name='spl2',\n",
    "    m_name='Meta-Llama-3.1-8B-Instruct',\n",
    "    n_sample=4000,\n",
    "    epoch=2,\n",
    "    train_resp=f'_train_resp',\n",
    "    outputs_dir=f'./outputs/test_{task}',\n",
    "    time=time\n",
    ")\n",
    "match task_data:\n",
    "    case 'aduc':\n",
    "        labels, tr_d, val_d, test_d = aduc.get_data(s_file)\n",
    "        change_lbl = aduc.change_lbl\n",
    "    case 'claim_detection':\n",
    "        labels, tr_d, val_d, test_d = cd.get_data(s_file)\n",
    "        change_lbl = cd.change_lbl\n",
    "    case 'evidence_detection':\n",
    "        labels, tr_d, val_d, test_d = ed.get_data(s_file)\n",
    "        change_lbl = ed.change_lbl\n",
    "    case 'evidence_type':\n",
    "        labels, tr_d, val_d, test_d = et.get_data(s_file)\n",
    "        change_lbl = et.change_lbl\n",
    "    case 'fallacies':\n",
    "        labels, tr_d, val_d, test_d = fd.get_data(s_file)\n",
    "        change_lbl = fd.change_lbl\n",
    "    case 'relation':\n",
    "        labels, tr_d, val_d, test_d = arc.get_data(s_file)\n",
    "        change_lbl = arc.change_lbl\n",
    "    case 'stance_detection':\n",
    "        labels, tr_d, val_d, test_d = sd.get_data(s_file)\n",
    "        change_lbl = sd.change_lbl\n",
    "    case 'quality':\n",
    "        labels, tr_d, val_d, test_d = aq.get_data(s_file)\n",
    "        change_lbl = aq.change_lbl\n",
    "# labels, tr_d, val_d, test_d = get_data(s_file)\n",
    "print(system_prompt)\n",
    "print(ckpt_name)\n",
    "print(gguf_file)\n",
    "print(change_lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Load Model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=ckpt_name,\n",
    "    max_seq_length=max_seq_lenght,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=gpu_mem_use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Get Datasets\n",
    "train_set, val_set, test_set = get_datasets(\n",
    "    tokenizer=tokenizer,\n",
    "    train=tr_d,\n",
    "    val=val_d,\n",
    "    test=test_d, \n",
    "    chat_template=chat_template,\n",
    "    sys_prt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on fewer data\n",
    "# from datasets import Dataset\n",
    "# tmp_set = test_set[:10]\n",
    "# tmp_set = pd.DataFrame().from_records(tmp_set)\n",
    "# tmp_set = Dataset.from_pandas(tmp_set)\n",
    "# tmp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint : Run On test_set\n",
    "from src.training import test\n",
    "res_test_chpt = test(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_test=test_set,\n",
    "    labels=labels,\n",
    ")\n",
    "# metric, metric_m = get_metrics(change_lbl, res_test_chpt)\n",
    "metric, _ = get_metrics(change_lbl, res_test_chpt, is_multi_lbl=False)\n",
    "plot_metric(\n",
    "    metric=metric,\n",
    "    # title=f'{task_title}: Scores ckpt{nbckpt}',\n",
    "    # file_plot=f'./img/{task}/scores_ckpt{nbckpt}_metric_single.png',\n",
    "    # file_metric=f'./test_res/{task}/scores_ckpt{nbckpt}_metric_single.csv'\n",
    ")\n",
    "# plot_metric(\n",
    "#     metric=metric_m\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\n",
    "#     './saved_model/claim_detection',\n",
    "#     tokenizer,\n",
    "#     save_method=\"merged_16bit\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\n",
    "    './saved_model/claim_detection/saved_lora',\n",
    "    tokenizer,\n",
    "    save_method=\"lora\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Method\n",
    "# model.save_pretrained_gguf(\n",
    "#     gguf_file,\n",
    "#     tokenizer,\n",
    "#     quantization_method = \"q8_0\", # choose quant method: q8_0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "res = []\n",
    "s = '<[|]ANSWER[|]>'\n",
    "quant = 'q8_0'\n",
    "names_dataset = test_d['datasets']\n",
    "true_labels = test_d['answer']\n",
    "# print(len(test_d['conversations']))\n",
    "for i in test_d['conversations']:\n",
    "    response = chat(model='unsloth_model', messages=[i[1]])\n",
    "    print(response['message']['content'])\n",
    "    tmp = re.split(s, response['message']['content'])\n",
    "    res.append(tmp[1])\n",
    "d_res = {'names': names_dataset, 'pred': res, 'lbl': true_labels}\n",
    "df_res = pd.DataFrame(data=d_res)\n",
    "df_res.to_csv(f'./test_res/{task}/test_result_gguf_{quant}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metric_single, metric_multi = get_metrics(change_lbl, df_res)\n",
    "metric_single, _ = get_metrics(change_lbl, df_res, is_multi_lbl=False)\n",
    "plot_metric(\n",
    "    metric=metric_single,\n",
    "    # title=f'{task_title}: Score gguf_{quant} single label',\n",
    "    # file_plot=f'./img/{task}/scores_gguf_{quant}_metric_single.png',\n",
    "    # file_metric=f'./test_res/{task}/scores_gguf_{quant}_metric_single.csv'\n",
    ")\n",
    "# For Fallacies Task Only\n",
    "# plot_metric(\n",
    "#     metric=metric_multi,\n",
    "#     title=f'{task_title}: Score gguf_{quant} multi label',\n",
    "#     file_plot=f'./img/{task}/scores_gguf_{quant}_metric_multi.png',\n",
    "#     file_metric=f'./test_res/{task}/scores_gguf_{quant}_metric_multi.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen(txt, model, sampling_params):\n",
    "#     output = model.fast_generate(\n",
    "#         txt,\n",
    "#         sampling_params = sampling_params,\n",
    "#     )[0].outputs[0].text\n",
    "    \n",
    "#     return output\n",
    "\n",
    "# def format_output(answer: str, fallacies: set) -> list:\n",
    "#     s = '<[|]ANSWER[|]>'\n",
    "#     tmp = re.split(s, answer)\n",
    "#     pred= [i for i in tmp if i in fallacies]\n",
    "#     return pred\n",
    "\n",
    "# def zero_shot_gen(data: list[str], model, fallacies: set, sampling_params) -> list:\n",
    "#     res = []\n",
    "#     for i in data:\n",
    "#         out = gen(i, model, sampling_params)\n",
    "#         pred = format_output(out, fallacies)\n",
    "#         res.append(pred)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class custom_validation_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies, n_step=10):\n",
    "#         super().__init__()\n",
    "#         self.val_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#         self.n_step=n_step\n",
    "#     def on_step_end(self, args, state, control, **kwargs):\n",
    "#         if state.global_step % self.n_step == 0 and state.global_step > 0 :\n",
    "#             model.save_lora('sft_save_lora')\n",
    "#             FastLanguageModel.for_inference(model)\n",
    "#             pred = zero_shot_gen(\n",
    "#                 data=self.val_dataset['text'],\n",
    "#                 model=model,\n",
    "#                 fallacies=self.fallacies,\n",
    "#                 sampling_params=self.sampling_params\n",
    "#             )\n",
    "#             tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#             d = pd.DataFrame().from_records(tmp_pred)\n",
    "#             d['truth_label'] = self.val_dataset['answer']\n",
    "#             d['step'] = np.full((len(d['truth_label']),), state.global_step)\n",
    "#             try:\n",
    "#                 d.to_csv(\n",
    "#                     './validation_res.csv',\n",
    "#                     index=False,\n",
    "#                     mode='a',\n",
    "#                     header=['pred', 'truth_label', 'step']\n",
    "#                 )\n",
    "#             except FileNotFoundError:\n",
    "#                 d.to_csv('./validation_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_step_end(args, state, control, **kwargs)\n",
    "\n",
    "# class custom_test_callback(TrainerCallback):\n",
    "#     def __init__(self, data, sampling_params, fallacies):\n",
    "#         super().__init__()\n",
    "#         self.test_dataset = data\n",
    "#         self.sampling_params = sampling_params\n",
    "#         self.fallacies = fallacies\n",
    "#     def on_train_end(self, args, state, control, **kwargs):\n",
    "#         model.save_lora('sft_save_lora')\n",
    "#         FastLanguageModel.for_inference(model)\n",
    "#         pred = zero_shot_gen(\n",
    "#             data=self.test_dataset['text'],\n",
    "#             model=model,\n",
    "#             fallacies=self.fallacies,\n",
    "#             sampling_params=self.sampling_params\n",
    "#         )\n",
    "#         tmp_pred = [i if i != [] else ['Failed'] for i in pred]\n",
    "#         d = pd.DataFrame().from_records(tmp_pred)\n",
    "#         d['truth_label'] = self.test_dataset['answer']\n",
    "#         try:\n",
    "#             d.to_csv('./test_res.csv', index=False, mode='a', header=['pred','truth_label'])\n",
    "#         except FileNotFoundError:\n",
    "#             d.to_csv('./test_res.csv', index=False, header=['pred', 'truth_label'])\n",
    "#         return super().on_train_end(args, state, control, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
